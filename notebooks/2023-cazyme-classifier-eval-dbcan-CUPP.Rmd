---
title: "Evaluation of CAZyme Classifiers dbCAN and CUPP"
author: "Emma E. M. Hobbs"
date: "2023-07-26"
output:
  bookdown::html_document2:
    toc: true
    toc_depth: 5
    toc_float:
      toc_collapsed: false
    number_sections: true
    css: "css/rmd_style.css"
    theme: lumen
---

```{r setup, include=FALSE}
#
# Import required libraries
#
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library('kableExtra')
library(broom)
library(tidyverse)
library(magrittr) 
library('ggplot2')
library('pROC')
library("DT")
library("datasets")
library("dplyr")
library("GGally")
library("ggridges")
library("rjson")
library("readr")
library(knitr)
library(tidyverse)
library("MLmetrics")
library('mclust')  # adjusted rand index
library('Metrics')  # Fbeta score
library("devtools")
library(ggpubr)
library(RColorBrewer)
library(cowplot)
library(scales)
library("MASS")
library("interp")
library('reshape2')
library(Rmisc)
library(yaml)
library("heatmaply")
library("tidyr")

#
# define global constants
#

# BG color for plots - should match .figure and .caption classes in rmd_style.css
figbg = "whitesmoke"

#
# Colour schemes for data
#

# define small colour set, with one colour per CAZyme classifier
colour_set <- c(
  "#940113",
  "#0a7a6d",
  "#d4af37",
  "#7844b8",
  "#acbf1b",
  "#c22176",
  "#3888e0",
  "#e07e38",
  "#13851e",
  "#959c9e"
)

colour_set_roc <- c(
  "#00547d", "#8f0034","#006321", "#e07e38"
)

colour_set_tools <- c(
  "#00547d", "#027eba", "#42bffc", "#7ad4ff",
  "#8f0034", "#bf005c", "#ff4d8e", "#ff94bb",
  "#006321", "#009130", "#19d156", "#69db8f",
  "#e07e38"
)

colour_set_tools <- c(
  "#00547d", 
  "#8f0034", 
  "#006321", 
  "#e07e38",
  "#c22176"
)

c(
  '[0.00]',
  '(0.00, 0.05]', '(0.05, 0.10]',
  '(0.10, 0.15]', '(0.15, 0.20]',
  '(0.20, 0.25]', '(0.25, 0.30]',
  '(0.30, 0.35]', '(0.35, 0.40]',
  '(0.40, 0.45]', '(0.45, 0.50]',
  '(0.50, 0.55]', '(0.55, 0.60]',
  '(0.60, 0.65]', '(0.65, 0.70]',
  '(0.70, 0.75]', '(0.75, 0.80]',
  '(0.80, 0.85]', '(0.85, 0.90]',
  '(0.90, 0.95]', '(0.95, 1.00]',
  '[1.00]')

# colour gradient from red-orange-yellow-blue-green
colour_grad <- c(
  "#1c0a00", "#620021", "#940113", "#b8170f", "#d74427", "#f37945", "#faa86e", "#fcac17", "#fcd42d", "#fae8a0", "#c7eef0", "#98D0E4", "#66A5CC",
  "#3D7A99", "#2e5a6b", "#074a38", "#2a6e46", "#228537", "#1ba841", "#3fcc3f", "#6ddb1a", "#a5f200"
)
# define large colour set when many colours are needed
n <- 60
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
qual_col = qual_col_pals[c(TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE), ]
large_colour_set = unlist(mapply(brewer.pal, qual_col$maxcolors, rownames(qual_col)))

#
# Classifiers evaluated
#
# This is used to define factor levels and order the data output
classifiers_short = c(
  'dbCAN_4', 'dbCAN_4:HMMER', 'dbCAN_4:DIAMOND', 'dbCAN_4:dbCAN-sub',
  'CUPP'
)
classifiers_full = c(
  'dbCAN_4', 'dbCAN_4:HMMER', 'dbCAN_4:DIAMOND', 'dbCAN_4:dbCAN-sub',
  'CUPP'
)
main_classifiers = c('dbCAN_4','CUPP')

cazy_class_list = c('GH', 'GT', 'PL', 'CE', 'AA', 'CBM')
```


```{r importData, include=FALSE}
tax_grps <- yaml.load_file("../data/test_sets/test_sets/test_set_taxs.yaml")

# import file with test set coverage of genomes and CAZomes
test_set_cov_df = read.table(
  "../data/test_sets/test_sets/cazome_coverage_2023_07_26-10_38_27.txt",
  sep="\t",
  header=TRUE
)

#
# CAZyme/non-CAZyme classifications
#

# import "binary_classification_evaluation_<date>.csv"
# Written in long form, with the columns: Statistic_parameter, Genomic_assembly, Prediction_tool, Statistic_value
binary_stat_df <- read.csv("../results/cazyme_classifications/cazyme-noncazyme_evaluation_2023_07_29.csv")
binary_stat_df <- binary_stat_df[which(binary_stat_df$Prediction_tool %in% classifiers_full), ]


# import "binary_bootstrap_accuracy_evaluation_<date>.csv"
# Contains the output from the bootstrap resampling of the binary classification of CAZymes/non-CAZymes
bootstrap_results_df <- read.csv("../results/cazyme_classifications/binary_bootstrap_accuracy_evaluation_2023_07_29.csv")
bootstrap_results_df <- bootstrap_results_df[which(bootstrap_results_df$Prediction_tool %in% classifiers_full), ]

# import binary false positive and negative predictions
binary_fp_predictions = read.csv("../results/cazyme_classifications/binary_false_positive_classifications_2023_07_29.csv")
binary_fn_predictions = read.csv("../results/cazyme_classifications/binary_false_negative_classifications_2023_07_29.csv")


#
# class classification
#

# import "class_predicted_classifications_<date>.csv"
# Contains the columns: Genomic_accession, Protein_accession, Prediciton_tool, one column per CAZy class, Rand_index and Adjusted_rand_index
# Contains the clasifications (0/1) for each prediction tool for every protein across all test sets
# Includes the calculated rand index and adjusted rand index
class_ri_ari_raw_df <- read.csv("../results/cazy_class_classifications/class_predicted_classifications_2023_07_29.csv")
class_ri_ari_raw_df <- class_ri_ari_raw_df[which(class_ri_ari_raw_df$Prediction_tool %in% classifiers_full), ]

# import "class_stats_per_test_set_<date>.csv"
# Contains the calculated performance statistics when evaluating performance of CAZy class prediction per test set.
# Written in long form with the columns: Genomic_accession, Prediction_tool, CAZy_class, Statistic_parameter, Statistic_value
cazy_class_df <- read.csv("../results/cazy_class_classifications/class_stats_per_test_set_2023_07_29.csv")
cazy_class_df <- cazy_class_df[which(cazy_class_df$Prediction_tool %in% classifiers_full), ]
#
# family classification
#

# import "family_long_form_stats_df_<date>.csv"
# Contains the calculated performance satistics when evaluating performance of CAZy family prediction per test set.
# Written in long form with the columns: CAZy_family, Prediction_tool, Statistical_parameter, Statistic_value
cazy_family_long_df <- read.csv("../results/cazy_family_classifications/family_long_form_stats_df_2023_07_29.csv")
cazy_family_long_df <- cazy_family_long_df[which(cazy_family_long_df$Prediction_tool %in% classifiers_full), ]
# import "family_predicted_classifications_<date>.csv
# Contains the CAZy family epredictions for every protein for every CAZy family, and includes the 
# Rand indx and Adjusted Rand Index for every protein
# contains RI and ARI data
fam_classification_df <- read.csv ("../results/cazy_family_classifications/family_predicted_classifications_2023_07_29.csv")
fam_classification_df <- fam_classification_df[which(fam_classification_df$Prediction_tool %in% classifiers_full), ]
# Load in data on CAZy family populations and sample sizes
fam.populations <- fromJSON(file = "../results/CAZy_fam_populations_2023_07_29.json")
fam.sample.sizes <- fromJSON(file = "../results/CAZy_fam_testset_freq_2023_07_29.json")

#
# Add tax data to dataframes
#

# add tax groups to df of cazyme/non-cazmye differnetiation
# binary_stat_df
bin.stat.tax.col <- c()
for (genome in binary_stat_df$Genomic_assembly){
  if (genome %in% tax_grps$Bacteria){
    bin.stat.tax.col <- append(bin.stat.tax.col, "Bacteria")
  } else if (genome %in% tax_grps$Eukaryote){
    bin.stat.tax.col <- append(bin.stat.tax.col, "Eukaryote")
  } else {
    print(genome)
  }
}

bin.stat.tax.col <- append(bin.stat.tax.col, rep("All", nrow(binary_stat_df)))
binary_tax_df <- rbind(binary_stat_df, binary_stat_df)
binary_tax_df$Tax_group <- bin.stat.tax.col
binary_tax_df

# add tax groups to df of cazy class classification
# cazy_class_df
class.stat.tax.col <- c()
for (genome in cazy_class_df$Genomic_accession){
  if (genome %in% tax_grps$Bacteria){
    class.stat.tax.col <- append(class.stat.tax.col, "Bacteria")
  } else if (genome %in% tax_grps$Eukaryote){
    class.stat.tax.col <- append(class.stat.tax.col, "Eukaryote")
  } else {
    print(genome)
  }
}

cazy_class_tax_df <- data.frame(cazy_class_df)
cazy_class_tax_df$Tax_group <- class.stat.tax.col
 
bact.cazy_class_df <- cazy_class_tax_df[which(cazy_class_tax_df$Tax_group == "Bacteria"), ]
euk.cazy_class_df <- cazy_class_tax_df[which(cazy_class_tax_df$Tax_group == "Eukaryote"), ]

# add tax groups to df of class classification predictions
# contains RI and ARI values
# class_ri_ari_raw_df
class.ri.stat.tax.col <- c()
for (genome in class_ri_ari_raw_df$Genomic_accession){
  if (genome %in% tax_grps$Bacteria){
    class.ri.stat.tax.col <- append(class.ri.stat.tax.col, "Bacteria")
  } else if (genome %in% tax_grps$Eukaryote){
    class.ri.stat.tax.col <- append(class.ri.stat.tax.col, "Eukaryote")
  } else {
    print(genome)
  }
}

class_tax_ri_ari_df <- data.frame(class_ri_ari_raw_df)
class_tax_ri_ari_df$Tax_group <- class.ri.stat.tax.col

# tax evaluation of CAZy family classification
# columns needed: Prediction_tool, CAZy_class, Statistic parameter, Statistic value
fam.tax.euk.df <- read.csv("../results/cazy_family_classifications/tax_performance/family_long_form_stats_tax_comparison_Eukaryote_2023_07_29.csv")
fam.tax.bact.df <- read.csv("../results/cazy_family_classifications/tax_performance/family_long_form_stats_tax_comparison_Bacteria_2023_07_29.csv")
# add CAZy class column

fam.tax.euk.df <- fam.tax.euk.df[which(fam.tax.euk.df$Prediction_tool %in% classifiers_full), ]
fam.tax.bact.df <- fam.tax.bact.df[which(fam.tax.bact.df$Prediction_tool %in% classifiers_full), ]


# separate names into one vector per CAZy class
gh.names = c()
gt.names = c()
pl.names = c()
ce.names = c()
aa.names = c()
cbm.names = c()
fam.long.df.tax.col = c()

for (name in cazy_family_long_df$CAZy_family){
  if (startsWith(name, "GH")){
    gh.names = append(gh.names, name)
    fam.long.df.tax.col = append(fam.long.df.tax.col, "GH")
  } else if (startsWith(name, "GT")){
    gt.names = append(gt.names, name)
    fam.long.df.tax.col = append(fam.long.df.tax.col, "GT")
  } else if (startsWith(name, "PL")){
    pl.names = append(pl.names, name)
    fam.long.df.tax.col = append(fam.long.df.tax.col, "PL")
  } else if (startsWith(name, "CE")){
    ce.names = append(ce.names, name)
    fam.long.df.tax.col = append(fam.long.df.tax.col, "CE")
  } else if (startsWith(name, "AA")){
    aa.names = append(aa.names, name)
    fam.long.df.tax.col = append(fam.long.df.tax.col, "AA")
  } else {
    cbm.names = append(cbm.names, name)
    fam.long.df.tax.col = append(fam.long.df.tax.col, "CBM")
  }
}

cazy_family_long_df$CAZy_class = fam.long.df.tax.col

# add tax column
add_cazy_class <- function(df, cazy.class, fam.names){
  df.subset <- df[which(df$CAZy_family %in% fam.names), ]
  class.col <- rep(cazy.class, nrow(df.subset))
  df.subset$CAZy_class <- class.col
  return(df.subset)
}
fam.euk.gh.subset <- add_cazy_class(fam.tax.euk.df, "GH", gh.names)
fam.euk.gh.subset$Tax_group <- rep('Eukaryote', nrow(fam.euk.gh.subset))
fam.euk.gt.subset <- add_cazy_class(fam.tax.euk.df, "GT", gt.names)
fam.euk.gt.subset$Tax_group <- rep('Eukaryote', nrow(fam.euk.gt.subset))
fam.euk.pl.subset <- add_cazy_class(fam.tax.euk.df, "PL", pl.names)
fam.euk.pl.subset$Tax_group <- rep('Eukaryote', nrow(fam.euk.pl.subset))
fam.euk.ce.subset <- add_cazy_class(fam.tax.euk.df, "CE", ce.names)
fam.euk.ce.subset$Tax_group <- rep('Eukaryote', nrow(fam.euk.ce.subset))
fam.euk.aa.subset <- add_cazy_class(fam.tax.euk.df, "AA", aa.names)
fam.euk.aa.subset$Tax_group <- rep('Eukaryote', nrow(fam.euk.aa.subset))
fam.euk.cbm.subset <- add_cazy_class(fam.tax.euk.df, "CBM", cbm.names)
fam.euk.cbm.subset$Tax_group <- rep('Eukaryote', nrow(fam.euk.cbm.subset))

fam.bact.gh.subset <- add_cazy_class(fam.tax.bact.df, "GH", gh.names)
fam.bact.gh.subset$Tax_group <- rep('Bacteria', nrow(fam.bact.gh.subset))
fam.bact.gt.subset <- add_cazy_class(fam.tax.bact.df, "GT", gt.names)
fam.bact.gt.subset$Tax_group <- rep('Bacteria', nrow(fam.bact.gt.subset))
fam.bact.pl.subset <- add_cazy_class(fam.tax.bact.df, "PL", pl.names)
fam.bact.pl.subset$Tax_group <- rep('Bacteria', nrow(fam.bact.pl.subset))
fam.bact.ce.subset <- add_cazy_class(fam.tax.bact.df, "CE", ce.names)
fam.bact.ce.subset$Tax_group <- rep('Bacteria', nrow(fam.bact.ce.subset))
fam.bact.aa.subset <- add_cazy_class(fam.tax.bact.df, "AA", aa.names)
fam.bact.aa.subset$Tax_group <- rep('Bacteria', nrow(fam.bact.aa.subset))
fam.bact.cbm.subset <- add_cazy_class(fam.tax.bact.df, "CBM", cbm.names)
fam.bact.cbm.subset$Tax_group <- rep('Bacteria', nrow(fam.bact.cbm.subset))

# add data for all tax classes
fam.tax.df <- data.frame(cazy_family_long_df)
fam.tax.df$Tax_group <- rep('All', nrow(cazy_family_long_df))

fam.tax.df <- rbind(fam.tax.df, fam.euk.gh.subset)
fam.tax.df <- rbind(fam.tax.df, fam.euk.gt.subset)
fam.tax.df <- rbind(fam.tax.df, fam.euk.pl.subset)
fam.tax.df <- rbind(fam.tax.df, fam.euk.ce.subset)
fam.tax.df <- rbind(fam.tax.df, fam.euk.aa.subset)
fam.tax.df <- rbind(fam.tax.df, fam.euk.cbm.subset)

fam.tax.df <- rbind(fam.tax.df, fam.bact.gh.subset)
fam.tax.df <- rbind(fam.tax.df, fam.bact.gt.subset)
fam.tax.df <- rbind(fam.tax.df, fam.bact.pl.subset)
fam.tax.df <- rbind(fam.tax.df, fam.bact.ce.subset)
fam.tax.df <- rbind(fam.tax.df, fam.bact.aa.subset)
fam.tax.df <- rbind(fam.tax.df, fam.bact.cbm.subset)

# for evaluation of multi label classification
# add taxonomic data to CAZy fam fam_classification_df
fam.tax.col <- c()
for (genome in fam_classification_df$Genomic_accession){
  if (genome %in% tax_grps$Bacteria){
    fam.tax.col <- append(fam.tax.col, "Bacteria")
  } else if (genome %in% tax_grps$Eukaryote){
    fam.tax.col <- append(fam.tax.col, "Eukaryote")
  } else {
    print(genome)
  }
}
cazy_fam_tax_df <- data.frame(fam_classification_df)
cazy_fam_tax_df$Tax_group <- fam.tax.col

bact.cazy_fam_tax_df <- cazy_fam_tax_df[which(cazy_fam_tax_df$Tax_group == "Bacteria"), ]
euk.cazy_fam_tax_df <- cazy_fam_tax_df[which(cazy_fam_tax_df$Tax_group == "Eukaryote"), ]
```

<div id="summary">
An independent and comprehensive evaluation of the CAZyme classifiers:
- dbCAN4
- CUPP
- HMMER (dbCAN4)
- dbCAN-sub (dbCAN4)
- DIAMOND (dbCAN4)

Evaluating performance of:
- Binary CAZyme/non-CAZyme classification
- Test set dependent performance of CAZyme/non-CAZyme classification
- Binary classification per CAZy class
- Multilabel CAZy class classification
- Binary classification per CAZy family
- Multilabel CAZy family classification
</div>

# CAZyme classifier references and names

The CAZyme classifier **dbCAN** is available as a webserver and a standalone tool. In this evaluation the standalone tool was used, and is referred to as **dbCAN**, references to the webserver are defined as the **dbCAN webserver**. The version numbers of the standalone tool and the webserver are independent of one another:
* The dbCAN2 webserver initally ran dbCAN version **2** (referred to as **dbCAN2**)
* The dbCAN2 webserver than implememented the standalone dbCAN version **3** (referred to as **dbCAN3**)
* The dbCAN3 webserver implements the standalone dbCAN version **4** (referred to as **dbCAN4**)

All references to implementing HMMER and DIAMOND refer to the implementation of these tools by dbCAN. For this evaluation, specifically the implementation of HMMER and DIAMOND by dbCAN4.
  
[**dbCAN2** and **dbCAN3**](https://doi.org/10.1093/nar/gky418)

Han Zhang and others, dbCAN2: a meta server for automated carbohydrate-active enzyme annotation, Nucleic Acids Research, Volume 46, Issue W1, 2 July 2018, Pages W95–W101

[**dbCAN4** and **dbCAN-sub (dbCAN)**](https://doi.org/10.1093/nar/gkad328)

Zheng J, Ge Q, Yan Y, Zhang X, Huang L, Yin Y. dbCAN3: automated carbohydrate-active enzyme and substrate annotation. Nucleic Acids Res. 2023 Jul 5;51(W1):W115-W121

[**eCAMI**](https://doi.org/10.1093/bioinformatics/btz908)

Xu J, Zhang H, Zheng J, Dovoedo P, Yin Y. eCAMI: simultaneous classification and motif identification for enzyme annotation. Bioinformatics. 2020 Apr 1;36(7):2068-2075

[**CUPP**](https://biotechnologyforbiofuels.biomedcentral.com/articles/10.1186/s13068-019-1436-5)

Barrett, K., Lange, L. Peptide-based functional annotation of carbohydrate-active enzymes by conserved unique peptide patterns (CUPP). Biotechnol Biofuels 12, 102 (2019). https://doi.org/10.1186/s13068-019-1436-5

[**HMMER**](https://academic.oup.com/bioinformatics/article/14/9/755/259550?login=false)

Eddy SR. Profile hidden Markov models. Bioinformatics. 1998;14(9):755-63.

[**DIAMOND**](https://www.nature.com/articles/nmeth.3176)

Buchfink, B., Xie, C. & Huson, D. Fast and sensitive protein alignment using DIAMOND. Nat Methods 12, 59–60 (2015). https://doi.org/10.1038/nmeth.3176

# Introduction

CAZyme classifiers analyse query protein sequence and predict CAZyme domains and associated CAZy family annotations. This enables exploratory analysis of CAZyme complements not presently catalogued in the CAZy database (www.cazy.org). Each CAZyme classifier implements a different method to predict CAZy family annotations.

We previously published an evaluation of dbCAN2, eCAMI and CUPP ([Hobbs et al., 2021](https://doi.org/10.6084/m9.figshare.14370836.v3)). Since then, two standalone versions of dbCAN have been released (dbCAN3 and dbCAN4). Additionally, the previous analysis was limited to 70 genomes, over weighted towards bacterial genomes. To address these points, we present here an independent and comprehensive evaluation of the CAZyme classifiers:
* dbCAN2 (v2.0.11)
* dbCAN3 (v3.0.7)
* dbCAN4 (v4.0.0)
* eCAMI (implemented by dbCAN_3 (3.0.7))
* CUPP (v???)
* HMMER (from dbCAN4)
* dbCAN-sub (from dbCAN4)
* DIAMOND (from dbCAN4)

Evaluating performance of:
* Binary CAZyme/non-CAZyme classification
* Test set dependent performance of CAZyme/non-CAZyme classification
* Binary classification per CAZy class
* Multilabel CAZy class classification
* Binary classification per CAZy family
* Multilabel CAZy family classification


> Hobbs, Emma E. M.; Gloster, Tracey M.; Chapman, Sean; Pritchard, Leighton (2021). Microbiology Society Annual Conference 2021. figshare. Poster. https://doi.org/10.6084/m9.figshare.14370836.v3

# Test sets

A single test set of 100 CAZymes and 100 non-CAZymes with the highest sequence similarity (rated by bit-score ratio) was created per genomic assembly selected to be included in the benchmark test set.

Choosing the 100 non-CAZymes with the highest sequence similarity was devised to increase the probability of causing confusion, to gather a better idea of the expected performance when using the classifiers. An equal number of CAZymes to non-CAZymes was selected to prevent over representation of one population over the other.

For inclusion of a genomic assembly for the creation of a test set, the assembly had to meet of all the following criteria:

- Contains at least 100 CAZymes
- Contains at least 100 non-CAZymes
- Has an 'Assembly level' of 'Complete Genome' in the NCBI Assembly database
- Protein records are still present in NCBI
- Not listed as an 'Anomalous assembly' in the NCBI Assembly database

The genomic assemblies were also chosen from a range of taxonomies to provide as informative image of the performance of the classifiers over a range of datasets that users may wish to analyse.

We took the 70 test sets used in the previous evaluation ([Hobbs et al., 2021](https://doi.org/10.6084/m9.figshare.14370836.v3)), and added an additional 10 genomes.

```{r cazomeCovStats, echo=FALSE}
mean_genome_cov = round(mean(test_set_cov_df$Genome_CAZome_percentage), digits=2)
print("Mean percentage of genome incorporated in the CAZome across all test sets:")
print(mean_genome_cov)

sd_genome_cov = round(sd(test_set_cov_df$Genome_CAZome_percentage), digits=2)
print("Standard deviation of the percentage of genome incorporated in the CAZome across all test sets:")
print(sd_genome_cov)

mean_cazome_cov = round(mean(test_set_cov_df$CAZome_coverage_percentage), digits=2)
print("Mean percentage of CAZomes incorporated in the test set across all genomes:")
print(mean_cazome_cov)

sd_cazome_cov = round(sd(test_set_cov_df$CAZome_coverage_percentage), digits=2)
print("Standard deviation of the percentage of CAZome incorporated in the test set across all genomes:")
print(sd_cazome_cov)
```
```{r cazomeCovHisto, echo=FALSE, results='asis' ,fig.cap="Histogram of CAZome coverage of the test sets for each respective source genomic assembly, overlayed by a box and whisker plot of the percentage of the CAZome incorproated in the test set."}
p.cazome.c <- ggplot(test_set_cov_df, aes(x=CAZome_coverage_percentage)) +
  geom_histogram(fill="#138d91", alpha=0.7, color="#0d4a4d") +
  geom_boxplot(
    outlier.shape=NA,
    width = 0.5,
    data=test_set_cov_df,
    aes(x=CAZome_coverage_percentage, y=5),
    alpha=0.5) +
  labs(x="CAZome coverage (%)", y="Number of test sets")
p.cazome.c
```

```{r saveCov, include=FALSE}
dir.create("../report/cazome_coverage", recursive=TRUE)
pdf(file = "../report/cazome_coverage/cazomeCovHis.pdf", width = 8.58, height = 5.5)
p.cazome.c
dev.off()
```

# CAZyme/non-CAZyme classification

The assignment of CAZy family annotations by a CAZyme classifier identifies the protein as a CAZyme. If no CAZy family annotations are assigned to a protein by a CAZyme classifier, the tool identified the protein as a non-CAZyme. Here we evaluate the performance of each CAZyme classifier to differentiate between CAZymes and non-CAZymes (defined as proteins catalogued and not catalogued in CAZy respectively).

## Summary statistics

For every classifier-test set pair, the specificity, sensitivity, prevision, F1-score and accuracy was calculated. The mean of each statistical parameter was calculated for each classifier across all tests, to represent the overall performance of each classifier. The 95% confidence interval (CI) was also calculated owing the tendancy of the mean to skew towards 1. These results are presented in table \@ref(tab:sumstats).

```{r funcCalcBinSumStats, include=FALSE}
get_binary_summary_stats <- function(stat_df){
  # Calculate statistics
  subset_spec <- stat_df[which(stat_df$Statistic_parameter == "Specificity"), ]
  binary_specificity <- subset_spec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Spec Mean"=mean(Statistic_value),
    "Spec Standard Deviation"=sd(Statistic_value),
    "Spec Lower CI"=CI(Statistic_value)[3],
    "Spec Upper CI"=CI(Statistic_value)[1]
  )
  
  subset_sens <- stat_df[which(stat_df$Statistic_parameter == "Sensitivity"), ]
  binary_sensitivity <- subset_sens %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Sens Mean"=mean(Statistic_value),
    "Sens Standard Deviation"=sd(Statistic_value),
    "Sens Lower CI"=CI(Statistic_value)[3],
    "Sens Upper CI"=CI(Statistic_value)[1]
  )
  
  subset_prec <- stat_df[which(stat_df$Statistic_parameter == "Precision"), ]
  binary_precision <- subset_prec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Prec Mean"=mean(Statistic_value),
    "Prec Standard Deviation"=sd(Statistic_value),
    "Prec Lower CI"=CI(Statistic_value)[3],
    "Prec Upper CI"=CI(Statistic_value)[1]
  )
  
  subset_f1 <- stat_df[which(stat_df$Statistic_parameter == "FBeta-score"), ]
  binary_f1_score <- subset_f1 %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "F1-score Mean"=mean(Statistic_value),
    "F1-score Standard Deviation"=sd(Statistic_value),
    "F1-score Lower CI"=CI(Statistic_value)[3],
    "F1-score Upper CI"=CI(Statistic_value)[1]
  )
  
  subset_acc <- stat_df[which(stat_df$Statistic_parameter == "Accuracy"), ]
  binary_accuracy <- subset_acc %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Acc Mean"=mean(Statistic_value),
    "Acc Standard Deviation"=sd(Statistic_value),
    "Acc Lower CI"=CI(Statistic_value)[3],
    "Acc Upper CI"=CI(Statistic_value)[1]
  )
  
  # combine data and build a single dataframe
  binary_summary_df <- merge(binary_specificity, binary_sensitivity)
  binary_summary_df <- merge(binary_summary_df, binary_precision)
  binary_summary_df <- merge(binary_summary_df, binary_f1_score)
  binary_summary_df <- merge(binary_summary_df, binary_accuracy)
  
  return(
    list(
      binary_summary_df,
      subset_spec,
      subset_sens,
      subset_prec,
      subset_f1,
      subset_acc
    )
  )
}
```

```{r sumstats, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of binary CAZyme/non-CAZyme prediction. Data collected is the mean of call calculated statistical parameters across all test sets, plus and minus the standard devliation. All figures are rounded to 4 decimal places. A confidence interval (CI) of 95% is presented. Spec, specificity; Sens, sensitivity; Prec, precision; Acc, accuracy"}

binary_dfs <- get_binary_summary_stats(binary_stat_df)
binary_summary_df <- binary_dfs[[1]]
subset_spec <- binary_dfs[[2]]
subset_sens <- binary_dfs[[3]]
subset_prec <- binary_dfs[[4]]
subset_f1 <- binary_dfs[[5]]
subset_acc <- binary_dfs[[6]]

# define factors
binary_summary_df$Prediction_tool <- factor(binary_summary_df$Prediction_tool, levels = classifiers_full) # set order data is presented

names(binary_summary_df)[names(binary_summary_df) == "Prediction_tool"] <- "Classifier"
# reorder the rows

# binary_summary_df <- binary_summary_df[c(2,5,6,3,1,4), ]
row.names(binary_summary_df) = NULL  # hides row names which are added by reordering the rows

dir.create("../report/cazyme_noncazyme_classification", recursive=FALSE) 
write.csv(binary_summary_df, "../report/cazyme_noncazyme_classification/summaryTable.csv", row.names=FALSE)

kable(
  binary_summary_df,
  caption="Overall performance of CAZyme classifiers differentiation between CAZymes and non-CAZymes",
  align='c',
  digits = 4
) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

The 95% CI was plotted as error bars around the mean CI (figure \@ref(fig:binaryCI)).

```{r binaryCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of binary CAZyme/non-CAZyme prediction. The mean plus and minus the 95% confidence interval."}
cal_tool_ci <- function(stat_subset){
  # calculate lower, upper and mean 95% CI
  UpperCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("UpperCI"=CI(Statistic_value)[1])
  MeanCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("MeanCI"=CI(Statistic_value)[2])
  LowerCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("LowerCI"=CI(Statistic_value)[3])

  ci_df <- merge(MeanCI, UpperCI)
  ci_df <- merge(ci_df, LowerCI)
  
  return(ci_df)
}

bin.spec.ci <- cal_tool_ci(subset_spec)
bin.spec.ci$Statistic_parameter <- rep('Specificity', nrow(bin.spec.ci))
bin.sens.ci <- cal_tool_ci(subset_sens)
bin.sens.ci$Statistic_parameter <- rep('Sensitivity', nrow(bin.sens.ci))
bin.prec.ci <- cal_tool_ci(subset_prec)
bin.prec.ci$Statistic_parameter <- rep('Precision', nrow(bin.prec.ci))
bin.f1.ci <- cal_tool_ci(subset_f1)
bin.f1.ci$Statistic_parameter <- rep('F1-score', nrow(bin.f1.ci))
bin.acc.ci <- cal_tool_ci(subset_acc)
bin.acc.ci$Statistic_parameter <- rep('Accuracy', nrow(bin.acc.ci))

ci_df <- rbind(bin.spec.ci, bin.sens.ci)
ci_df <- rbind(ci_df, bin.prec.ci)
ci_df <- rbind(ci_df, bin.f1.ci)
ci_df <- rbind(ci_df, bin.acc.ci)

ci_df$Prediction_tool <- factor(ci_df$Prediction_tool, levels = classifiers_full)
ci_df$Statistic_parameter <- factor(ci_df$Statistic_parameter, levels = c("Specificity", "Sensitivity", "Precision", "F1-score", "Accuracy"))

binary.CI = ggplot(ci_df %>% dplyr::group_by(Statistic_parameter),
                aes(x=Prediction_tool, y=MeanCI, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  scale_colour_manual(values = colour_set_tools) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=11,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Statistical value") +
  facet_wrap(~ Statistic_parameter, ncol=2)
binary.CI
```

```{r saveCnCCi, include=FALSE}
dir.create("../report/cazyme_noncazyme_classification", recursive=FALSE)
pdf(
  file = "../report/cazyme_noncazyme_classification/statsConfidenceIntervals.pdf",
  width = 6.3,
  height = 8.2
)
binary.CI
dev.off()
```

## Specificity

Specificity is the proportion of known negatives (known non-CAZymes) which are correctly classified as negatives (non-CAZymes). Figure \@ref(fig:spec) is a graphical representation of the results calculated in table \@ref(tab:sumstats).

```{r spec, echo=FALSE, fig.cap="One-dimensional scatter plot of specificity scores of CAZyme and non-CAZyme predictions per test set, overlaying box plot of standard deviation."}

subset_spec$Prediction_tool <- factor(subset_spec$Prediction_tool, levels = classifiers_full) # set order data is presented

p.binary.spec = ggplot(subset_spec %>% dplyr::group_by(Prediction_tool),
                aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set_tools) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90)) +
  xlab("Classifier") + 
  ylab("Specificity")
p.binary.spec
```

```{r savebinarySpec, include=FALSE}
pdf(file = "../report/cazyme_noncazyme_classification/CnCSpecificity.pdf", width = 8.58, height = 5.5)
p.binary.spec
dev.off()
```

```{r binarySpecAnova, echo=FALSE}
subset_spec.df <- data.frame(subset_spec)
subset_spec.df$Prediction_tool <- as.factor(subset_spec.df$Prediction_tool)
subset_spec.df <- subset_spec.df[complete.cases(subset_spec.df), ]

bin.spec.anova <- aov(Statistic_value ~ Prediction_tool, data = subset_spec.df)
summary(bin.spec.anova)
```
Test for statistically significant differences between the main classifiers.

```{r binarySpecMainAnova, echo=FALSE}
subset_spec.df.main <- subset_spec.df[subset_spec.df$Prediction_tool %in% main_classifiers, ]
subset_spec.df.main$Prediction_tool <- as.factor(subset_spec.df.main$Prediction_tool)

bin.spec.main.anova <- aov(Statistic_value ~ Prediction_tool, data = subset_spec.df.main)
summary(bin.spec.main.anova)
```


## Sensitivity

Sensitivity (also known as recall) is the proportion of known positives (CAZymes) that are correctly identified as positives (CAZymes). Figure \@ref(fig:recallbc) graphically represents of the results calculated in table \@ref(tab:sumstats).

```{r recallbc, echo=FALSE, fig.cap="One-dimensional scatter plot of recall (sensitivity) scores of CAZyme and non-CAZyme predictions per test set, overlaying box plot of standard deviation."}

subset_sens$Prediction_tool <- factor(subset_sens$Prediction_tool, levels = classifiers_full) # set order data is presented

p.binary.sens = ggplot(subset_sens %>% dplyr::group_by(Prediction_tool),
                  aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set_tools) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90)) +
  xlab("Classifier") + 
  ylab("Sensitivity")
p.binary.sens
```

```{r saveBinSens, include=FALSE}
pdf(file = "../report/cazyme_noncazyme_classification/CnCSensitivity.pdf", width = 8.58, height = 5.5)
p.binary.sens
dev.off()
```

```{r binarySensAnova, echo=FALSE}
subset_sens.df <- data.frame(subset_sens)
subset_sens.df$Prediction_tool <- as.factor(subset_sens.df$Prediction_tool)
subset_sens.df <- subset_sens.df[complete.cases(subset_sens.df), ]

bin.sens.anova <- aov(Statistic_value ~ Prediction_tool, data = subset_sens.df)
summary(bin.sens.anova)
```
Having found a statistically significant difference, use a Tukey HSD test to determine between which tools the means are significantly different.

```{r binarySensMainTurkey1, echo=FALSE}
bin.sens.turkey <- TukeyHSD(bin.sens.anova, conf.level=.95)
write.csv(
  as.data.frame(bin.sens.turkey$Prediction_tool),
  "../report/cazyme_noncazyme_classification/binary.sens.tukeyHSD.csv"
)
bin.sens.turkey
```

Test to see if the mean sensitivity is significantly different between the main classifiers.

```{r binarySensMainAnova, echo=FALSE}
subset_sens.df.main <- subset_sens.df[subset_sens.df$Prediction_tool %in% main_classifiers, ]
subset_sens.df.main$Prediction_tool <- as.factor(subset_sens.df.main$Prediction_tool)

bin.sens.main.anova <- aov(Statistic_value ~ Prediction_tool, data = subset_sens.df.main)
summary(bin.sens.main.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r binarySensMainTurkey, echo=FALSE}
bin.sens.turkey <- TukeyHSD(bin.sens.main.anova, conf.level=.95)
write.csv(
  as.data.frame(bin.sens.turkey$Prediction_tool),
  "../report/cazyme_noncazyme_classification/binary.sens.main.classifiers.tukeyHSD.csv"
)
bin.sens.turkey
```

## Precision

Precision is the proportion of positive predictions by the classifiers that are correct. In this case, precision represents the fraction of CAZyme predictions by the classifiers that are correct, specifically the proportion of predicted CAZymes that are known CAZymes. Figure \@ref(fig:precbc) is a visual representation of the results calculated in table \@ref(tab:sumstats).

```{r precbc, echo=FALSE, fig.cap="One-dimensional scatter plot of precision scores of CAZyme and non-CAZyme predictions per test set, overlaying box plot of standard deviation."}

subset_prec$Prediction_tool <- factor(subset_prec$Prediction_tool, levels = classifiers_full) # set order data is presented

p.binary.prec = ggplot(subset_prec %>% dplyr::group_by(Prediction_tool),
                       aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set_tools) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90)) +
  xlab("Classifier") + 
  ylab("Precision")
p.binary.prec
```

```{r saveBinPrec, include=FALSE}
pdf(file = "../report/cazyme_noncazyme_classification/CnCPrecision.pdf", width = 8.58, height = 5.5)
p.binary.prec
dev.off()
```

```{r binaryPrecAnova, echo=FALSE}
subset_prec.df <- data.frame(subset_prec)
subset_prec.df$Prediction_tool <- as.factor(subset_prec.df$Prediction_tool)
subset_prec.df <- subset_prec.df[complete.cases(subset_prec.df), ]

bin.prec.anova <- aov(Statistic_value ~ Prediction_tool, data = subset_prec.df)
summary(bin.prec.anova)
```
```{r binaryPrecMainAnova, echo=FALSE}
subset_prec.df.main <- subset_prec.df[subset_prec.df$Prediction_tool %in% main_classifiers, ]
subset_prec.df.main$Prediction_tool <- as.factor(subset_prec.df.main$Prediction_tool)

bin.prec.main.anova <- aov(Statistic_value ~ Prediction_tool, data = subset_prec.df.main)
summary(bin.prec.main.anova)
```

## F1-score

The F1-score is a harmonic (or weighted) average of recall and precision and provides an idea of the overall performance of the tool, 0 being the lowest and 1 being the best performance. Figure \@ref(fig:f1bc) shows the F1-score from each test set, for each classifier.

```{r f1bc, echo=FALSE, fig.cap="Bar chart of specificity of CAZyme classifiers differentiation between CAZymes and non-CAZymes."}

subset_f1$Prediction_tool <- factor(subset_f1$Prediction_tool, levels = classifiers_full) # set order data is presented

p.binary.f1 = ggplot(subset_f1 %>% dplyr::group_by(Prediction_tool),
              aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set_tools) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90)) +
  xlab("Classifier") + 
  ylab("F1-score")
p.binary.f1
```

```{r saveBinF1, include=FALSE}
pdf(file = "../report/cazyme_noncazyme_classification/CnCF1.pdf", width = 8.58, height = 5.5)
p.binary.f1
dev.off()
```

A one-way ANOVA was performed to determine if the mean F1-scores were statistically significantly different between the tools.

```{r binaryF1Anova, echo=FALSE}
subset_f1.df <- data.frame(subset_f1)
subset_f1.df$Prediction_tool <- as.factor(subset_f1.df$Prediction_tool)
# subset_f1.df$Prediction_tool <- factor(subset_f1.df$Prediction_tool, levels = classifiers_full)

subset_f1.df <- subset_f1.df[complete.cases(subset_f1.df), ]

bin.f1.anova <- aov(Statistic_value ~ Prediction_tool, data = subset_f1.df)
summary(bin.f1.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r binaryF1Turkey, echo=FALSE}
bin.f1.turkey <- TukeyHSD(bin.f1.anova, conf.level=.95)
write.csv(
  as.data.frame(bin.f1.turkey$Prediction_tool),
  "../report/cazyme_noncazyme_classification/binary.f1.tukeyHSD.csv"
)
bin.f1.turkey
```

A one-way ANOVA test was used to see if the mean F1-scores are significantly different between the main classifiers.

```{r binaryF1MainAnova, echo=FALSE}
subset_f1.df.main <- subset_f1.df[subset_f1.df$Prediction_tool %in% main_classifiers, ]
subset_f1.df.main$Prediction_tool <- as.factor(subset_f1.df.main$Prediction_tool)

bin.f1.main.anova <- aov(Statistic_value ~ Prediction_tool, data = subset_f1.df.main)
summary(bin.f1.main.anova)
```
The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r binaryF1MainTurkey, echo=FALSE}
bin.f1.turkey <- TukeyHSD(bin.f1.main.anova, conf.level=.95)
write.csv(
  as.data.frame(bin.f1.turkey$Prediction_tool),
  "../report/cazyme_noncazyme_classification/binary.f1.main.classifiers.tukeyHSD.csv"
)
bin.f1.turkey
```


## Accuracy

Accuracy (calculated using (TP + TN) / (TP + TN + FP + FN) ) provides an idea of the overall performance of the classifiers as a measure of the degree to which their CAZyme/non-CAZyme predictions conforms to the correct result. Figure \@ref(fig:accbc) is a plot of respective data from table \@ref(tab:sumstats).


```{r accbc, echo=FALSE, fig.cap="Bar chart of specificity of CAZyme classifiers differentiation between CAZymes and non-CAZymes."}
subset_acc$Prediction_tool <- factor(subset_acc$Prediction_tool, levels = classifiers_full) # set order data is presented

p.binary.acc = ggplot(subset_acc %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set_tools) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90)) +
  xlab("Classifier") + 
  ylab("Accuracy") 
p.binary.acc
```

```{r saveBinAcc, include=FALSE}
pdf(file = "../report/cazyme_noncazyme_classification/CnCAccuracy.pdf", width = 8.58, height = 5.5)
p.binary.acc
dev.off()
```

Test if the means are significantly different between the tools.

```{r binaryaccAnova, echo=FALSE}
subset_acc.df <- data.frame(subset_acc)
subset_acc.df$Prediction_tool <- as.factor(subset_acc.df$Prediction_tool)
# subset_acc.df$Prediction_tool <- factor(subset_acc.df$Prediction_tool, levels = classifiers_full)

subset_acc.df <- subset_acc.df[complete.cases(subset_acc.df), ]

bin.acc.anova <- aov(Statistic_value ~ Prediction_tool, data = subset_acc.df)
summary(bin.acc.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r binaryaccTurkey, echo=FALSE}
bin.acc.turkey <- TukeyHSD(bin.acc.anova, conf.level=.95)
write.csv(
  as.data.frame(bin.acc.turkey$Prediction_tool),
  "../report/cazyme_noncazyme_classification/binary.acc.tukeyHSD.csv"
)
bin.acc.turkey
```

Test if the means are significantly different between the main classifiers.

```{r binaryaccMainAnova, echo=FALSE}
subset_acc.df.main <- subset_acc.df[subset_acc.df$Prediction_tool %in% main_classifiers, ]
subset_acc.df.main$Prediction_tool <- as.factor(subset_acc.df.main$Prediction_tool)

bin.acc.main.anova <- aov(Statistic_value ~ Prediction_tool, data = subset_acc.df.main)
summary(bin.acc.main.anova)
```
The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r binaryaccMainTurkey, echo=FALSE}
bin.acc.turkey <- TukeyHSD(bin.acc.main.anova, conf.level=.95)
write.csv(
  as.data.frame(bin.acc.turkey$Prediction_tool),
  "../report/cazyme_noncazyme_classification/binary.acc.main.classifiers.tukeyHSD.csv"
)
bin.acc.turkey
```

## Combined statistics plot

Here we generate a plot that combines the plots from above into a single figure.

```{r binaryCombo, echo=FALSE, fig.cap="Box and whisker plots of the performance of CAZyme/non-CAZyme classification."}
# repeat but with facet wrap so is neater
bin.mean.df <- rbind(subset_spec, subset_sens)
bin.mean.df <- rbind(bin.mean.df, subset_prec)
bin.mean.df <- rbind(bin.mean.df, subset_f1)
bin.mean.df <- rbind(bin.mean.df, subset_acc)

# replace 'FBeta-score' with 'F1-score'
bin.mean.df[bin.mean.df == 'FBeta-score'] <- 'F1-score'

bin.mean.df$Prediction_tool <- factor(bin.mean.df$Prediction_tool, levels = classifiers_full)
bin.mean.df$Statistic_parameter <- factor(bin.mean.df$Statistic_parameter, levels = c("Specificity", "Sensitivity", "Precision", "F1-score", "Accuracy"))

binary.mean.p = ggplot(bin.mean.df %>% dplyr::group_by(Statistic_parameter),
                aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0, size=0.3) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  scale_fill_manual(values = colour_set_tools) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=11,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Statistical value") +
  facet_wrap(~ Statistic_parameter, ncol=2)
binary.mean.p
```

```{r saveBinMuliPlot, include=FALSE}
pdf("../report/cazyme_noncazyme_classification/CnCmultiplot.pdf", width=6.3, height=8)
binary.mean.p
dev.off()
```

## ROC curve - Receiver Operator Characteristic curve

The Receiver Operator Characteristic (ROC) curve, which enables us to compare sensitivity to specificity but plotting sensitivity versus 1-specificity.

```{r rocPlotcreation, echo=FALSE}
# iterate files in dir
# reitreve predictions
# gt in column called CAZy
# combine into single DF
roc.df <- data.frame(matrix(ncol = 7, nrow = 0))
colnames(roc.df) <- c("Genomic_accession", "CUPP", "dbCAN_4",	"dbCAN_4.HMMER", "dbCAN_4.DIAMOND", "dbCAN_4.dbCAN-sub", "CAZy")

files <- list.files("../results/cazyme_classifications/predictions")
files = paste0("../results/cazyme_classifications/predictions/", files)


for (filename in files){
  if (grepl("2023_07_29", filename, fixed=TRUE)){
      df<-read.csv(filename, header = TRUE)
      
      roc.df <- rbind(roc.df, df[,c("Genomic_accession", "CUPP", "dbCAN_4", "dbCAN_4.HMMER", "dbCAN_4.DIAMOND", "dbCAN_4.dbCAN.sub", "CAZy")])
}}
nrow(roc.df)

colnames(roc.df) <- c("Genomic_accession", "CUPP", "dbCAN_4",	"dbCAN_4:HMMER", "dbCAN_4:DIAMOND", "dbCAN_4:dbCAN-sub", "CAZy")

get.roc.job <- function(tool, FUNC.df){
  ground.truths <- FUNC.df$CAZy
  predictions <- FUNC.df[,tool]
  roc.model <- roc(ground.truths, predictions)
  auc <- round(auc(ground.truths, predictions),4)
  print(paste0(tool, ' ROC Curve ', '(AUC = ', auc, ')'))
  return(roc.model)
}
# "dbCAN_4",	"dbCAN_4:HMMER", "dbCAN_4:DIAMOND", "dbCAN_4:dbCAN-sub"
roc.job.cupp <- get.roc.job("CUPP", roc.df)
roc.job.dbcan <- get.roc.job("dbCAN_4", roc.df)
roc.job.dbcan.hmmer <- get.roc.job("dbCAN_4:HMMER", roc.df)
roc.job.dbcan.diamond <- get.roc.job("dbCAN_4:DIAMOND", roc.df)
roc.job.dbcan.sub <- get.roc.job("dbCAN_4:dbCAN-sub", roc.df)

g2 <- ggroc(
  list(
    `dbCAN_4`=roc.job.dbcan,
    `dbCAN_4:HMMER`=roc.job.dbcan.hmmer,
    `dbCAN_4:DIAMOND`=roc.job.dbcan.diamond,
    `dbCAN_4:dCAN-sub`=roc.job.dbcan.sub,
    CUPP=roc.job.cupp
  ), size = 1
) + scale_colour_manual(values = colour_set_tools)
g2
```

```{r saveBinROC, include=FALSE}

pdf("../report/cazyme_noncazyme_classification/roc.pdf", width=8, height=4.5)
g2
dev.off()

```


## Expected Range of Accuracy

The statistics evaluated above provide an idea of the general performance of the tools, but they do not provide an idea of the expect range of performance. Specifically, the data does not provide a clear image of the best and worse performance a user can expect when using these tools.

To compare the expected typical range in accuracies for each classifier, 6 test sets (identified by the source genomic assemblies) were selected at random. The CAZyme/non-CAZyme predictions for each classifier, for each test set, were bootstrap resampled 100 times each, and for each bootstrap sample the accuracy calculated. The accuracies of the bootstrap samples for each classifier were plotted on stacked histograms, shown in figure \@ref(fig:bsacc).

```{r bsacc, echo=FALSE, fig.cap="Stacked histograms of bootstrap sample accuracies of CAZyme classifiers' differentiation between CAZymes and non-CAZymes. 6 test sets (identified by their source genomic assembly) were selected at random. The CAZyme/non-CAZyme predictions for each classifier, for each test set, were bootstrap resampled 100 times. The accuracy of each of the 600 bootstrap samples per test set were plotted as a stacked histogram."}
bootstrap_results_df$Prediction_tool <- factor(bootstrap_results_df$Prediction_tool, levels = classifiers_full) # set order data is presented

# plot generated to see the number of items in each bin for the figure above, and additional boxplots to represent the distribution of the data
p.bs.annotated = ggplot(bootstrap_results_df %>% dplyr::group_by(Genomic_accession), aes(x=accuracy)) +
  geom_histogram(alpha=0.7, color="black", binwidth = 0.01, aes(fill=Genomic_accession), position = ) +
  geom_boxplot(outlier.shape=NA, width = 10, data=bootstrap_results_df, aes(x=accuracy, y=100), alpha=0.5) +
  geom_vline(xintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  geom_vline(xintercept=0.95, linetype="dashed", color = "#5c5c5c") +
  scale_fill_brewer(palette="Set1") +
  # stat_bin(aes(y=..count.. + 5, label=..count..), geom="text", binwidth = 0.01, size=3.5, angle=90) +
  theme(plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        strip.text = element_text(size=12),
        legend.position="bottom") +
  labs(x = "Accuracy", y = "Number of bootstrap samples", fill="Test set key") +
  # scale_y_continuous(breaks = seq(0,125, by = 25)) +
  # scale_x_continuous(breaks = seq(0.75,1, by = 0.05)) +
  facet_wrap(~ Prediction_tool, ncol=2) 
  # remove the comment if you want to see the histogram with the total number per column printed
p.bs.annotated
```

```{r saveBootstrap, include=FALSE}
pdf(file = "../report/cazyme_noncazyme_classification/BootstrapAccuracy.pdf", width = 8.25, height = 11)
p.bs.annotated
dev.off()
```

```{r bsacc2, include=FALSE}
bootstrap_results_df$Prediction_tool <- factor(bootstrap_results_df$Prediction_tool, levels = classifiers_full) # set order data is presented

# plot generated to see the number of items in each bin for the figure above, and additional boxplots to represent the distribution of the data
p.bs.annotated.landscape = ggplot(bootstrap_results_df %>% dplyr::group_by(Genomic_accession), aes(x=accuracy)) +
  geom_histogram(alpha=0.7, color="black", binwidth = 0.01, aes(fill=Genomic_accession), position = ) +
  geom_boxplot(outlier.shape=NA, width = 10, data=bootstrap_results_df, aes(x=accuracy, y=100), alpha=0.5) +
  geom_vline(xintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  geom_vline(xintercept=0.95, linetype="dashed", color = "#5c5c5c") +
  scale_fill_brewer(palette="Set1") +
  # stat_bin(aes(y=..count.. + 5, label=..count..), geom="text", binwidth = 0.01, size=3.5, angle=90) +
  theme(plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        strip.text = element_text(size=12),
        legend.position="bottom") +
  labs(x = "Accuracy", y = "Number of bootstrap samples", fill="Test set key") +
  # scale_y_continuous(breaks = seq(0,125, by = 25)) +
  # scale_x_continuous(breaks = seq(0.75,1, by = 0.05)) +
  facet_wrap(~ Prediction_tool, ncol=4) 
  # remove the comment if you want to see the histogram with the total number per column printed
p.bs.annotated.landscape
```

```{r saveBootstrap2, include=FALSE}
pdf(file = "../report/cazyme_noncazyme_classification/BootstrapAccuracyLandscape.pdf", width = 11, height = 8.25)
p.bs.annotated.landscape
dev.off()
```

Plot as box and whisker plots.

```{r BootstrapBox, echo=FALSE}

bootstrap_results_df$Genomic_accession <- gsub("GCA_000092285.1", "Bacteria - Caulobacter segnis", bootstrap_results_df$Genomic_accession)
bootstrap_results_df$Genomic_accession <- gsub("GCA_005221905.1", "Bacteria - Escherichia coli", bootstrap_results_df$Genomic_accession)
bootstrap_results_df$Genomic_accession <- gsub("GCA_009936315.1", "Bacteria - Streptomyces antimycoticus", bootstrap_results_df$Genomic_accession)

bootstrap_results_df$Genomic_accession <- gsub("GCA_001592805.2", "Eukaryote - Peltaster fructicola", bootstrap_results_df$Genomic_accession)
bootstrap_results_df$Genomic_accession <- gsub("GCA_009498115.1", "Eukaryote - Clavispora lusitaniae", bootstrap_results_df$Genomic_accession)
bootstrap_results_df$Genomic_accession <- gsub("GCA_016861735.1", "Eukaryote - Aspergillus chevalieri", bootstrap_results_df$Genomic_accession)


bootstrap_results_df$Genomic_accession <- factor(bootstrap_results_df$Genomic_accession, levels=c(
  "Bacteria - Caulobacter segnis", "Bacteria - Escherichia coli", "Bacteria - Streptomyces antimycoticus", "Eukaryote - Peltaster fructicola",
  "Eukaryote - Clavispora lusitaniae", "Eukaryote - Aspergillus chevalieri"
))

binary.boot.bp.p = ggplot(bootstrap_results_df %>% dplyr::group_by(Prediction_tool),
                aes(x=Genomic_accession, y=accuracy)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0, size=0.3) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  scale_fill_manual(values = colour_set_tools) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=11,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Test set") + 
  ylab("Accuracy") +
  facet_wrap(~ Prediction_tool, ncol=2)
binary.boot.bp.p
```
```{r saveBootstrapBP, include=FALSE}
pdf(file = "../report/cazyme_noncazyme_classification/BootstrapAccuracyBOXPLOT.pdf", width = 8.25, height = 11)
binary.boot.bp.p
dev.off()
```





## Conclusions on the Binary CAZyme/non-CAZyme Prediction Performance

Overall, all tools showed a low probability of producing false positives (missclassifying a non-CAZyme as a CAZyme), and few of the positive predictions are false positives. Therefore, we can be confident in that the CAZyme predictions made by each of these tools are most likely correct. However, all the classifiers demonstrated a consistent behaviour to not identify all CAZymes within a CAZome. Therefore, we can be confident in the CAZyme predictions, but should not presume all non-CAZyme predictions are correct; these classifiers are unlikely to identify the complete CAZome although a near-complete CAZome will be accurately identified.

# CAZyme/non-CAZyme classification: Taxonomic evaluation

The performance for a classifier per taxonomy group may vary. For this evaluation the test sets were separated into the taxonomy groups:
- Bacteria
- Eukaryote

The evaluation per classifier per taxonomy group, versus all test sets pooled together was evaluated.

```{r binTaxGrpFuncs, include=FALSE}
build.bin.tax.df <- function(bin.stat.df, stat.param){
	tax_binary_stat_df <- bin.stat.df[which(bin.stat.df$Statistic_parameter == stat.param), ]
	
	tax_binary_stat_df.bact <- tax_binary_stat_df[which(tax_binary_stat_df$Tax_group == "Bacteria"), ]
	tax_binary_stat_df.euk <- tax_binary_stat_df[which(tax_binary_stat_df$Tax_group == "Eukaryote"), ]
	
	# create dataframe with tax_group set as "All"
	all.vector <- rep("All", nrow(tax_binary_stat_df))
	all.tax_binary_stat_df <- tax_binary_stat_df
	all.tax_binary_stat_df$Tax_group <- all.vector
	
	bact.tax_binary_stat_mean_sd <- tax_binary_stat_df.bact %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
	  "Bact Mean"=mean(Statistic_value), "Bact Standard Deviation"=sd(Statistic_value),
	  "Bact Lower CI"=CI(Statistic_value)[3],
	  "Bact Upper CI"=CI(Statistic_value)[1]
	  )
	euk.tax_binary_stat_mean_sd <- tax_binary_stat_df.euk %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
	  "Euk Mean"=mean(Statistic_value), "Euk Standard Deviation"=sd(Statistic_value),
	  "Euk Lower CI"=CI(Statistic_value)[3],
	  "Euk Upper CI"=CI(Statistic_value)[1]
	  )
	all.tax_binary_stat_mean_sd <- all.tax_binary_stat_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
	  "All Mean"=mean(Statistic_value), "All Standard Deviation"=sd(Statistic_value),
	  "All Lower CI"=CI(Statistic_value)[3],
	  "All Upper CI"=CI(Statistic_value)[1]
	  )

	tax_binary_stat_mean_sd <- bact.tax_binary_stat_mean_sd

	tax_binary_stat_mean_sd$`Euk Mean` <- euk.tax_binary_stat_mean_sd$`Euk Mean`
	tax_binary_stat_mean_sd$`Euk Standard Deviation` <- euk.tax_binary_stat_mean_sd$`Euk Standard Deviation`
	tax_binary_stat_mean_sd$`Euk Lower CI` <- euk.tax_binary_stat_mean_sd$`Euk Lower CI`
	tax_binary_stat_mean_sd$`Euk Upper CI` <- euk.tax_binary_stat_mean_sd$`Euk Upper CI`

	tax_binary_stat_mean_sd$`All Mean` <- all.tax_binary_stat_mean_sd$`All Mean`
	tax_binary_stat_mean_sd$`All Standard Deviation` <- all.tax_binary_stat_mean_sd$`All Standard Deviation`
	tax_binary_stat_mean_sd$`All Lower CI` <- all.tax_binary_stat_mean_sd$`All Lower CI`
	tax_binary_stat_mean_sd$`All Upper CI` <- all.tax_binary_stat_mean_sd$`All Upper CI`

	tax_binary_stat_mean_sd$Prediction_tool <- factor(tax_binary_stat_mean_sd$Prediction_tool, levels=classifiers_full) # set order data is presented
	
	return(tax_binary_stat_mean_sd)
}

build.bin.tax.p <- function(tax_binary_stat_mean_sd, stat.val){
	t.b.means <- tax_binary_stat_mean_sd$`Bact Mean`
	t.b.means <- append(tax_binary_stat_mean_sd$`Euk Mean`, t.b.means)
	t.b.means <- append(tax_binary_stat_mean_sd$`All Mean`, t.b.means)

	t.b.lower.cis <- tax_binary_stat_mean_sd$`Bact Lower CI`
	t.b.lower.cis <- append(tax_binary_stat_mean_sd$`Euk Lower CI`, t.b.lower.cis)
	t.b.lower.cis <- append(tax_binary_stat_mean_sd$`All Lower CI`, t.b.lower.cis)

	t.b.upper.cis <- tax_binary_stat_mean_sd$`Bact Upper CI`
	t.b.upper.cis <- append(tax_binary_stat_mean_sd$`Euk Upper CI`, t.b.upper.cis)
	t.b.upper.cis <- append(tax_binary_stat_mean_sd$`All Upper CI`, t.b.upper.cis)

	t.b.tools <- tax_binary_stat_mean_sd$Prediction_tool
	t.b.tool.names <- c()
	for (t in t.b.tools){
	  t.b.tool.names <- append(t.b.tool.names, t)
	}

	t.b.taxonomic_group <- rep('Bacteria', length(classifiers_full))
	t.b.taxonomic_group <- append(t.b.taxonomic_group, rep('Eukaryote', length(classifiers_full)))
	t.b.taxonomic_group <- append(t.b.taxonomic_group, rep('All', length(classifiers_full)))

	ci.tax.bin.df <- data.frame(t.b.means, t.b.lower.cis, t.b.upper.cis, t.b.tool.names, t.b.taxonomic_group)
	colnames(ci.tax.bin.df) <- c('MeanCI', 'LowerCI', 'UpperCI', 'Prediction_tool', 'Taxonomic_group')

	ci.tax.bin.df$Prediction_tool <- factor(ci.tax.bin.df$Prediction_tool, levels = classifiers_full)

	p.t.b.ci <- ggplot(ci.tax.bin.df %>% dplyr::group_by(Taxonomic_group),
				  aes(x=Taxonomic_group, y=MeanCI, color=Taxonomic_group)) +
	  geom_point() +
	  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
	  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
	  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
	  scale_colour_manual(values = colour_set) +
	  theme(legend.position = "none",
			plot.background = element_rect(fill = figbg, color = figbg),
			axis.text=element_text(size=10),
			axis.title=element_text(size=12,face="bold")) +
	  xlab("Classifier") + 
	  ylab(stat.val) +
	  facet_wrap(~ Prediction_tool, ncol=2)
	  
	return(p.t.b.ci)
}

build.bin.tax.p.scatter <- function(bin.stat.df, stat.param){
	tax_binary_stat_df <- bin.stat.df[which(bin.stat.df$Statistic_parameter == stat.param), ]
	tax_binary_stat_df.all <- data.frame(tax_binary_stat_df)
	tax_binary_stat_df.all$Tax_group <- rep('All', nrow(tax_binary_stat_df.all))
	tax_binary_stat_df <- rbind(tax_binary_stat_df, tax_binary_stat_df.all)
	
	bin.stat.scat.p = ggplot(tax_binary_stat_df %>% dplyr::group_by(Prediction_tool),
						 aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
		geom_boxplot(outlier.shape=NA) +
		geom_jitter(width=0.1, height=0, size=0.3) +
		geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
		geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
		scale_fill_manual(values = colour_set) +
		theme(legend.position = "none",
			  plot.background = element_rect(fill = figbg, color = figbg),
			  axis.text=element_text(size=10),
			  axis.title=element_text(size=11,face="bold")) +
		xlab("Kingdom") + 
		ylab(stat.param) +
		facet_wrap(~ Prediction_tool, ncol=2)
	
	return(bin.stat.scat.p)
}

convert.wide.to.tidy.df <- function(tax_binary_stat_mean_sd){
	t.b.means <- tax_binary_stat_mean_sd$`Bact Mean`
	t.b.means <- append(tax_binary_stat_mean_sd$`Euk Mean`, t.b.means)
	t.b.means <- append(tax_binary_stat_mean_sd$`All Mean`, t.b.means)

	t.b.lower.cis <- tax_binary_stat_mean_sd$`Bact Lower CI`
	t.b.lower.cis <- append(tax_binary_stat_mean_sd$`Euk Lower CI`, t.b.lower.cis)
	t.b.lower.cis <- append(tax_binary_stat_mean_sd$`All Lower CI`, t.b.lower.cis)

	t.b.upper.cis <- tax_binary_stat_mean_sd$`Bact Upper CI`
	t.b.upper.cis <- append(tax_binary_stat_mean_sd$`Euk Upper CI`, t.b.upper.cis)
	t.b.upper.cis <- append(tax_binary_stat_mean_sd$`All Upper CI`, t.b.upper.cis)

	t.b.tools <- tax_binary_stat_mean_sd$Prediction_tool
	t.b.tool.names <- c()
	for (t in t.b.tools){
	  t.b.tool.names <- append(t.b.tool.names, t)
	}

	t.b.taxonomic_group <- rep('Bacteria', length(classifiers_full))
	t.b.taxonomic_group <- append(t.b.taxonomic_group, rep('Eukaryote', length(classifiers_full)))
	t.b.taxonomic_group <- append(t.b.taxonomic_group, rep('All', length(classifiers_full)))

	ci.tax.bin.df <- data.frame(t.b.means, t.b.lower.cis, t.b.upper.cis, t.b.tool.names, t.b.taxonomic_group)
	colnames(ci.tax.bin.df) <- c('MeanCI', 'LowerCI', 'UpperCI', 'Prediction_tool', 'Taxonomic_group')

	ci.tax.bin.df$Prediction_tool <- factor(ci.tax.bin.df$Prediction_tool, levels = classifiers_full)
	
	return(ci.tax.bin.df)
}
```

## Specificity 

```{r binSpecTax, echo=FALSE}
bin.spec.tax <- build.bin.tax.df(binary_tax_df, "Specificity")

dir.create("../report/cazyme_noncazyme_classification/tax_performance", recursive=FALSE)
write.csv(bin.spec.tax, "../report/cazyme_noncazyme_classification/tax_performance/specificity.sum.table.csv", row.names=FALSE)

kable(bin.spec.tax, caption="The specificity of binary CAZyme/non-CAZyme classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13), border_right=TRUE)
```

```{r binSpecTaxCi, echo=FALSE}
p.bin.spec.tax.ci <- build.bin.tax.p(bin.spec.tax, "Specificity")
p.bin.spec.tax.ci
```

```{r binSpecTaxScatter, echo=FALSE}
p.bin.spec.tax.scatter <- build.bin.tax.p.scatter(binary_tax_df, "Specificity")
p.bin.spec.tax.scatter
```


```{r binSpecTaxCiSave, include=FALSE}
pdf(file = "../report/cazyme_noncazyme_classification/tax_performance/specificity.ci.pdf",  width = 6.23, height = 4)
p.bin.spec.tax.ci
dev.off()

pdf(file = "../report/cazyme_noncazyme_classification/tax_performance/specificity.scatter.pdf",  width = 6.23, height = 8.2)
p.bin.spec.tax.scatter
dev.off()
```

Test if there is a statistically significant difference between the means of the tools and kingdoms using a two-way ANOVA.

```{r binSpecTwoANOVA, echo=FALSE}
bin.tax.spec.2.aov <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = binary_tax_df[binary_tax_df$Statistic_parameter=='Specificity',])
write.csv(
  summary(bin.tax.spec.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazyme_noncazyme_classification/tax_performance/binary.spec.two-way-ANOVA.csv"
)
summary(bin.tax.spec.2.aov)
```
Statistically significant difference was detected so follow up with a Tukey test to determine if there are differences between specific groups.

```{r}
bin.tax.spec.tukey <- TukeyHSD(bin.tax.spec.2.aov, "Tax_group:Prediction_tool")
write.csv(
  as.data.frame(bin.tax.spec.tukey$`Tax_group:Prediction_tool`),
  "../report/cazyme_noncazyme_classification/tax_performance/binary.spec.classifiers.tukeyHSD.csv"
)
bin.tax.spec.tukey
```


## Sensitivity 

```{r binSensTax, echo=FALSE}
bin.sens.tax <- build.bin.tax.df(binary_tax_df, "Sensitivity")

dir.create("../report/cazyme_noncazyme_classification/tax_performance", recursive=FALSE)
write.csv(bin.sens.tax, "../report/cazyme_noncazyme_classification/tax_performance/sensitivity.sum.table.csv", row.names=FALSE)

kable(bin.sens.tax, caption="The sensitivity of binary CAZyme/non-CAZyme classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13), border_right=TRUE)
```

```{r binSensTaxCi, echo=FALSE}
p.bin.sens.tax.ci <- build.bin.tax.p(bin.sens.tax, "Sensitivity")
p.bin.sens.tax.ci
```

```{r binSensTaxScatter, echo=FALSE}
p.bin.sens.tax.scatter <- build.bin.tax.p.scatter(binary_tax_df, "Sensitivity")
p.bin.sens.tax.scatter
```


```{r binSensTaxCiSave, include=FALSE}
pdf(file = "../report/cazyme_noncazyme_classification/tax_performance/sensitivity.ci.pdf",  width = 6.23, height = 4)
p.bin.sens.tax.ci
dev.off()

pdf(file = "../report/cazyme_noncazyme_classification/tax_performance/sensitivity.scatter.pdf",  width = 6.23, height = 8.2)
p.bin.sens.tax.scatter
dev.off()
```

Test if there is a statistically significant difference between the means of the tools and kingdoms using a two-way ANOVA.

```{r binsensTwoANOVA, echo=FALSE}
bin.tax.sens.2.aov <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = binary_tax_df[binary_tax_df$Statistic_parameter=='Sensitivity',])
write.csv(
  summary(bin.tax.sens.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazyme_noncazyme_classification/tax_performance/binary.sens.two-way-ANOVA.csv"
)
summary(bin.tax.sens.2.aov)
```
Statistically significant difference was detected so follow up with a Tukey test to determine if there are differences between specific groups.

```{r}
bin.tax.sens.tukey <- TukeyHSD(bin.tax.sens.2.aov, "Tax_group:Prediction_tool")
write.csv(
  as.data.frame(bin.tax.sens.tukey$`Tax_group:Prediction_tool`),
  "../report/cazyme_noncazyme_classification/tax_performance/binary.sens.classifiers.tukeyHSD.csv"
)
bin.tax.sens.tukey
```


## Precision 

```{r binPrecTax, echo=FALSE}
bin.prec.tax <- build.bin.tax.df(binary_tax_df, "Precision")

dir.create("../report/cazyme_noncazyme_classification/tax_performance", recursive=FALSE)
write.csv(bin.prec.tax, "../report/cazyme_noncazyme_classification/tax_performance/precision.sum.table.csv", row.names=FALSE)

kable(bin.prec.tax, caption="The precision of binary CAZyme/non-CAZyme classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13), border_right=TRUE)
```

```{r binPrecTaxCi, echo=FALSE}
p.bin.prec.tax.ci <- build.bin.tax.p(bin.prec.tax, "Precision")
p.bin.prec.tax.ci
```

```{r binPrecTaxScatter, echo=FALSE}
p.bin.prec.tax.scatter <- build.bin.tax.p.scatter(binary_tax_df, "Precision")
p.bin.prec.tax.scatter
```


```{r binPrecTaxCiSave, include=FALSE}
pdf(file = "../report/cazyme_noncazyme_classification/tax_performance/precision.ci.pdf",  width = 6.23, height = 4)
p.bin.prec.tax.ci
dev.off()

pdf(file = "../report/cazyme_noncazyme_classification/tax_performance/precision.scatter.pdf",  width = 6.23, height = 8.2)
p.bin.prec.tax.scatter
dev.off()
```

Test if there is a statistically significant difference between the means of the tools and kingdoms using a two-way ANOVA.

```{r binprecTwoANOVA, echo=FALSE}
bin.tax.prec.2.aov <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = binary_tax_df[binary_tax_df$Statistic_parameter=='Precision',])
write.csv(
  summary(bin.tax.prec.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazyme_noncazyme_classification/tax_performance/binary.prec.two-way-ANOVA.csv"
)
summary(bin.tax.prec.2.aov)
```
Statistically significant difference was detected so follow up with a Tukey test to determine if there are differences between groups.

```{r}
bin.tax.prec.tukey <- TukeyHSD(bin.tax.prec.2.aov, "Tax_group:Prediction_tool")
write.csv(
  as.data.frame(bin.tax.prec.tukey$`Tax_group:Prediction_tool`),
  "../report/cazyme_noncazyme_classification/tax_performance/binary.prec.classifiers.tukeyHSD.csv"
)
bin.tax.prec.tukey
```


## F1-score

```{r binF1Tax, echo=FALSE}
binary_tax_df[binary_tax_df == 'FBeta-score'] <- 'F1-score'
bin.f1.tax <- build.bin.tax.df(binary_tax_df, 'F1-score')

dir.create("../report/cazyme_noncazyme_classification/tax_performance", recursive=FALSE)
write.csv(bin.f1.tax, "../report/cazyme_noncazyme_classification/tax_performance/F1-score.sum.table.csv", row.names=FALSE)

kable(bin.f1.tax, caption="The F1-score of binary CAZyme/non-CAZyme classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13), border_right=TRUE)
```

```{r binF1TaxCi, echo=FALSE}
p.bin.f1.tax.ci <- build.bin.tax.p(bin.f1.tax, "F1-score")
p.bin.f1.tax.ci
```

```{r binF1TaxScatter, echo=FALSE}
p.bin.f1.tax.scatter <- build.bin.tax.p.scatter(binary_tax_df, "F1-score")
p.bin.f1.tax.scatter
```


```{r binF1TaxCiSave, include=FALSE}
pdf(file = "../report/cazyme_noncazyme_classification/tax_performance/F1-score.ci.pdf",  width = 6.23, height = 4)
p.bin.f1.tax.ci
dev.off()

pdf(file = "../report/cazyme_noncazyme_classification/tax_performance/F1-score.scatter.pdf",  width = 6.23, height = 8.2)
p.bin.f1.tax.scatter
dev.off()
```

Test if there is a statistically significant difference between the means of the tools and kingdoms using a two-way ANOVA.

```{r binf1TwoANOVA, echo=FALSE}
bin.tax.f1.2.aov <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = binary_tax_df[binary_tax_df$Statistic_parameter=='F1-score',])
write.csv(
  summary(bin.tax.f1.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazyme_noncazyme_classification/tax_performance/binary.f1.two-way-ANOVA.csv"
)
summary(bin.tax.f1.2.aov)
```
Statistically significant difference was detected so follow up with a Tukey test to determine if there are differences between groups.

```{r}
bin.tax.f1.tukey <- TukeyHSD(bin.tax.f1.2.aov, "Tax_group:Prediction_tool")
write.csv(
  as.data.frame(bin.tax.f1.tukey$`Tax_group:Prediction_tool`),
  "../report/cazyme_noncazyme_classification/tax_performance/binary.f1.classifiers.tukeyHSD.csv"
)
bin.tax.f1.tukey
```


## Accuracy

```{r binAccTax, echo=FALSE}
bin.acc.tax <- build.bin.tax.df(binary_tax_df, "Accuracy")

dir.create("../report/cazyme_noncazyme_classification/tax_performance", recursive=FALSE)
write.csv(bin.acc.tax, "../report/cazyme_noncazyme_classification/tax_performance/accuracy.sum.table.csv", row.names=FALSE)

kable(bin.acc.tax, caption="The accuracy of binary CAZyme/non-CAZyme classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13), border_right=TRUE)
```

```{r binAccTaxCi, echo=FALSE}
p.bin.acc.tax.ci <- build.bin.tax.p(bin.acc.tax, "Accuracy")
p.bin.acc.tax.ci
```

```{r binAccTaxScatter, echo=FALSE}
p.bin.acc.tax.scatter <- build.bin.tax.p.scatter(binary_tax_df, "Accuracy")
p.bin.acc.tax.scatter
```


```{r binAccTaxCiSave, include=FALSE}
pdf(file = "../report/cazyme_noncazyme_classification/tax_performance/accuracy.ci.pdf",  width = 6.23, height = 4)
p.bin.acc.tax.ci
dev.off()

pdf(file = "../report/cazyme_noncazyme_classification/tax_performance/accuracy.scatter.pdf",  width = 6.23, height = 8.2)
p.bin.acc.tax.scatter
dev.off()
```

Test if there is a statistically significant difference between the means of the tools and kingdoms using a two-way ANOVA.

```{r binaccTwoANOVA, echo=FALSE}
bin.tax.acc.2.aov <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = binary_tax_df[binary_tax_df$Statistic_parameter=='Accuracy',])
write.csv(
  summary(bin.tax.acc.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazyme_noncazyme_classification/tax_performance/binary.acc.two-way-ANOVA.csv"
)
summary(bin.tax.acc.2.aov)
```
Statistically significant difference was detected so follow up with a Tukey test to determine if there are differences between groups.

```{r}
bin.tax.acc.tukey <- TukeyHSD(bin.tax.acc.2.aov, "Tax_group:Prediction_tool")
write.csv(
  as.data.frame(bin.tax.acc.tukey$`Tax_group:Prediction_tool`),
  "../report/cazyme_noncazyme_classification/tax_performance/binary.acc.classifiers.tukeyHSD.csv"
)
bin.tax.acc.tukey
```


```{r binaryTaxCombinedPlot, echo=FALSE}
build.class.tax.per.class.p <- function(stat_sum_df, stat.val){
  # df has colnames:
  # Prediction_tool, Mean, Standard Deviation, LowerCI, UpperCI, Tax_group, CAZy_clas
	colnames(stat_sum_df) <- c('Prediction_tool', 'Mean', 'SD', 'LowerCI', 'UpperCI', 'Taxonomic_group', 'CAZy_class')
	stat_sum_df$Prediction_tool <- factor(stat_sum_df$Prediction_tool, levels = classifiers_full)
	stat_sum_df$CAZy_class <- factor(stat_sum_df$CAZy_class, levels=cazy_class_list)
	
	p.t.b.ci <- ggplot(stat_sum_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::group_by(Taxonomic_group),
				  aes(x=Prediction_tool, y=Mean, color=Taxonomic_group, alpha=0.85)) +
	  geom_point(aes(shape=Taxonomic_group), position=position_dodge(width=0.75)) +
	  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI), width = 1, position=position_dodge(width=0.75)) +
	  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
	  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
	  geom_vline(xintercept=stat_sum_df$Prediction_tool, linetype="dashed", color = "#9c9c9c") +
	  scale_colour_manual(values = colour_set) +
	  theme(legend.position = "bottom",
			plot.background = element_rect(fill = figbg, color = figbg),
			axis.text=element_text(size=10),
			axis.title=element_text(size=12,face="bold"),
			axis.text.x=element_text(angle = 90, size=9, vjust=0.45)) +
	  xlab("Classifier") + 
	  ylab(stat.val) +
	  facet_wrap(~ CAZy_class, ncol=2)
	  
	return(p.t.b.ci)
}
```






















































# CAZy class classification

CAZy groups CAZymes into CAZy families by sequence similarity, and CAZy families are grouped into one of 6 functional classes. The CAZyme classifiers predict the CAZy family annotations of predicted CAZymes, but it is of interest to see what the level of performance of the classiferis is at the CAZy class level. Specifically, a classifier may struggle to predict the correct CAZy class for a CAZyme but consistently predict the correct CAZy class. Therefore, the aim of this part of the evaluation is to evaluate the performance of the classifiers to predict the correct CAZy class of predict CAZymes.

## General trends in CAZy class classification performance

Below is a table summary all statistical parameters calculated in order to evaluate the performance of the CAZy class classification for each prediction tool across all CAZy classes.

```{r cazyClassStasTable, echo=FALSE}
# each row in cazy_class_df is a unique combination of genomic acc/test set, CAZy class, and statistical parameter.

# Calculate statistics
class_subset_spec <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Specificity"), ]
class_subset_spec  <- class_subset_spec[complete.cases(class_subset_spec), ]
class_specificity <- class_subset_spec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Spec Mean"=mean(Statistic_value),
    "Spec Standard Deviation"=sd(Statistic_value),
    "Spec Lower CI"=CI(Statistic_value)[3],
    "Spec Upper CI"=CI(Statistic_value)[1]
)

class_subset_sens <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Sensitivity"), ]
class_subset_sens  <- class_subset_sens[complete.cases(class_subset_sens), ]
class_sensitivity <- class_subset_sens %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Sens Mean"=mean(Statistic_value),
    "Sens Standard Deviation"=sd(Statistic_value),
    "Sens Lower CI"=CI(Statistic_value)[3],
    "Sens Upper CI"=CI(Statistic_value)[1]
)

class_subset_prec <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Precision"), ]
class_subset_prec  <- class_subset_prec[complete.cases(class_subset_prec), ]
class_precision <- class_subset_prec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Prec Mean"=mean(Statistic_value),
    "Prec Standard Deviation"=sd(Statistic_value),
    "Prec Lower CI"=CI(Statistic_value)[3],
    "Prec Upper CI"=CI(Statistic_value)[1]
)

class_subset_f1 <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Fbeta_score"), ]
class_subset_f1  <- class_subset_f1[complete.cases(class_subset_f1), ]
class_f1_score <- class_subset_f1 %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "F1-score Mean"=mean(Statistic_value),
    "F1-score Standard Deviation"=sd(Statistic_value),
    "F1-score Lower CI"=CI(Statistic_value)[3],
    "F1-score Upper CI"=CI(Statistic_value)[1]
)

class_subset_acc <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Accuracy"), ]
class_subset_acc  <- class_subset_acc[complete.cases(class_subset_acc), ]
class_accuracy <- class_subset_acc %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Acc Mean"=mean(Statistic_value),
    "Acc Standard Deviation"=sd(Statistic_value),
    "Acc Lower CI"=CI(Statistic_value)[3],
    "Acc Upper CI"=CI(Statistic_value)[1]
)

# combine data and build a single dataframe
class_summary_df <- merge(class_specificity, class_sensitivity)
class_summary_df <- merge(class_summary_df, class_precision)
class_summary_df <- merge(class_summary_df, class_f1_score)
class_summary_df <- merge(class_summary_df, class_accuracy)

# define factors
class_summary_df$Prediction_tool <- factor(class_summary_df$Prediction_tool, levels = classifiers_full) # set order data is presented

names(class_summary_df)[names(class_summary_df) == "Prediction_tool"] <- "Classifier"
class_summary_df$Classifier <- factor(class_summary_df$Classifier, levels = classifiers_full) # set order data is presented

dir.create("../report/cazy_class_classification", recursive=FALSE)
write.csv(class_summary_df, "../report/cazy_class_classification/summaryTable.csv", row.names=FALSE)

kable(
  class_summary_df,
  caption="Overall performance of CAZyme classifiers CAZy class classification performance",
  align='c',
  digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

Here we plot the 95% confidence intervals for CAZy class classification, when pooling all CAZy classes and test sets.

```{r classCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of CAZy class classification, plotting the mean plus and minus the 95% confidence interval."}
cal_tool_ci <- function(stat_subset){
  # calculate lower, upper and mean 95% CI
  UpperCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("UpperCI"=CI(Statistic_value)[1])
  MeanCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("MeanCI"=CI(Statistic_value)[2])
  LowerCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("LowerCI"=CI(Statistic_value)[3])

  ci_df <- merge(MeanCI, UpperCI)
  ci_df <- merge(ci_df, LowerCI)
  
  # return dataframe with cols: Prediction_tool, MeanCI, UpperCI, LowerCI, Statistic_parameter
  return(ci_df)
}

class.spec.ci <- cal_tool_ci(class_subset_spec)
class.spec.ci$Statistic_parameter <- rep('Specificity', nrow(class.spec.ci))
class.sens.ci <- cal_tool_ci(class_subset_sens)
class.sens.ci$Statistic_parameter <- rep('Sensitivity', nrow(class.sens.ci))
class.prec.ci <- cal_tool_ci(class_subset_prec)
class.prec.ci$Statistic_parameter <- rep('Precision', nrow(class.prec.ci))
class.f1.ci <- cal_tool_ci(class_subset_f1)
class.f1.ci$Statistic_parameter <- rep('F1-score', nrow(class.f1.ci))
class.acc.ci <- cal_tool_ci(class_subset_acc)
class.acc.ci$Statistic_parameter <- rep('Accuracy', nrow(class.acc.ci))

class.ci_df <- rbind(class.spec.ci, class.sens.ci)
class.ci_df <- rbind(class.ci_df, class.prec.ci)
class.ci_df <- rbind(class.ci_df, class.f1.ci)
class.ci_df <- rbind(class.ci_df, class.acc.ci)

class.ci_df$Prediction_tool <- factor(class.ci_df$Prediction_tool, levels = classifiers_full)
class.ci_df$Statistic_parameter <- factor(class.ci_df$Statistic_parameter, levels = c("Specificity", "Sensitivity", "Precision", "F1-score", "Accuracy"))

p.class.CI = ggplot(class.ci_df %>% dplyr::group_by(Statistic_parameter),
                   aes(x=Prediction_tool, y=MeanCI, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  scale_colour_manual(values = colour_set_tools) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Statistical value") +
  facet_wrap(~ Statistic_parameter, ncol=2)
p.class.CI
```

```{r saveClassCI1, include=FALSE}
dir.create("../report/cazy_class_classification", recursive=FALSE)
pdf(
  file = "../report/cazy_class_classification/statsConfidenceIntervals.pdf",
  width = 6.3,
  height = 8.2
)
p.class.CI
dev.off()
```

```{r allCazyClassesScatterPlots, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of CAZy class classification, plotting statistical patermate value per test set for binary CAZy class classification across all CAZy classes."}
build.cazy.class.scatter.plt <- function(subset_spec, subset_sens, subset_prec, subset_f1, subset_acc){
  bin.mean.df <- rbind(subset_spec, subset_sens)
  bin.mean.df <- rbind(bin.mean.df, subset_prec)
  bin.mean.df <- rbind(bin.mean.df, subset_f1)
  bin.mean.df <- rbind(bin.mean.df, subset_acc)
  # replace 'FBeta-score' with 'F1-score'
  bin.mean.df[bin.mean.df == 'FBeta-score'] <- 'F1-score'
  bin.mean.df[bin.mean.df == 'Fbeta_score'] <- 'F1-score'
  bin.mean.df$Prediction_tool <- factor(bin.mean.df$Prediction_tool, levels = classifiers_full)
  bin.mean.df$Statistic_parameter <- factor(bin.mean.df$Statistic_parameter, levels = c("Specificity", "Sensitivity", "Precision", "F1-score", "Accuracy"))
  
  binary.mean.p = ggplot(bin.mean.df %>% dplyr::group_by(Statistic_parameter),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
    geom_boxplot(outlier.shape=NA) +
    geom_jitter(width=0.1, height=0, size=0.3) +
    geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
    geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
    scale_fill_manual(values = colour_set_tools) +
    theme(legend.position = "none",
          plot.background = element_rect(fill = figbg, color = figbg),
          axis.text=element_text(size=10),
          axis.title=element_text(size=11,face="bold"),
          axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
    xlab("Classifier") + 
    ylab("Statistical value") +
    facet_wrap(~ Statistic_parameter, ncol=2)
  
  return(binary.mean.p)
}

class.scatter.plt <- build.cazy.class.scatter.plt(class_subset_spec, class_subset_sens, class_subset_prec, class_subset_f1, class_subset_acc)
class.scatter.plt
```

```{r saveClassScatter, include=FALSE}
dir.create("../report/cazy_class_classification", recursive=FALSE)
pdf(
  file = "../report/cazy_class_classification/statsScatterPlots.pdf",
  width = 6.3,
  height = 8.2
)
class.scatter.plt
dev.off()
```

Below is a proportional area plot representing the F1-score for each CAZyme classifier for each test set. Each square is sized proportional to the relative sample size. Every class was not included in every sample, resulting in different sample sizes between CAZy classes.

```{r statsFbeta, echo=FALSE, fig.cap="Proportional area plot of CAZy class classification performance. Performance is represented by the F1-score. Plots are proptional to the number of test sets, after excluding true negative classifications."}
cazy_class_df$Prediction_tool <- factor(cazy_class_df$Prediction_tool, levels = classifiers_full) # set order data is presented

# subset Fbeta-scores
class_fbeta_subset <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Fbeta_score"), ]

# Not every CAZy class is present in every test set, where it was not present a value of NA was given.
# these rows need to be dropped so that the proportional area plot represents the number of test sets containing
# that CAZy class
class_fbeta_subset  <- class_fbeta_subset[complete.cases(class_fbeta_subset), ]

# classify the Fbeta-scores into bins
val.class <- vector()

for(i in 1:nrow(class_fbeta_subset)) {
  if(class_fbeta_subset[i, 6] == 1){val.class <- append(val.class, '[1.00]')} 
  else if (class_fbeta_subset[i, 6] == 0){val.class <- append(val.class, '[0.00]')}
  else if (class_fbeta_subset[i, 6] < 1 && class_fbeta_subset[i, 6] >= 0.95){val.class <- append(val.class, '(0.95, 1.00]')}
  else if (class_fbeta_subset[i, 6] < 0.95 && class_fbeta_subset[i, 6] >= 0.9){val.class <- append(val.class, '(0.90, 0.95]')}
  else if (class_fbeta_subset[i, 6] < 0.90 && class_fbeta_subset[i, 6] >= 0.85){val.class <- append(val.class, '(0.85, 0.90]')}
  else if (class_fbeta_subset[i, 6] < 0.85 && class_fbeta_subset[i, 6] >= 0.80){val.class <- append(val.class, '(0.80, 0.85]')}
  else if (class_fbeta_subset[i, 6] < 0.80 && class_fbeta_subset[i, 6] >= 0.75){val.class <- append(val.class, '(0.75, 0.80]')}
  else if (class_fbeta_subset[i, 6] < 0.75 && class_fbeta_subset[i, 6] >= 0.70){val.class <- append(val.class, '(0.70, 0.75]')}
  else if (class_fbeta_subset[i, 6] < 0.70 && class_fbeta_subset[i, 6] >= 0.65){val.class <- append(val.class, '(0.65, 0.70]')}
  else if (class_fbeta_subset[i, 6] < 0.65 && class_fbeta_subset[i, 6] >= 0.60){val.class <- append(val.class, '(0.60, 0.65]')}
  else if (class_fbeta_subset[i, 6] < 0.60 && class_fbeta_subset[i, 6] >= 0.55){val.class <- append(val.class, '(0.55, 0.60]')}
  else if (class_fbeta_subset[i, 6] < 0.55 && class_fbeta_subset[i, 6] >= 0.50){val.class <- append(val.class, '(0.50, 0.55]')}
  else if (class_fbeta_subset[i, 6] < 0.50 && class_fbeta_subset[i, 6] >= 0.45){val.class <- append(val.class, '(0.45, 0.50]')}
  else if (class_fbeta_subset[i, 6] < 0.45 && class_fbeta_subset[i, 6] >= 0.40){val.class <- append(val.class, '(0.40, 0.45]')}
  else if (class_fbeta_subset[i, 6] < 0.40 && class_fbeta_subset[i, 6] >= 0.35){val.class <- append(val.class, '(0.35, 0.40]')}
  else if (class_fbeta_subset[i, 6] < 0.35 && class_fbeta_subset[i, 6] >= 0.30){val.class <- append(val.class, '(0.30, 0.35]')}
  else if (class_fbeta_subset[i, 6] < 0.30 && class_fbeta_subset[i, 6] >= 0.25){val.class <- append(val.class, '(0.25, 0.30]')}
  else if (class_fbeta_subset[i, 6] < 0.25 && class_fbeta_subset[i, 6] >= 0.20){val.class <- append(val.class, '(0.20, 0.25]')}
  else if (class_fbeta_subset[i, 6] < 0.20 && class_fbeta_subset[i, 6] >= 0.15){val.class <- append(val.class, '(0.15, 0.20]')}
  else if (class_fbeta_subset[i, 6] < 0.15 && class_fbeta_subset[i, 6] >= 0.10){val.class <- append(val.class, '(0.10, 0.15]')}
  else if (class_fbeta_subset[i, 6] < 0.10 && class_fbeta_subset[i, 6] >= 0.05){val.class <- append(val.class, '(0.05, 0.10]')}
  else if (class_fbeta_subset[i, 6] < 0.05 && class_fbeta_subset[i, 6] >= 0){val.class <- append(val.class, '(0.00, 0.05]')}
  else {val.class <- append(val.class, '(0.00, 0.05]')}
}
class_fbeta_subset$val.class <- val.class

# set order data is presented
class_fbeta_subset$Prediction_tool <- factor(class_fbeta_subset$Prediction_tool, levels = classifiers_full) 
class_fbeta_subset$CAZy_class <- factor(class_fbeta_subset$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))
class_fbeta_subset$val.class <- factor(class_fbeta_subset$val.class, levels = c(
  '[0.00]',
  '(0.00, 0.05]', '(0.05, 0.10]',
  '(0.10, 0.15]', '(0.15, 0.20]',
  '(0.20, 0.25]', '(0.25, 0.30]',
  '(0.30, 0.35]', '(0.35, 0.40]',
  '(0.40, 0.45]', '(0.45, 0.50]',
  '(0.50, 0.55]', '(0.55, 0.60]',
  '(0.60, 0.65]', '(0.65, 0.70]',
  '(0.70, 0.75]', '(0.75, 0.80]',
  '(0.80, 0.85]', '(0.85, 0.90]',
  '(0.90, 0.95]', '(0.95, 1.00]',
  '[1.00]')
)

p.classF1 = ggally_count(class_fbeta_subset, mapping=ggplot2::aes(x=Prediction_tool, y=CAZy_class, fill=val.class)) +
  scale_fill_manual(values = colour_grad, drop=FALSE) +
  xlab("Classifier") + 
  ylab("CAZy class") +
  theme(
      legend.position = "right",
      legend.background = element_rect(fill = figbg, color = figbg),
      plot.background = element_rect(fill = figbg, color = figbg),
      legend.text=element_text(size=10),
      legend.title=element_text(size=11),
      axis.text=element_text(size=10),
      axis.title=element_text(size=12,face="bold"),
      axis.text.x=element_text(angle = 90, size=9, vjust=0.45)) +
  labs(fill = "F1-score") +
  guides(fill = guide_legend(reverse = TRUE))
p.classF1
```

```{r saveCazyClassFbeta, include=FALSE}
pdf(file = "../report/cazy_class_classification/proportionAreaClassF1.pdf",  width = 8, height = 11)
p.classF1
dev.off()

p.classF1.LS = ggally_count(class_fbeta_subset, mapping=ggplot2::aes(x=Prediction_tool, y=CAZy_class, fill=val.class)) +
  scale_fill_manual(values = colour_grad, drop=FALSE) +
  xlab("Classifier") + 
  ylab("CAZy class") +
  theme(legend.background = element_rect(fill = figbg, color = figbg),
        legend.position = 'bottom',
        plot.background = element_rect(fill = figbg, color = figbg),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 45, size=8, vjust=1, hjust=1)) +
  labs(fill = "F1-score") +
  guides(fill = guide_legend(reverse = TRUE))

pdf(file = "../report/cazy_class_classification/cazyClassF1paPlto-LegendCut.pdf",  width = 5.5, height = 8)
p.classF1.LS
dev.off()

pdf(file = "../report/cazy_class_classification/cazyClassF1paPlto-LegendFull.pdf",  width = 8, height = 8)
p.classF1.LS
dev.off()
```

A dataframe of the number of test sets containing each CAZy class is generated (table \@ref(tab:calcCazyClassSamples)).

```{r calcCazyClassSamples, echo=FALSE}
class_fbeta_subset$Prediction_tool <- factor(class_fbeta_subset$Prediction_tool, levels = classifiers_full) # set order data is presented

calc_class_sample_size <- function(class_df, tool) {
  class_df <- class_df[complete.cases(class_df), ]
  tool_data <- c(tool)
  tool_df = subset(class_df, class_df$Prediction_tool == tool)
  
  cazy_classes = c('GH', 'GT', 'PL', 'CE', 'AA', 'CBM')
  for (cazy_class in cazy_classes) {
    sample_size <- sum(tool_df$CAZy_class == cazy_class)
    tool_data = c(tool_data, sample_size)
  }
  tool_data_m = matrix(tool_data,nrow=1)
  return(tool_data_m)
}

class_pt_row_1 = calc_class_sample_size(class_fbeta_subset, 'dbCAN_2')
class_pt_row_2 = calc_class_sample_size(class_fbeta_subset, 'dbCAN_2:HMMER')
class_pt_row_3 = calc_class_sample_size(class_fbeta_subset, 'dbCAN_2:DIAMOND')
class_pt_row_4 = calc_class_sample_size(class_fbeta_subset, 'dbCAN_2:Hotpep')
class_pt_row_5 = calc_class_sample_size(class_fbeta_subset, 'dbCAN_3')
class_pt_row_6 = calc_class_sample_size(class_fbeta_subset, 'dbCAN_3:HMMER')
class_pt_row_7 = calc_class_sample_size(class_fbeta_subset, 'dbCAN_3:DIAMOND')
class_pt_row_8 = calc_class_sample_size(class_fbeta_subset, 'dbCAN_3:eCAMI')
class_pt_row_9 = calc_class_sample_size(class_fbeta_subset, 'dbCAN_4')
class_pt_row_10 = calc_class_sample_size(class_fbeta_subset, 'dbCAN_4:HMMER')
class_pt_row_11 = calc_class_sample_size(class_fbeta_subset, 'dbCAN_4:DIAMOND')
class_pt_row_12 = calc_class_sample_size(class_fbeta_subset, 'dbCAN_4:dbCAN-sub')
class_pt_row_13 = calc_class_sample_size(class_fbeta_subset, 'CUPP')

cazy_class_fam_sample_size_df <- rbind(class_pt_row_1, class_pt_row_2)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, class_pt_row_3)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, class_pt_row_4)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, class_pt_row_5)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, class_pt_row_6)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, class_pt_row_7)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, class_pt_row_8)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, class_pt_row_9)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, class_pt_row_10)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, class_pt_row_11)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, class_pt_row_12)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, class_pt_row_13)
cazy_class_fam_sample_size_df <- as.data.frame(cazy_class_fam_sample_size_df)

names(cazy_class_fam_sample_size_df) <- c('Prediction_tool', 'GH', 'GT', 'PL', 'CE', 'AA', 'CBM')

kable(
  cazy_class_fam_sample_size_df,
  caption="Number of test set each CAZy class appeared in according to CAZy (July 2023).",
  align='c',
  digits = 4) %>% kable_styling(full_width = F)
```

## Statistically testing across all CAzy classes

Test for statistically significant differences between the means of the tools when pooling all CAZymes and CAZy classes and evaluating the overall performance of CAZy class classification.

Perform a one-way ANOVA to test for significant differences between the tools, and if found, using a post hoc Tukey HSD test to determine between which tools the means are significant different.

### Specificity

```{r classOverallOneAnovaspec, echo=FALSE}
class_subset_spec.df <- data.frame(class_subset_spec)
class_subset_spec.df$Prediction_tool <- factor(class_subset_spec.df$Prediction_tool, levels=classifiers_full)
class_subset_spec.df <- class_subset_spec.df[complete.cases(class_subset_spec.df), ]
cl.spec.anova <- aov(Statistic_value ~ Prediction_tool, data=class_subset_spec.df)
summary(cl.spec.anova)
```

```{r classspecTurkey, echo=FALSE}
cl.spec.turkey <- TukeyHSD(cl.spec.anova, conf.level=.95)
write.csv(
  as.data.frame(cl.spec.turkey$Prediction_tool),
  "../report/cazy_class_classification/class.spec.tukeyHSD.csv"
)
cl.spec.turkey
```

### Sensitivity

```{r classOverallOneAnovasens, echo=FALSE}
class_subset_sens.df <- data.frame(class_subset_sens)
class_subset_sens.df$Prediction_tool <- factor(class_subset_sens.df$Prediction_tool, levels=classifiers_full)
class_subset_sens.df <- class_subset_sens.df[complete.cases(class_subset_sens.df), ]
cl.sens.anova <- aov(Statistic_value ~ Prediction_tool, data=class_subset_sens.df)
summary(cl.sens.anova)
```

```{r classsensTurkey, echo=FALSE}
cl.sens.turkey <- TukeyHSD(cl.sens.anova, conf.level=.95)
write.csv(
  as.data.frame(cl.sens.turkey$Prediction_tool),
  "../report/cazy_class_classification/class.sens.tukeyHSD.csv"
)
cl.sens.turkey
```

### Precision

```{r classOverallOneAnovaprec, echo=FALSE}
class_subset_prec.df <- data.frame(class_subset_prec)
class_subset_prec.df$Prediction_tool <- factor(class_subset_prec.df$Prediction_tool, levels=classifiers_full)
class_subset_prec.df <- class_subset_prec.df[complete.cases(class_subset_prec.df), ]
cl.prec.anova <- aov(Statistic_value ~ Prediction_tool, data=class_subset_prec.df)
summary(cl.prec.anova)
```

```{r classprecTurkey, echo=FALSE}
cl.prec.turkey <- TukeyHSD(cl.prec.anova, conf.level=.95)
write.csv(
  as.data.frame(cl.prec.turkey$Prediction_tool),
  "../report/cazy_class_classification/class.prec.tukeyHSD.csv"
)
cl.prec.turkey
```

### F1-score

```{r classOverallOneAnovaF1, echo=FALSE}
class_subset_f1.df <- data.frame(class_subset_f1)
class_subset_f1.df$Prediction_tool <- factor(class_subset_f1.df$Prediction_tool, levels=classifiers_full)
class_subset_f1.df <- class_subset_f1.df[complete.cases(class_subset_f1.df), ]
cl.f1.anova <- aov(Statistic_value ~ Prediction_tool, data=class_subset_f1.df)
summary(cl.f1.anova)
```

```{r classf1Turkey, echo=FALSE}
cl.f1.turkey <- TukeyHSD(cl.f1.anova, conf.level=.95)
write.csv(
  as.data.frame(cl.f1.turkey$Prediction_tool),
  "../report/cazy_class_classification/class.f1.tukeyHSD.csv"
)
cl.f1.turkey
```


### Accuracy

```{r classOverallOneAnovaAcc, echo=FALSE}
class_subset_acc.df <- data.frame(class_subset_acc)
class_subset_acc.df$Prediction_tool <- factor(class_subset_acc.df$Prediction_tool, levels=classifiers_full)
class_subset_acc.df <- class_subset_acc.df[complete.cases(class_subset_acc.df), ]
cl.acc.anova <- aov(Statistic_value ~ Prediction_tool, data=class_subset_acc.df)
summary(cl.acc.anova)
```
```{r classAccTurkey, echo=FALSE}
cl.acc.turkey <- TukeyHSD(cl.acc.anova, conf.level=.95)
write.csv(
  as.data.frame(cl.acc.turkey$Prediction_tool),
  "../report/cazy_class_classification/class.acc.tukeyHSD.csv"
)
cl.acc.turkey
```



## Sensitivity versus specificity

The sensitivity of each CAZyme classifier can be plotted against the specificity for each CAZy class, however plotting all CAZy classes in a single plot produces a cramped plot, unless very few test sets were used.

```{r cazyClassSpecVsSens, echo=FALSE, fig.cap="Scatter plot of sensitivity against specificity for predicting CAZy class members per CAZyme classier"}
# subset specificity scores
cazy_class_specificity_df <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Specificity"), ]
# rename columns
names(cazy_class_specificity_df)[names(cazy_class_specificity_df) == "Statistic_value"] <- "Specificity"
cazy_class_specificity_df <- cazy_class_specificity_df[c("Genomic_accession", "Prediction_tool", "CAZy_class", "Specificity")]

# subset sensitivity scores
cazy_class_sens_df <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Sensitivity"), ]
names(cazy_class_sens_df)[names(cazy_class_sens_df) == "Statistic_value"] <- "Sensitivity"
cazy_class_sens_df <- cazy_class_sens_df[c("Genomic_accession", "Prediction_tool", "CAZy_class", "Sensitivity")]

# merge dataframes
cazy_class_spec_sense_df <- merge(cazy_class_specificity_df, cazy_class_sens_df, by=c("Genomic_accession", "Prediction_tool", "CAZy_class"))

# specificity was not predicted for every CAZy class in eveyr test set, this is the result of the CAZy class not being present in the test set and the classifier not predicting the presence of the CAZy class in the test set
# In these cases specificity is NA
# NA values are removed
cazy_class_spec_sense_df  <- cazy_class_spec_sense_df[complete.cases(cazy_class_spec_sense_df), ]

cazy_class_spec_sense_df$CAZy_class <- factor(cazy_class_spec_sense_df$CAZy_class, levels = cazy_class_list)

# one plot per class with be made using facet_wrap by Prediciton_tool.
p.class.spec.sense = ggplot(cazy_class_spec_sense_df %>% dplyr::group_by(Prediction_tool), aes(x=Sensitivity, y=Specificity, color=Prediction_tool)) +
  geom_point() +
  scale_color_manual(values=colour_set_tools) +
  theme(plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=11,face="bold"),
        strip.text = element_text(size=11)) +
  labs(x = "Sensitivity", y = "Specificity", color="Classifer") +
  facet_wrap(~CAZy_class)
p.class.spec.sense
```

```{r saveclassAllClassesSpecSens, include=FALSE}
pdf(file = "../report/cazy_class_classification/classAllSpecVsSens.pdf", width = 11.25, height = 7)
p.class.spec.sense
dev.off()
```

## Performance per CAZy class

Below the prediction sensitivity is plotted against the specificity for each classifier, and a separate plot is generated for each CAZy class.

The scatter plots of sensitivity against specificity overlay a coloured contour to highlight the distribution of the points. When too many points have the same value a contour cannot be generated. In order to plot a contour noise is added to the data. The original data is used to plot the scatter plot and the data with added noise is used to plot the contour.

The percentage of the data points which need noise to be added to them in order to generate a contour varies from data set to data set. To change the percentage of the data points with noise added to them, change the third value of call to the function `plot.class.sens.vs.spec()`, which is used to generate the plots. The third value is the percentage of data points to add noise to, written in decimal form.

```{r buildClassSensVsSpecPlot, echo=FALSE}
# When plotting scatter plots overlaying a coloured contour, contours cannot be ploted if too many data points have the same value
# To sort this add noise to the data

add_noise <- function(data, corrupt_percent, min.val, max.val){
  corrupt <- rbinom(length(data), 1, corrupt_percent)    # choose an average of <corrupt_percent>% to corrupt at random
  corrupt <- as.logical(corrupt)
  noise <- rnorm(sum(corrupt), min.val, max.val) # generate the noise to add
  data[corrupt] <- data[corrupt] - noise      # about 10% of x has been corrupted
  return(data)
}

plot.class.sens.vs.spec <- function(
  class.data.df,
  cazy_class,
  corruption_size,
  min.val,
  max.val,
  plot_colours,
  factors.list
){
  # subset rows containing data for the given CAZy class
  class.subset.df <- class.data.df[which(class.data.df$CAZy_class == cazy_class), ]
  
  # set the order prediction tools are plotted in the plot
  class.subset.df$Prediction_tool <- factor(class.subset.df$Prediction_tool, levels = factors.list)

  # subset and add noise to the data to enable plotting contours, contours cannot be plotted if too many data points have the same value
  class.sens.data <- class.subset.df$Sensitivity
  class.sens.data.noise <- add_noise(class.sens.data, corruption_size, min.val, max.val)

  class.spec.data <- class.subset.df$Specificity
  class.spec.data.noise <- add_noise(class.spec.data, corruption_size, min.val, max.val)
  
  # add the data with noise as new columns to the df
  class.subset.df$Sens_noise <- class.sens.data.noise
  class.subset.df$Spec_noise <- class.spec.data.noise
  
  # generate the plot
  p.class.sens.spec = ggplot(class.subset.df %>% dplyr::group_by(Prediction_tool), aes(x=Sensitivity, y=Specificity)) +
    geom_density_2d_filled(alpha = 0.5, aes(x=Sens_noise, y=Spec_noise)) +
    geom_point() +
    scale_color_manual(values=plot_colours) +
    theme(plot.background = element_rect(fill = figbg, color = figbg),
          axis.text=element_text(size=10),
          axis.title=element_text(size=11,face="bold"),
          strip.text = element_text(size=11)) +
    labs(x = "Sensitivity", y = "Specificity", color="Classifer", fill="Density") +
    xlim(0.4, 1.0) +
    ylim(0.9, 1.0) +
    facet_wrap(~ Prediction_tool)
  p.class.sens.spec
}

get.class.summary.table.and.plots <- function(class.data.df, cazy_class){
  
  stat_df <- class.data.df[which(class.data.df$CAZy_class == cazy_class), ]
  stat_df  <- stat_df[complete.cases(stat_df), ]
  
  subset_spec <- stat_df[which(stat_df$Statistic_parameter == "Specificity"), ]
  binary_specificity <- subset_spec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Spec Mean"=mean(Statistic_value),
    "Spec Standard Deviation"=sd(Statistic_value),
    "Spec CI Lower"=CI(Statistic_value)[3],
    "Spec CI Upper"=CI(Statistic_value)[1]
  )
  
  subset_sens <- stat_df[which(stat_df$Statistic_parameter == "Sensitivity"), ]
  binary_sensitivity <- subset_sens %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Sens Mean"=mean(Statistic_value),
    "Sens Standard Deviation"=sd(Statistic_value),
    "Sens CI Lower"=CI(Statistic_value)[3],
    "Sens CI Upper"=CI(Statistic_value)[1]
    )
  
  subset_prec <- stat_df[which(stat_df$Statistic_parameter == "Precision"), ]
  binary_precision <- subset_prec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Prec Mean"=mean(Statistic_value),
    "Prec Standard Deviation"=sd(Statistic_value),
    "Prec CI Lower"=CI(Statistic_value)[3],
    "Prec CI Upper"=CI(Statistic_value)[1]
    )
  
  subset_f1 <- stat_df[which(stat_df$Statistic_parameter == "Fbeta_score"), ]
  binary_f1_score <- subset_f1 %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "F1-score Mean"=mean(Statistic_value),
    "F1-score Standard Deviation"=sd(Statistic_value),
    "F1-score CI Lower"=CI(Statistic_value)[3],
    "F1-score CI Upper"=CI(Statistic_value)[1]
    )
  
  subset_acc <- stat_df[which(stat_df$Statistic_parameter == "Accuracy"), ]
  binary_accuracy <- subset_acc %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Acc Mean"=mean(Statistic_value),
    "Acc Standard Deviation"=sd(Statistic_value),
    "Acc CI Lower"=CI(Statistic_value)[3],
    "Acc CI Upper"=CI(Statistic_value)[1]
    )
  
  # combine data and build a single dataframe
  binary_summary_df <- merge(binary_specificity, binary_sensitivity)
  binary_summary_df <- merge(binary_summary_df, binary_precision)
  binary_summary_df <- merge(binary_summary_df, binary_f1_score)
  binary_summary_df <- merge(binary_summary_df, binary_accuracy)
  
  return(
    list(
      binary_summary_df,
      subset_spec,
      subset_sens,
      subset_prec,
      subset_f1,
      subset_acc
    )
  )
}

plot.cazy.class.ci <- function(stat_df.spec, stat_df.sens, stat_df.prec, stat_df.f1, stat_df.acc){

  class.spec.ci <- cal_tool_ci(stat_df.spec)
  class.spec.ci$Statistic_parameter <- rep('Specificity', nrow(class.spec.ci))
  
  class.sens.ci <- cal_tool_ci(stat_df.sens)
  class.sens.ci$Statistic_parameter <- rep('Sensitivity', nrow(class.sens.ci))

  class.prec.ci <- cal_tool_ci(stat_df.prec)
  class.prec.ci$Statistic_parameter <- rep('Precision', nrow(class.prec.ci))

  class.f1.ci <- cal_tool_ci(stat_df.f1)
  class.f1.ci$Statistic_parameter <- rep('F1-score', nrow(class.f1.ci))

  class.acc.ci <- cal_tool_ci(stat_df.acc)
  class.acc.ci$Statistic_parameter <- rep('Accuracy', nrow(class.acc.ci))
  
  class.ci_df <- rbind(class.spec.ci, class.sens.ci)
  class.ci_df <- rbind(class.ci_df, class.prec.ci)
  class.ci_df <- rbind(class.ci_df, class.f1.ci)
  class.ci_df <- rbind(class.ci_df, class.acc.ci)
  
  class.ci_df$Prediction_tool <- factor(class.ci_df$Prediction_tool, levels = classifiers_full)
  class.ci_df$Statistic_parameter <- factor(class.ci_df$Statistic_parameter, levels = c("Specificity", "Sensitivity", "Precision", "F1-score", "Accuracy"))
 
  # generate plot, facet wrapped by statistical parameter
  p.class.pl <- ggplot(class.ci_df %>% dplyr::group_by(Statistic_parameter),
                aes(x=Prediction_tool, y=MeanCI, color=Prediction_tool)) +
    geom_point() +
    geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
    geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
    geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
    scale_colour_manual(values = colour_set_tools) +
    theme(legend.position = "none",
          plot.background = element_rect(fill = figbg, color = figbg),
          axis.text=element_text(size=10),
          axis.title=element_text(size=12,face="bold"), axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
    xlab("Classifier") + 
    ylab("Statistical value") +
    facet_wrap(~ Statistic_parameter, ncol=2)
  
  return(p.class.pl)
}
```

### GH class classification

```{r ghSummaryTableClass, echo=FALSE}
gh_class_dfs <- get.class.summary.table.and.plots(cazy_class_df, 'GH')
gh_binary_summary_df <- gh_class_dfs[[1]]
gh_subset_spec <- gh_class_dfs[[2]]
gh_subset_sens <- gh_class_dfs[[3]]
gh_subset_prec <- gh_class_dfs[[4]]
gh_subset_f1 <- gh_class_dfs[[5]]
gh_subset_acc <- gh_class_dfs[[6]]

dir.create("../report/cazy_class_classification/GH/", recursive=FALSE)
write.csv(gh_binary_summary_df, "../report/cazy_class_classification/GH/summaryTableGH.csv", row.names=FALSE)

kable(gh_binary_summary_df, caption="Overall performance of CAZyme classifiers classification of GH class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r classGhSpecSens, echo=FALSE, fig.cap="Scatter plot of sensitivity against specificity for predicting GH CAZy class members per CAZyme classier, overlaying a density map."}
p.class.gh.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'GH', 0.7, 0.001, 0.05,
  colour_set_tools,
  classifiers_full
)
p.class.gh.spec.sens
```

```{r saveclassGhSpecSens, include=FALSE}
pdf(file = "../report/cazy_class_classification/GH/classGhSpecVsSens.pdf", width = 11.25, height = 7)
p.class.gh.spec.sens
dev.off()
```

```{r ghClassCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of GH class classification, plotting the mean plus and minus the 95% confidence interval."}
# dataframes all have the same structure, each containing data for a different statistic parameter
# X, Genomic_accession, Prediction_tool, CAZy_class, Statistic_parameter, Statistic_value
p.gh.class.ci <- plot.cazy.class.ci(
  gh_subset_spec,
  gh_subset_sens,
  gh_subset_prec,
  gh_subset_f1,
  gh_subset_acc
)
p.gh.class.ci
```

```{r ghClassCIsave, include=FALSE}
pdf(file = "../report/cazy_class_classification/GH/classGhCI.pdf", width = 6.3, height = 8.2)
p.gh.class.ci
dev.off()
```


```{r ghClassClassificationScatterPlot, echo=FALSE, fig.cap="One dimensional scatter plot of the statistical parameters per test set for the classification of GH class members, overlaying a box plot"}

gh.scatter.plt <- build.cazy.class.scatter.plt(gh_subset_spec, gh_subset_sens, gh_subset_prec, gh_subset_f1, gh_subset_acc)
gh.scatter.plt
```

```{r ghClassScattersave, include=FALSE}
pdf(file = "../report/cazy_class_classification/GH/classGhScatter.pdf", width = 6.3, height = 8.2)
gh.scatter.plt
dev.off()
```

#### Specificity test

A one-way ANOVA was performed to determine if the mean specificity was statistically significantly different between the tools.

```{r classGHspecAnova, echo=FALSE}
gh_subset_spec.df <- data.frame(gh_subset_spec)
gh_subset_spec.df$Prediction_tool <- as.factor(gh_subset_spec.df$Prediction_tool)
# subset_spec.df$Prediction_tool <- factor(subset_spec.df$Prediction_tool, levels = classifiers_full)

gh_subset_spec.df <- gh_subset_spec.df[complete.cases(gh_subset_spec.df), ]

class.gh.spec.anova <- aov(Statistic_value ~ Prediction_tool, data = gh_subset_spec.df)
summary(class.gh.spec.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classGHspecTurkey, echo=FALSE}
class.gh.spec.turkey <- TukeyHSD(class.gh.spec.anova, conf.level=.95)
write.csv(
  as.data.frame(class.gh.spec.turkey$Prediction_tool),
  "../report/cazy_class_classification/GH/class.gh.spec.tukeyHSD.csv"
)
class.gh.spec.turkey
```

#### Sensitivity test

A one-way ANOVA was performed to determine if the mean sensitivity was statistically significantly different between the tools.

```{r classGHsensAnova, echo=FALSE}
gh_subset_sens.df <- data.frame(gh_subset_sens)
gh_subset_sens.df$Prediction_tool <- as.factor(gh_subset_sens.df$Prediction_tool)
# subset_sens.df$Prediction_tool <- factor(subset_sens.df$Prediction_tool, levels = classifiers_full)

gh_subset_sens.df <- gh_subset_sens.df[complete.cases(gh_subset_sens.df), ]

class.gh.sens.anova <- aov(Statistic_value ~ Prediction_tool, data = gh_subset_sens.df)
summary(class.gh.sens.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classGHsensTurkey, echo=FALSE}
class.gh.sens.turkey <- TukeyHSD(class.gh.sens.anova, conf.level=.95)
write.csv(
  as.data.frame(class.gh.sens.turkey$Prediction_tool),
  "../report/cazy_class_classification/GH/class.gh.sens.tukeyHSD.csv"
)
class.gh.sens.turkey
```

#### Precision test

A one-way ANOVA was performed to determine if the mean precision was statistically significantly different between the tools.

```{r classGHprecAnova, echo=FALSE}
gh_subset_prec.df <- data.frame(gh_subset_prec)
gh_subset_prec.df$Prediction_tool <- as.factor(gh_subset_prec.df$Prediction_tool)
# subset_prec.df$Prediction_tool <- factor(subset_prec.df$Prediction_tool, levels = classifiers_full)

gh_subset_prec.df <- gh_subset_prec.df[complete.cases(gh_subset_prec.df), ]

class.gh.prec.anova <- aov(Statistic_value ~ Prediction_tool, data = gh_subset_prec.df)
summary(class.gh.prec.anova)
```

The means are not statistically significantly different.

#### F1 test

A one-way ANOVA was performed to determine if the mean F1-scores was statistically significantly different between the tools.

```{r classGHF1Anova, echo=FALSE}
gh_subset_f1.df <- data.frame(gh_subset_f1)
gh_subset_f1.df$Prediction_tool <- as.factor(gh_subset_f1.df$Prediction_tool)
# subset_f1.df$Prediction_tool <- factor(subset_f1.df$Prediction_tool, levels = classifiers_full)

gh_subset_f1.df <- gh_subset_f1.df[complete.cases(gh_subset_f1.df), ]

class.gh.f1.anova <- aov(Statistic_value ~ Prediction_tool, data = gh_subset_f1.df)
summary(class.gh.f1.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classGHF1Turkey, echo=FALSE}
class.gh.f1.turkey <- TukeyHSD(class.gh.f1.anova, conf.level=.95)
write.csv(
  as.data.frame(class.gh.f1.turkey$Prediction_tool),
  "../report/cazy_class_classification/GH/class.gh.f1.tukeyHSD.csv"
)
class.gh.f1.turkey
```

#### Accuracy test

A one-way ANOVA was performed to determine if the mean accuracy was statistically significantly different between the tools.

```{r classGHaccAnova, echo=FALSE}
gh_subset_acc.df <- data.frame(gh_subset_acc)
gh_subset_acc.df$Prediction_tool <- as.factor(gh_subset_acc.df$Prediction_tool)
# subset_acc.df$Prediction_tool <- factor(subset_acc.df$Prediction_tool, levels = classifiers_full)

gh_subset_acc.df <- gh_subset_acc.df[complete.cases(gh_subset_acc.df), ]

class.gh.acc.anova <- aov(Statistic_value ~ Prediction_tool, data = gh_subset_acc.df)
summary(class.gh.acc.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classGHaccTurkey, echo=FALSE}
class.gh.acc.turkey <- TukeyHSD(class.gh.acc.anova, conf.level=.95)
write.csv(
  as.data.frame(class.gh.acc.turkey$Prediction_tool),
  "../report/cazy_class_classification/GH/class.gh.acc.tukeyHSD.csv"
)
class.gh.acc.turkey
```



### GT class classification

```{r gtSummaryTableClass, echo=FALSE}
gt_class_dfs <- get.class.summary.table.and.plots(cazy_class_df, 'GT')
gt_binary_summary_df <- gt_class_dfs[[1]]
gt_subset_spec <- gt_class_dfs[[2]]
gt_subset_sens <- gt_class_dfs[[3]]
gt_subset_prec <- gt_class_dfs[[4]]
gt_subset_f1 <- gt_class_dfs[[5]]
gt_subset_acc <- gt_class_dfs[[6]]

dir.create("../report/cazy_class_classification/GT/", recursive=FALSE)
write.csv(gt_binary_summary_df, "../report/cazy_class_classification/GT/summaryTableGT.csv", row.names=FALSE)

kable(gt_binary_summary_df, caption="Overall performance of CAZyme classifiers classification of GT class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r classGtSpecSens, echo=FALSE, fig.cap="Scatter plot of sensitivity against specificity for predicting GT CAZy class members per CAZyme classier, overlaying a density map."}
p.class.gt.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'GT', 0.8, 0.005, 0.02,
  colour_set_tools,
  classifiers_full
)
p.class.gt.spec.sens
```

```{r saveclassGtSpecSens, include=FALSE}
pdf(file = "../report/cazy_class_classification/GT/GtSpecVsSens.pdf", width = 11.25, height = 7)
p.class.gt.spec.sens
dev.off()
```

```{r gtClassCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of GT class classification, plotting the mean plus and minus the 95% confidence interval."}
p.gt.class.ci <- plot.cazy.class.ci(
  gt_subset_spec,
  gt_subset_sens,
  gt_subset_prec,
  gt_subset_f1,
  gt_subset_acc
)
p.gt.class.ci
```

```{r gtClassCIsave, include=FALSE}
pdf(file = "../report/cazy_class_classification/GT/GtCI.pdf", width = 6.3, height = 8.2)
p.gt.class.ci
dev.off()
```

```{r gtClassClassificationScatterPlot, echo=FALSE, fig.cap="One dimensional scatter plot of the statistical parameters per test set for the classification of GT class members, overlaying a box plot"}
gt.scatter.plt <- build.cazy.class.scatter.plt(gt_subset_spec, gt_subset_sens, gt_subset_prec, gt_subset_f1, gt_subset_acc)
gt.scatter.plt
```

```{r gtClassScattersave, include=FALSE}
pdf(file = "../report/cazy_class_classification/GT/classGtScatter.pdf", width = 6.3, height = 8.2)
gt.scatter.plt
dev.off()
```



#### Specificity test

A one-way ANOVA was performed to determine if the mean specificity was statistically significantly different between the tools.

```{r classGTspecAnova, echo=FALSE}
gt_subset_spec.df <- data.frame(gt_subset_spec)
gt_subset_spec.df$Prediction_tool <- as.factor(gt_subset_spec.df$Prediction_tool)
# subset_spec.df$Prediction_tool <- factor(subset_spec.df$Prediction_tool, levels = classifiers_full)

gt_subset_spec.df <- gt_subset_spec.df[complete.cases(gt_subset_spec.df), ]

class.gt.spec.anova <- aov(Statistic_value ~ Prediction_tool, data = gt_subset_spec.df)
summary(class.gt.spec.anova)
```

The means are not statistically significantly different.

#### Sensitivity test

A one-way ANOVA was performed to determine if the mean sensitivity was statistically significantly different between the tools.

```{r classGTsensAnova, echo=FALSE}
gt_subset_sens.df <- data.frame(gt_subset_sens)
gt_subset_sens.df$Prediction_tool <- as.factor(gt_subset_sens.df$Prediction_tool)
# subset_sens.df$Prediction_tool <- factor(subset_sens.df$Prediction_tool, levels = classifiers_full)

gt_subset_sens.df <- gt_subset_sens.df[complete.cases(gt_subset_sens.df), ]

class.gt.sens.anova <- aov(Statistic_value ~ Prediction_tool, data = gt_subset_sens.df)
summary(class.gt.sens.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classGTsensTurkey, echo=FALSE}
class.gt.sens.turkey <- TukeyHSD(class.gt.sens.anova, conf.level=.95)
write.csv(
  as.data.frame(class.gt.sens.turkey$Prediction_tool),
  "../report/cazy_class_classification/GT/class.gt.sens.tukeyHSD.csv"
)
class.gt.sens.turkey
```

#### Precision test

A one-way ANOVA was performed to determine if the mean precision was statistically significantly different between the tools.

```{r classGTprecAnova, echo=FALSE}
gt_subset_prec.df <- data.frame(gt_subset_prec)
gt_subset_prec.df$Prediction_tool <- as.factor(gt_subset_prec.df$Prediction_tool)
# subset_prec.df$Prediction_tool <- factor(subset_prec.df$Prediction_tool, levels = classifiers_full)

gt_subset_prec.df <- gt_subset_prec.df[complete.cases(gt_subset_prec.df), ]

class.gt.prec.anova <- aov(Statistic_value ~ Prediction_tool, data = gt_subset_prec.df)
summary(class.gt.prec.anova)
```

The means are not statistically significantly different.

#### F1 test

A one-way ANOVA was performed to determine if the mean F1-scores was statistically significantly different between the tools.

```{r classGTF1Anova, echo=FALSE}
gt_subset_f1.df <- data.frame(gt_subset_f1)
gt_subset_f1.df$Prediction_tool <- as.factor(gt_subset_f1.df$Prediction_tool)
# subset_f1.df$Prediction_tool <- factor(subset_f1.df$Prediction_tool, levels = classifiers_full)

gt_subset_f1.df <- gt_subset_f1.df[complete.cases(gt_subset_f1.df), ]

class.gt.f1.anova <- aov(Statistic_value ~ Prediction_tool, data = gt_subset_f1.df)
summary(class.gt.f1.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classGTF1Turkey, echo=FALSE}
class.gt.f1.turkey <- TukeyHSD(class.gt.f1.anova, conf.level=.95)
write.csv(
  as.data.frame(class.gt.f1.turkey$Prediction_tool),
  "../report/cazy_class_classification/GT/class.gt.f1.tukeyHSD.csv"
)
class.gt.f1.turkey
```

#### Accuracy test

A one-way ANOVA was performed to determine if the mean accuracy was statistically significantly different between the tools.

```{r classGTaccAnova, echo=FALSE}
gt_subset_acc.df <- data.frame(gt_subset_acc)
gt_subset_acc.df$Prediction_tool <- as.factor(gt_subset_acc.df$Prediction_tool)
# subset_acc.df$Prediction_tool <- factor(subset_acc.df$Prediction_tool, levels = classifiers_full)

gt_subset_acc.df <- gt_subset_acc.df[complete.cases(gt_subset_acc.df), ]

class.gt.acc.anova <- aov(Statistic_value ~ Prediction_tool, data = gt_subset_acc.df)
summary(class.gt.acc.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classGTaccTurkey, echo=FALSE}
class.gt.acc.turkey <- TukeyHSD(class.gt.acc.anova, conf.level=.95)
write.csv(
  as.data.frame(class.gt.acc.turkey$Prediction_tool),
  "../report/cazy_class_classification/GT/class.gt.acc.tukeyHSD.csv"
)
class.gt.acc.turkey
```






### PL class classification

```{r plSummaryTableClass, echo=FALSE}
pl_class_dfs <- get.class.summary.table.and.plots(cazy_class_df, 'PL')
pl_binary_summary_df <- pl_class_dfs[[1]]
pl_subset_spec <- pl_class_dfs[[2]]
pl_subset_sens <- pl_class_dfs[[3]]
pl_subset_prec <- pl_class_dfs[[4]]
pl_subset_f1 <- pl_class_dfs[[5]]
pl_subset_acc <- pl_class_dfs[[6]]

dir.create("../report/cazy_class_classification/PL/", recursive=FALSE)
write.csv(pl_binary_summary_df, "../report/cazy_class_classification/PL/summaryTablePL.csv", row.names=FALSE)

kable(pl_binary_summary_df, caption="Overall performance of CAZyme classifiers classification of PL class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r classPlSpecSens, echo=FALSE, fig.cap="Scatter plot of sensitivity against specificity for predicting PL CAZy class members per CAZyme classier, overlaying a density map."}
p.class.pl.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'PL', 0.8, 0.005, 0.02,
  colour_set_tools,
  classifiers_full
)
p.class.pl.spec.sens
```

```{r saveclassPlSpecSens, include=FALSE}
pdf(file = "../report/cazy_class_classification/PL/PlSpecVsSens.pdf", width = 11.25, height = 7)
p.class.pl.spec.sens
dev.off()
```

```{r plClassCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of PL class classification, plotting the mean plus and minus the 95% confidence interval."}
p.pl.class.ci <- plot.cazy.class.ci(
  pl_subset_spec,
  pl_subset_sens,
  pl_subset_prec,
  pl_subset_f1,
  pl_subset_acc
)
p.pl.class.ci
```

```{r plClassCIsave, include=FALSE}
pdf(file = "../report/cazy_class_classification/PL/PlCI.pdf", width = 6.3, height = 8.2)
p.pl.class.ci
dev.off()
```

```{r plClassClassificationScatterPlot, echo=FALSE, fig.cap="One dimensional scatter plot of the statistical parameters per test set for the classification of PL class members, overlaying a box plot"}

pl.scatter.plt <- build.cazy.class.scatter.plt(pl_subset_spec, pl_subset_sens, pl_subset_prec, pl_subset_f1, pl_subset_acc)
pl.scatter.plt
```

```{r plClassScattersave, include=FALSE}
pdf(file = "../report/cazy_class_classification/PL/classPlScatter.pdf", width = 6.3, height = 8.2)
pl.scatter.plt
dev.off()
```


#### Specificity test

A one-way ANOVA was performed to determine if the mean specificity was statistically significantly different between the tools.

```{r classPLspecAnova, echo=FALSE}
pl_subset_spec.df <- data.frame(pl_subset_spec)
pl_subset_spec.df$Prediction_tool <- as.factor(pl_subset_spec.df$Prediction_tool)
# subset_spec.df$Prediction_tool <- factor(subset_spec.df$Prediction_tool, levels = classifiers_full)

pl_subset_spec.df <- pl_subset_spec.df[complete.cases(pl_subset_spec.df), ]

class.pl.spec.anova <- aov(Statistic_value ~ Prediction_tool, data = pl_subset_spec.df)
summary(class.pl.spec.anova)
```

The means are not statistically significantly different.

#### Sensitivity test

A one-way ANOVA was performed to determine if the mean sensitivity was statistically significantly different between the tools.

```{r classPLsensAnova, echo=FALSE}
pl_subset_sens.df <- data.frame(pl_subset_sens)
pl_subset_sens.df$Prediction_tool <- as.factor(pl_subset_sens.df$Prediction_tool)
# subset_sens.df$Prediction_tool <- factor(subset_sens.df$Prediction_tool, levels = classifiers_full)

pl_subset_sens.df <- pl_subset_sens.df[complete.cases(pl_subset_sens.df), ]

class.pl.sens.anova <- aov(Statistic_value ~ Prediction_tool, data = pl_subset_sens.df)
summary(class.pl.sens.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classPLsensTurkey, echo=FALSE}
class.pl.sens.turkey <- TukeyHSD(class.pl.sens.anova, conf.level=.95)
write.csv(
  as.data.frame(class.pl.sens.turkey$Prediction_tool),
  "../report/cazy_class_classification/PL/class.pl.sens.tukeyHSD.csv"
)
class.pl.sens.turkey
```

#### Precision test

A one-way ANOVA was performed to determine if the mean precision was statistically significantly different between the tools.

```{r classPLprecAnova, echo=FALSE}
pl_subset_prec.df <- data.frame(pl_subset_prec)
pl_subset_prec.df$Prediction_tool <- as.factor(pl_subset_prec.df$Prediction_tool)
# subset_prec.df$Prediction_tool <- factor(subset_prec.df$Prediction_tool, levels = classifiers_full)

pl_subset_prec.df <- pl_subset_prec.df[complete.cases(pl_subset_prec.df), ]

class.pl.prec.anova <- aov(Statistic_value ~ Prediction_tool, data = pl_subset_prec.df)
summary(class.pl.prec.anova)
```

The means are not statistically significantly different.

#### F1 test

A one-way ANOVA was performed to determine if the mean F1-scores was statistically significantly different between the tools.

```{r classPLF1Anova, echo=FALSE}
pl_subset_f1.df <- data.frame(pl_subset_f1)
pl_subset_f1.df$Prediction_tool <- as.factor(pl_subset_f1.df$Prediction_tool)
# subset_f1.df$Prediction_tool <- factor(subset_f1.df$Prediction_tool, levels = classifiers_full)

pl_subset_f1.df <- pl_subset_f1.df[complete.cases(pl_subset_f1.df), ]

class.pl.f1.anova <- aov(Statistic_value ~ Prediction_tool, data = pl_subset_f1.df)
summary(class.pl.f1.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classPLF1Turkey, echo=FALSE}
class.pl.f1.turkey <- TukeyHSD(class.pl.f1.anova, conf.level=.95)
write.csv(
  as.data.frame(class.pl.f1.turkey$Prediction_tool),
  "../report/cazy_class_classification/PL/class.pl.f1.tukeyHSD.csv"
)
class.pl.f1.turkey
```

#### Accuracy test

A one-way ANOVA was performed to determine if the mean accuracy was statistically significantly different between the tools.

```{r classPLaccAnova, echo=FALSE}
pl_subset_acc.df <- data.frame(pl_subset_acc)
pl_subset_acc.df$Prediction_tool <- as.factor(pl_subset_acc.df$Prediction_tool)
# subset_acc.df$Prediction_tool <- factor(subset_acc.df$Prediction_tool, levels = classifiers_full)

pl_subset_acc.df <- pl_subset_acc.df[complete.cases(pl_subset_acc.df), ]

class.pl.acc.anova <- aov(Statistic_value ~ Prediction_tool, data = pl_subset_acc.df)
summary(class.pl.acc.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classPLaccTurkey, echo=FALSE}
class.pl.acc.turkey <- TukeyHSD(class.pl.acc.anova, conf.level=.95)
write.csv(
  as.data.frame(class.pl.acc.turkey$Prediction_tool),
  "../report/cazy_class_classification/PL/class.pl.acc.tukeyHSD.csv"
)
class.pl.acc.turkey
```





### CE class classification

```{r ceSummaryTableClass, echo=FALSE}
ce_class_dfs <- get.class.summary.table.and.plots(cazy_class_df, 'CE')
ce_binary_summary_df <- ce_class_dfs[[1]]
ce_subset_spec <- ce_class_dfs[[2]]
ce_subset_sens <- ce_class_dfs[[3]]
ce_subset_prec <- ce_class_dfs[[4]]
ce_subset_f1 <- ce_class_dfs[[5]]
ce_subset_acc <- ce_class_dfs[[6]]

dir.create("../report/cazy_class_classification/CE/", recursive=FALSE)
write.csv(ce_binary_summary_df, "../report/cazy_class_classification/CE/summaryTableCE.csv", row.names=FALSE)

kable(ce_binary_summary_df, caption="Overall performance of CAZyme classifiers classification of CE class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r classCeSpecSens, echo=FALSE, fig.cap="Scatter plot of sensitivity against specificity for predicting CE CAZy class members per CAZyme classier, overlaying a density map."}
p.class.ce.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'CE', 0.8, 0.001, 0.02,
  colour_set_tools,
  classifiers_full
)
p.class.ce.spec.sens
```

```{r saveclassCeSpecSens, include=FALSE}
pdf(file = "../report/cazy_class_classification/CE/CeSpecVsSens.pdf", width = 11.25, height = 7)
p.class.ce.spec.sens
dev.off()
```

```{r ceClassCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of CE class classification, plotting the mean plus and minus the 95% confidence interval."}
p.ce.class.ci <- plot.cazy.class.ci(
  ce_subset_spec,
  ce_subset_sens,
  ce_subset_prec,
  ce_subset_f1,
  ce_subset_acc
)
p.ce.class.ci
```

```{r ceClassCIsave, include=FALSE}
pdf(file = "../report/cazy_class_classification/CE/CeCI.pdf", width = 6.3, height = 8.2)
p.ce.class.ci
dev.off()
```

```{r ceClassClassificationScatterPlot, echo=FALSE, fig.cap="One dimensional scatter plot of the statistical parameters per test set for the classification of CE class members, overlaying a box plot"}

ce.scatter.plt <- build.cazy.class.scatter.plt(ce_subset_spec, ce_subset_sens, ce_subset_prec, ce_subset_f1, ce_subset_acc)
ce.scatter.plt
```

```{r ceClassScattersave, include=FALSE}
pdf(file = "../report/cazy_class_classification/CE/classCeScatter.pdf", width = 6.3, height = 8.2)
ce.scatter.plt
dev.off()
```



#### Specificity test

A one-way ANOVA was performed to determine if the mean specificity was statistically significantly different between the tools.

```{r classCEspecAnova, echo=FALSE}
ce_subset_spec.df <- data.frame(ce_subset_spec)
ce_subset_spec.df$Prediction_tool <- as.factor(ce_subset_spec.df$Prediction_tool)
# subset_spec.df$Prediction_tool <- factor(subset_spec.df$Prediction_tool, levels = classifiers_full)

ce_subset_spec.df <- ce_subset_spec.df[complete.cases(ce_subset_spec.df), ]

class.ce.spec.anova <- aov(Statistic_value ~ Prediction_tool, data = ce_subset_spec.df)
summary(class.ce.spec.anova)
```

The means are not statistically significantly different.

#### Sensitivity test

A one-way ANOVA was performed to determine if the mean sensitivity was statistically significantly different between the tools.

```{r classCEsensAnova, echo=FALSE}
ce_subset_sens.df <- data.frame(ce_subset_sens)
ce_subset_sens.df$Prediction_tool <- as.factor(ce_subset_sens.df$Prediction_tool)
# subset_sens.df$Prediction_tool <- factor(subset_sens.df$Prediction_tool, levels = classifiers_full)

ce_subset_sens.df <- ce_subset_sens.df[complete.cases(ce_subset_sens.df), ]

class.ce.sens.anova <- aov(Statistic_value ~ Prediction_tool, data = ce_subset_sens.df)
summary(class.ce.sens.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classCEsensTurkey, echo=FALSE}
class.ce.sens.turkey <- TukeyHSD(class.ce.sens.anova, conf.level=.95)
write.csv(
  as.data.frame(class.ce.sens.turkey$Prediction_tool),
  "../report/cazy_class_classification/CE/class.ce.sens.tukeyHSD.csv"
)
class.ce.sens.turkey
```

#### Precision test

A one-way ANOVA was performed to determine if the mean precision was statistically significantly different between the tools.

```{r classCEprecAnova, echo=FALSE}
ce_subset_prec.df <- data.frame(ce_subset_prec)
ce_subset_prec.df$Prediction_tool <- as.factor(ce_subset_prec.df$Prediction_tool)
# subset_prec.df$Prediction_tool <- factor(subset_prec.df$Prediction_tool, levels = classifiers_full)

ce_subset_prec.df <- ce_subset_prec.df[complete.cases(ce_subset_prec.df), ]

class.ce.prec.anova <- aov(Statistic_value ~ Prediction_tool, data = ce_subset_prec.df)
summary(class.ce.prec.anova)
```

The means are not statistically significantly different.

#### F1 test

A one-way ANOVA was performed to determine if the mean F1-scores was statistically significantly different between the tools.

```{r classCEF1Anova, echo=FALSE}
ce_subset_f1.df <- data.frame(ce_subset_f1)
ce_subset_f1.df$Prediction_tool <- as.factor(ce_subset_f1.df$Prediction_tool)
# subset_f1.df$Prediction_tool <- factor(subset_f1.df$Prediction_tool, levels = classifiers_full)

ce_subset_f1.df <- ce_subset_f1.df[complete.cases(ce_subset_f1.df), ]

class.ce.f1.anova <- aov(Statistic_value ~ Prediction_tool, data = ce_subset_f1.df)
summary(class.ce.f1.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classCEF1Turkey, echo=FALSE}
class.ce.f1.turkey <- TukeyHSD(class.ce.f1.anova, conf.level=.95)
write.csv(
  as.data.frame(class.ce.f1.turkey$Prediction_tool),
  "../report/cazy_class_classification/CE/class.ce.f1.tukeyHSD.csv"
)
class.ce.f1.turkey
```

#### Accuracy test

A one-way ANOVA was performed to determine if the mean accuracy was statistically significantly different between the tools.

```{r classCEaccAnova, echo=FALSE}
ce_subset_acc.df <- data.frame(ce_subset_acc)
ce_subset_acc.df$Prediction_tool <- as.factor(ce_subset_acc.df$Prediction_tool)
# subset_acc.df$Prediction_tool <- factor(subset_acc.df$Prediction_tool, levels = classifiers_full)

ce_subset_acc.df <- ce_subset_acc.df[complete.cases(ce_subset_acc.df), ]

class.ce.acc.anova <- aov(Statistic_value ~ Prediction_tool, data = ce_subset_acc.df)
summary(class.ce.acc.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classCEaccTurkey, echo=FALSE}
class.ce.acc.turkey <- TukeyHSD(class.ce.acc.anova, conf.level=.95)
write.csv(
  as.data.frame(class.ce.acc.turkey$Prediction_tool),
  "../report/cazy_class_classification/CE/class.ce.acc.tukeyHSD.csv"
)
class.ce.acc.turkey
```





### AA class classification

```{r aaSummaryTableClass, echo=FALSE}
aa_class_dfs <- get.class.summary.table.and.plots(cazy_class_df, 'AA')
aa_binary_summary_df <- aa_class_dfs[[1]]
aa_subset_spec <- aa_class_dfs[[2]]
aa_subset_sens <- aa_class_dfs[[3]]
aa_subset_prec <- aa_class_dfs[[4]]
aa_subset_f1 <- aa_class_dfs[[5]]
aa_subset_acc <- aa_class_dfs[[6]]

dir.create("../report/cazy_class_classification/AA/", recursive=FALSE)
write.csv(aa_binary_summary_df, "../report/cazy_class_classification/AA/summaryTableAA.csv", row.names=FALSE)

kable(aa_binary_summary_df, caption="Overall performance of CAZyme classifiers classification of AA class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r classAaSpecSens, echo=FALSE, fig.cap="Scatter plot of sensitivity against specificity for predicting AA CAZy class members per CAZyme classier, overlaying a density map."}
p.class.aa.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'AA', 0.8, 0.001, 0.025,
  colour_set_tools,
  classifiers_full
)
p.class.aa.spec.sens
```

```{r saveclassAaSpecSens, include=FALSE}
pdf(file = "../report/cazy_class_classification/AA/AaSpecVsSens.pdf", width = 11.25, height = 7)
p.class.aa.spec.sens
dev.off()
```

```{r aaClassCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of AA class classification, plotting the mean plus and minus the 95% confidence interval."}
p.aa.class.ci <- plot.cazy.class.ci(
  aa_subset_spec,
  aa_subset_sens,
  aa_subset_prec,
  aa_subset_f1,
  aa_subset_acc
)
p.aa.class.ci
```

```{r aaClassCIsave, include=FALSE}
pdf(file = "../report/cazy_class_classification/AA/AaCI.pdf", width = 6.3, height = 8.2)
p.aa.class.ci
dev.off()
```

```{r aaClassClassificationScatterPlot, echo=FALSE, fig.cap="One dimensional scatter plot of the statistical parameters per test set for the classification of AA class members, overlaying a box plot"}

aa.scatter.plt <- build.cazy.class.scatter.plt(aa_subset_spec, aa_subset_sens, aa_subset_prec, aa_subset_f1, aa_subset_acc)
aa.scatter.plt
```

```{r aaClassScattersave, include=FALSE}
pdf(file = "../report/cazy_class_classification/AA/classAaScatter.pdf", width = 6.3, height = 8.2)
aa.scatter.plt
dev.off()
```



#### Specificity test

A one-way ANOVA was performed to determine if the mean specificity was statistically significantly different between the tools.

```{r classAAspecAnova, echo=FALSE}
aa_subset_spec.df <- data.frame(aa_subset_spec)
aa_subset_spec.df$Prediction_tool <- as.factor(aa_subset_spec.df$Prediction_tool)
# subset_spec.df$Prediction_tool <- factor(subset_spec.df$Prediction_tool, levels = classifiers_full)

aa_subset_spec.df <- aa_subset_spec.df[complete.cases(aa_subset_spec.df), ]

class.aa.spec.anova <- aov(Statistic_value ~ Prediction_tool, data = aa_subset_spec.df)
summary(class.aa.spec.anova)
```

The means are statistically significantly different.

#### Sensitivity test

A one-way ANOVA was performed to determine if the mean sensitivity was statistically significantly different between the tools.

```{r classAAsensAnova, echo=FALSE}
aa_subset_sens.df <- data.frame(aa_subset_sens)
aa_subset_sens.df$Prediction_tool <- as.factor(aa_subset_sens.df$Prediction_tool)
# subset_sens.df$Prediction_tool <- factor(subset_sens.df$Prediction_tool, levels = classifiers_full)

aa_subset_sens.df <- aa_subset_sens.df[complete.cases(aa_subset_sens.df), ]

class.aa.sens.anova <- aov(Statistic_value ~ Prediction_tool, data = aa_subset_sens.df)
summary(class.aa.sens.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classAAsensTurkey, echo=FALSE}
class.aa.sens.turkey <- TukeyHSD(class.aa.sens.anova, conf.level=.95)
write.csv(
  as.data.frame(class.aa.sens.turkey$Prediction_tool),
  "../report/cazy_class_classification/AA/class.aa.sens.tukeyHSD.csv"
)
class.aa.sens.turkey
```

#### Precision test

A one-way ANOVA was performed to determine if the mean precision was statistically significantly different between the tools.

```{r classAAprecAnova, echo=FALSE}
aa_subset_prec.df <- data.frame(aa_subset_prec)
aa_subset_prec.df$Prediction_tool <- as.factor(aa_subset_prec.df$Prediction_tool)
# subset_prec.df$Prediction_tool <- factor(subset_prec.df$Prediction_tool, levels = classifiers_full)

aa_subset_prec.df <- aa_subset_prec.df[complete.cases(aa_subset_prec.df), ]

class.aa.prec.anova <- aov(Statistic_value ~ Prediction_tool, data = aa_subset_prec.df)
summary(class.aa.prec.anova)
```

The means are not statistically significantly different.

#### F1 test

A one-way ANOVA was performed to determine if the mean F1-scores was statistically significantly different between the tools.

```{r classAAF1Anova, echo=FALSE}
aa_subset_f1.df <- data.frame(aa_subset_f1)
aa_subset_f1.df$Prediction_tool <- as.factor(aa_subset_f1.df$Prediction_tool)
# subset_f1.df$Prediction_tool <- factor(subset_f1.df$Prediction_tool, levels = classifiers_full)

aa_subset_f1.df <- aa_subset_f1.df[complete.cases(aa_subset_f1.df), ]

class.aa.f1.anova <- aov(Statistic_value ~ Prediction_tool, data = aa_subset_f1.df)
summary(class.aa.f1.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classAAF1Turkey, echo=FALSE}
class.aa.f1.turkey <- TukeyHSD(class.aa.f1.anova, conf.level=.95)
write.csv(
  as.data.frame(class.aa.f1.turkey$Prediction_tool),
  "../report/cazy_class_classification/AA/class.aa.f1.tukeyHSD.csv"
)
class.aa.f1.turkey
```

#### Accuracy test

A one-way ANOVA was performed to determine if the mean accuracy was statistically significantly different between the tools.

```{r classAAaccAnova, echo=FALSE}
aa_subset_acc.df <- data.frame(aa_subset_acc)
aa_subset_acc.df$Prediction_tool <- as.factor(aa_subset_acc.df$Prediction_tool)
# subset_acc.df$Prediction_tool <- factor(subset_acc.df$Prediction_tool, levels = classifiers_full)

aa_subset_acc.df <- aa_subset_acc.df[complete.cases(aa_subset_acc.df), ]

class.aa.acc.anova <- aov(Statistic_value ~ Prediction_tool, data = aa_subset_acc.df)
summary(class.aa.acc.anova)
```

The means are statistically significantly different.




### CBM class classification

```{r cbmSummaryTableClass, echo=FALSE}
cbm_class_dfs <- get.class.summary.table.and.plots(cazy_class_df, 'CBM')
cbm_binary_summary_df <- cbm_class_dfs[[1]]
cbm_subset_spec <- cbm_class_dfs[[2]]
cbm_subset_sens <- cbm_class_dfs[[3]]
cbm_subset_prec <- cbm_class_dfs[[4]]
cbm_subset_f1 <- cbm_class_dfs[[5]]
cbm_subset_acc <- cbm_class_dfs[[6]]

dir.create("../report/cazy_class_classification/CBM/", recursive=FALSE)
write.csv(cbm_binary_summary_df, "../report/cazy_class_classification/CBM/summaryTableCBM.csv", row.names=FALSE)

kable(cbm_binary_summary_df, caption="Overall performance of CAZyme classifiers classification of CBM class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r classCbmSpecSens, echo=FALSE, fig.cap="Scatter plot of sensitivity against specificity for predicting CBM CAZy class members per CAZyme classier, overlaying a density map."}
p.class.cbm.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'CBM', 0.8, 0.001, 0.02,
  colour_set_tools,
  classifiers_full
)
p.class.cbm.spec.sens
```

```{r saveclassCbmSpecSens, include=FALSE}
pdf(file = "../report/cazy_class_classification/CBM/CbmSpecVsSens.pdf", width = 11.25, height = 7)
p.class.cbm.spec.sens
dev.off()
```

```{r cbmClassCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of CBM class classification, plotting the mean plus and minus the 95% confidence interval."}
p.cbm.class.ci <- plot.cazy.class.ci(
  cbm_subset_spec,
  cbm_subset_sens,
  cbm_subset_prec,
  cbm_subset_f1,
  cbm_subset_acc
)
p.cbm.class.ci
```

```{r cbmClassCIsave, include=FALSE}
pdf(file = "../report/cazy_class_classification/CBM/CbmCI.pdf", width = 6.3, height = 8.2)
p.cbm.class.ci
dev.off()
```

```{r cbmClassClassificationScatterPlot, echo=FALSE, fig.cap="One dimensional scatter plot of the statistical parameters per test set for the classification of CBM class members, overlaying a box plot"}

cbm.scatter.plt <- build.cazy.class.scatter.plt(cbm_subset_spec, cbm_subset_sens, cbm_subset_prec, cbm_subset_f1, cbm_subset_acc)
cbm.scatter.plt
```

```{r cbmClassScattersave, include=FALSE}
pdf(file = "../report/cazy_class_classification/CBM/classPCbmScatter.pdf", width = 6.3, height = 8.2)
cbm.scatter.plt
dev.off()
```


#### Specificity test

A one-way ANOVA was performed to determine if the mean specificity was statistically significantly different between the tools.

```{r classCBMspecAnova, echo=FALSE}
cbm_subset_spec.df <- data.frame(cbm_subset_spec)
cbm_subset_spec.df$Prediction_tool <- as.factor(cbm_subset_spec.df$Prediction_tool)
# subset_spec.df$Prediction_tool <- factor(subset_spec.df$Prediction_tool, levels = classifiers_full)

cbm_subset_spec.df <- cbm_subset_spec.df[complete.cases(cbm_subset_spec.df), ]

class.cbm.spec.anova <- aov(Statistic_value ~ Prediction_tool, data = cbm_subset_spec.df)
summary(class.cbm.spec.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classCBMspecTurkey, echo=FALSE}
class.cbm.spec.turkey <- TukeyHSD(class.cbm.spec.anova, conf.level=.95)
write.csv(
  as.data.frame(class.cbm.spec.turkey$Prediction_tool),
  "../report/cazy_class_classification/CBM/class.cbm.spec.tukeyHSD.csv"
)
class.cbm.spec.turkey
```

#### Sensitivity test

A one-way ANOVA was performed to determine if the mean sensitivity was statistically significantly different between the tools.

```{r classCBMsensAnova, echo=FALSE}
cbm_subset_sens.df <- data.frame(cbm_subset_sens)
cbm_subset_sens.df$Prediction_tool <- as.factor(cbm_subset_sens.df$Prediction_tool)
# subset_sens.df$Prediction_tool <- factor(subset_sens.df$Prediction_tool, levels = classifiers_full)

cbm_subset_sens.df <- cbm_subset_sens.df[complete.cases(cbm_subset_sens.df), ]

class.cbm.sens.anova <- aov(Statistic_value ~ Prediction_tool, data = cbm_subset_sens.df)
summary(class.cbm.sens.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classCBMsensTurkey, echo=FALSE}
class.cbm.sens.turkey <- TukeyHSD(class.cbm.sens.anova, conf.level=.95)
write.csv(
  as.data.frame(class.cbm.sens.turkey$Prediction_tool),
  "../report/cazy_class_classification/CBM/class.cbm.sens.tukeyHSD.csv"
)
class.cbm.sens.turkey
```

#### Precision test

A one-way ANOVA was performed to determine if the mean precision was statistically significantly different between the tools.

```{r classCBMprecAnova, echo=FALSE}
cbm_subset_prec.df <- data.frame(cbm_subset_prec)
cbm_subset_prec.df$Prediction_tool <- as.factor(cbm_subset_prec.df$Prediction_tool)
# subset_prec.df$Prediction_tool <- factor(subset_prec.df$Prediction_tool, levels = classifiers_full)

cbm_subset_prec.df <- cbm_subset_prec.df[complete.cases(cbm_subset_prec.df), ]

class.cbm.prec.anova <- aov(Statistic_value ~ Prediction_tool, data = cbm_subset_prec.df)
summary(class.cbm.prec.anova)
```

The means are statistically significantly different.


```{r classCBMprecTurkey, echo=FALSE}
class.cbm.prec.turkey <- TukeyHSD(class.cbm.prec.anova, conf.level=.95)
write.csv(
  as.data.frame(class.cbm.prec.turkey$Prediction_tool),
  "../report/cazy_class_classification/CBM/class.cbm.prec.tukeyHSD.csv"
)
class.cbm.prec.turkey
```

#### F1 test

A one-way ANOVA was performed to determine if the mean F1-scores was statistically significantly different between the tools.

```{r classCBMF1Anova, echo=FALSE}
cbm_subset_f1.df <- data.frame(cbm_subset_f1)
cbm_subset_f1.df$Prediction_tool <- as.factor(cbm_subset_f1.df$Prediction_tool)
# subset_f1.df$Prediction_tool <- factor(subset_f1.df$Prediction_tool, levels = classifiers_full)

cbm_subset_f1.df <- cbm_subset_f1.df[complete.cases(cbm_subset_f1.df), ]

class.cbm.f1.anova <- aov(Statistic_value ~ Prediction_tool, data = cbm_subset_f1.df)
summary(class.cbm.f1.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classCBMF1Turkey, echo=FALSE}
class.cbm.f1.turkey <- TukeyHSD(class.cbm.f1.anova, conf.level=.95)
write.csv(
  as.data.frame(class.cbm.f1.turkey$Prediction_tool),
  "../report/cazy_class_classification/CBM/class.cbm.f1.tukeyHSD.csv"
)
class.cbm.f1.turkey
```

#### Accuracy test

A one-way ANOVA was performed to determine if the mean accuracy was statistically significantly different between the tools.

```{r classCBMaccAnova, echo=FALSE}
cbm_subset_acc.df <- data.frame(cbm_subset_acc)
cbm_subset_acc.df$Prediction_tool <- as.factor(cbm_subset_acc.df$Prediction_tool)
# subset_acc.df$Prediction_tool <- factor(subset_acc.df$Prediction_tool, levels = classifiers_full)

cbm_subset_acc.df <- cbm_subset_acc.df[complete.cases(cbm_subset_acc.df), ]

class.cbm.acc.anova <- aov(Statistic_value ~ Prediction_tool, data = cbm_subset_acc.df)
summary(class.cbm.acc.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classCBMaccTurkey, echo=FALSE}
class.cbm.acc.turkey <- TukeyHSD(class.cbm.acc.anova, conf.level=.95)
write.csv(
  as.data.frame(class.cbm.acc.turkey$Prediction_tool),
  "../report/cazy_class_classification/CBM/class.cbm.acc.tukeyHSD.csv"
)
class.cbm.acc.turkey
```






## Performance per statistic

Instead of facet wrapping the plots by statistic and producing a plot per CAZy class, we can produce a plot per statistic (sensitivity, precision, etc.) and facet wrap by CAZy class to facilitate comparing between CAZy classes, and evaluting the performance statistic by statistic.

For each statistical parameter, perform a two-way ANOVA to test for statistically significant differences between the CAZy classes, prediction tools and the interaction of the two. If a significant difference is found, follow up with a post hoc test to determine between which tools and classes there is a statistically significant difference.

### CAZy class Specificity

```{r classStatCiPlot, include=FALSE}
build.stat.ci.plot <- function(stat.name, mean.col, lower.ci.col, upper.ci.col, gh.df, gt.df, pl.df, ce.df, aa.df, cbm.df){
  means <- gh.df[[mean.col]]
  means <- append(means, gt.df[[mean.col]])
  means <- append(means, pl.df[[mean.col]])
  means <- append(means, ce.df[[mean.col]])
  means <- append(means, aa.df[[mean.col]])
  means <- append(means, cbm.df[[mean.col]])
  
  pts <- gh.df$Prediction_tool
  pts <- append(pts, gt.df$Prediction_tool)
  pts <- append(pts, pl.df$Prediction_tool)
  pts <- append(pts, ce.df$Prediction_tool)
  pts <- append(pts, aa.df$Prediction_tool)
  pts <- append(pts, cbm.df$Prediction_tool)
  
  lower.ci <- gh.df[[lower.ci.col]]
  lower.ci <- append(lower.ci, gt.df[[lower.ci.col]])
  lower.ci <- append(lower.ci, pl.df[[lower.ci.col]])
  lower.ci <- append(lower.ci, ce.df[[lower.ci.col]])
  lower.ci <- append(lower.ci, aa.df[[lower.ci.col]])
  lower.ci <- append(lower.ci, cbm.df[[lower.ci.col]])
  
  upper.ci <- gh.df[[upper.ci.col]]
  upper.ci <- append(upper.ci, gt.df[[upper.ci.col]])
  upper.ci <- append(upper.ci, pl.df[[upper.ci.col]])
  upper.ci <- append(upper.ci, ce.df[[upper.ci.col]])
  upper.ci <- append(upper.ci, aa.df[[upper.ci.col]])
  upper.ci <- append(upper.ci, cbm.df[[upper.ci.col]])
  
  cazy.class <- rep('GH', nrow(gh.df))
  cazy.class <- append(cazy.class, rep('GT', nrow(gt.df)))
  cazy.class <- append(cazy.class, rep('PL', nrow(pl.df)))
  cazy.class <- append(cazy.class, rep('CE', nrow(ce.df)))
  cazy.class <- append(cazy.class, rep('AA', nrow(aa.df)))
  cazy.class <- append(cazy.class, rep('CBM', nrow(cbm.df)))
  
  df <- data.frame(means, pts, lower.ci, upper.ci, cazy.class)
  colnames(df) <- c('Mean', 'Prediction_tool', 'LowerCI', 'UpperCI', 'CAZy_class')
  
  df$Prediction_tool <- factor(df$Prediction_tool, levels = classifiers_full) # set order data is presented
  df$CAZy_class <- factor(df$CAZy_class, levels = cazy_class_list) # set order data is presented
  
  p.class.stat.ci = ggplot(df %>% dplyr::group_by(CAZy_class),
                   aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
    scale_colour_manual(values = colour_set_tools) +
    geom_point() +
    geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
    geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
    geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
    theme(legend.position = "none",
          plot.background = element_rect(fill = figbg, color = figbg),
          axis.text=element_text(size=10),
          axis.title=element_text(size=12,face="bold"),
          axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
    xlab("Classifier") + 
    ylab(stat.name) +
    facet_wrap(~ CAZy_class, ncol=2)
  
  return(p.class.stat.ci)
  
}
```

```{r classSpec, echo=FALSE}
p.class.spec.ci <- build.stat.ci.plot(
  'Specificity',
  'Spec Mean', 'Spec CI Lower', 'Spec CI Upper',
  gh_binary_summary_df,
  gt_binary_summary_df,
  pl_binary_summary_df,
  ce_binary_summary_df,
  aa_binary_summary_df,
  cbm_binary_summary_df
)
p.class.spec.ci
```

```{r classSpecSave4, include=FALSE}
pdf(file = "../report/cazy_class_classification/Specificity.CI.pdf", width = 6.3, height = 8.2)
p.class.spec.ci
dev.off()
```


```{r cazyClassSpecScatter, echo=FALSE}

build.cazy.class.scatter.STAT.plt <- function(gh.sub.df, gt.sub.df, pl.sub.df, ce.sub.df, aa.sub.df, cbm.sub.df, stat.param){
  # provide df containing data for only one statitical parameter
  
  bin.mean.df <- rbind(gh.sub.df, gt.sub.df)
  bin.mean.df <- rbind(bin.mean.df, pl.sub.df)
  bin.mean.df <- rbind(bin.mean.df, ce.sub.df)
  bin.mean.df <- rbind(bin.mean.df, aa.sub.df)
  bin.mean.df <- rbind(bin.mean.df, cbm.sub.df)
  
  # replace 'FBeta-score' with 'F1-score'
  bin.mean.df[bin.mean.df == 'FBeta-score'] <- 'F1-score'
  bin.mean.df[bin.mean.df == 'Fbeta_score'] <- 'F1-score'
  
  bin.mean.df$Prediction_tool <- factor(bin.mean.df$Prediction_tool, levels = classifiers_full)
  bin.mean.df$CAZy_class <- factor(bin.mean.df$CAZy_class, levels=cazy_class_list)
  bin.mean.df$Statistic_parameter <- factor(bin.mean.df$Statistic_parameter, levels = c("Specificity", "Sensitivity", "Precision", "F1-score", "Accuracy"))
  
  binary.mean.p = ggplot(bin.mean.df %>% dplyr::group_by(CAZy_class),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
    geom_boxplot(outlier.shape=NA) +
    geom_jitter(width=0.1, height=0, size=0.3) +
    geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
    geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
    scale_fill_manual(values = colour_set_tools) +
    theme(legend.position = "none",
          plot.background = element_rect(fill = figbg, color = figbg),
          axis.text=element_text(size=10),
          axis.title=element_text(size=11,face="bold"),
          axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
    xlab("Classifier") + 
    ylab(stat.param) +
    facet_wrap(~ CAZy_class, ncol=2)
  
  return(binary.mean.p)
}

p.class.spec.scatter <- build.cazy.class.scatter.STAT.plt(gh_subset_spec,gt_subset_spec, pl_subset_spec, ce_subset_spec, aa_subset_spec, cbm_subset_spec, "Specificity")
p.class.spec.scatter
```
```{r classSpecScatterSave, include=FALSE}
pdf(file = "../report/cazy_class_classification/Specificity.scatter.pdf", width = 6.3, height = 8.2)
p.class.spec.scatter
dev.off()
```

Two-way ANOVA to test for differences between the CAZy classes and prediction tools.

```{r clspecTwoANOVA, echo=FALSE}
cl.spec.2.aov <- aov(Statistic_value ~ CAZy_class * Prediction_tool, data = class_subset_spec.df)
write.csv(
  summary(cl.spec.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_class_classification/class.spec.two-way-ANOVA.csv"
)
summary(cl.spec.2.aov)
```
Statistically significant difference was detected, so follow up with a Tukey test to determine if there are differences between specific groups.

```{r clspecTukey, echo=FALSE}
class.spec.tukey.2 <- TukeyHSD(cl.spec.2.aov, "CAZy_class:Prediction_tool")
write.csv(
  as.data.frame(class.spec.tukey.2$`CAZy_class:Prediction_tool`),
  "../report/cazy_class_classification/class.spec.anova2.tukeyHSD.csv"
)
class.spec.tukey.2
```


### CAZy class sensitivity

```{r classSens, echo=FALSE}
p.class.sens.ci <- build.stat.ci.plot(
  'Sensitivity',
  'Sens Mean', 'Sens CI Lower', 'Sens CI Upper',
  gh_binary_summary_df,
  gt_binary_summary_df,
  pl_binary_summary_df,
  ce_binary_summary_df,
  aa_binary_summary_df,
  cbm_binary_summary_df
)
p.class.sens.ci
```

```{r classSpecSave2, include=FALSE}
pdf(file = "../report/cazy_class_classification/Sensitivity.CI.pdf", width = 6.3, height = 8.2)
p.class.sens.ci
dev.off()
```

```{r cazyClassSensScatter, echo=FALSE}
p.class.sens.scatter <- build.cazy.class.scatter.STAT.plt(gh_subset_sens,gt_subset_sens, pl_subset_sens, ce_subset_sens, aa_subset_sens, cbm_subset_sens, "Sensitivity")
p.class.sens.scatter
```

```{r classSensScatterSave, include=FALSE}
pdf(file = "../report/cazy_class_classification/Sensitivity.scatter.pdf", width = 6.3, height = 8.2)
p.class.sens.scatter
dev.off()
```

Two-way ANOVA to test for differences between the CAZy classes and prediction tools.

```{r clsensTwoANOVA, echo=FALSE}
cl.sens.2.aov <- aov(Statistic_value ~ CAZy_class * Prediction_tool, data = class_subset_sens.df)
write.csv(
  summary(cl.sens.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_class_classification/class.sens.two-way-ANOVA.csv"
)
summary(cl.sens.2.aov)
```

Statistically significant difference was detected, so follow up with a Tukey test to determine if there are differences between specific groups.

```{r clsensTukey, echo=FALSE}
class.sens.tukey.2 <- TukeyHSD(cl.sens.2.aov, "CAZy_class:Prediction_tool")
write.csv(
  as.data.frame(class.sens.tukey.2$`CAZy_class:Prediction_tool`),
  "../report/cazy_class_classification/class.sens.anova2.tukeyHSD.csv"
)
class.sens.tukey.2
```

### CAZy class precision

```{r classPrec, echo=FALSE}
p.class.prec.ci <- build.stat.ci.plot(
  'Precision',
  'Prec Mean', 'Prec CI Lower', 'Prec CI Upper',
  gh_binary_summary_df,
  gt_binary_summary_df,
  pl_binary_summary_df,
  ce_binary_summary_df,
  aa_binary_summary_df,
  cbm_binary_summary_df
)
p.class.prec.ci
```

```{r classPrecSave, include=FALSE}
pdf(file = "../report/cazy_class_classification/Precision.CI.pdf", width = 6.3, height = 8.2)
p.class.prec.ci
dev.off()
```

```{r cazyClassPrecScatter, echo=FALSE}
p.class.prec.scatter <- build.cazy.class.scatter.STAT.plt(gh_subset_prec,gt_subset_prec, pl_subset_prec, ce_subset_prec, aa_subset_prec, cbm_subset_prec, "Precision")
p.class.prec.scatter
```

```{r classPrecScatterSave, include=FALSE}
pdf(file = "../report/cazy_class_classification/Precision.scatter.pdf", width = 6.3, height = 8.2)
p.class.prec.scatter
dev.off()
```

Two-way ANOVA to test for differences between the CAZy classes and prediction tools.

```{r clprecTwoANOVA, echo=FALSE}
cl.prec.2.aov <- aov(Statistic_value ~ CAZy_class * Prediction_tool, data = class_subset_prec.df)
write.csv(
  summary(cl.prec.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_class_classification/class.prec.two-way-ANOVA.csv"
)
summary(cl.prec.2.aov)
```
Statistically significant difference was detected, so follow up with a Tukey test to determine if there are differences between specific groups.

```{r clprecTukey, echo=FALSE}
class.prec.tukey.2 <- TukeyHSD(cl.prec.2.aov, "CAZy_class:Prediction_tool")
write.csv(
  as.data.frame(class.prec.tukey.2$`CAZy_class:Prediction_tool`),
  "../report/cazy_class_classification/class.prec.anova2.tukeyHSD.csv"
)
class.prec.tukey.2
```

### CAZy class F1-score

```{r classF1, echo=FALSE}
p.class.f1.ci <- build.stat.ci.plot(
  'F1-score',
  'F1-score Mean', 'F1-score CI Lower', 'F1-score CI Upper',
  gh_binary_summary_df,
  gt_binary_summary_df,
  pl_binary_summary_df,
  ce_binary_summary_df,
  aa_binary_summary_df,
  cbm_binary_summary_df
)
p.class.f1.ci
```

```{r classSpecSave3, include=FALSE}
pdf(file = "../report/cazy_class_classification/F1.CI.pdf", width = 6.3, height = 8.2)
p.class.f1.ci
dev.off()

build.stat.ci.plot.limit <- function(stat.name, mean.col, lower.ci.col, upper.ci.col, gh.df, gt.df, pl.df, ce.df, aa.df, cbm.df){
  means <- gh.df[[mean.col]]
  means <- append(means, gt.df[[mean.col]])
  means <- append(means, pl.df[[mean.col]])
  means <- append(means, ce.df[[mean.col]])
  means <- append(means, aa.df[[mean.col]])
  means <- append(means, cbm.df[[mean.col]])
  
  pts <- gh.df$Prediction_tool
  pts <- append(pts, gt.df$Prediction_tool)
  pts <- append(pts, pl.df$Prediction_tool)
  pts <- append(pts, ce.df$Prediction_tool)
  pts <- append(pts, aa.df$Prediction_tool)
  pts <- append(pts, cbm.df$Prediction_tool)
  
  lower.ci <- gh.df[[lower.ci.col]]
  lower.ci <- append(lower.ci, gt.df[[lower.ci.col]])
  lower.ci <- append(lower.ci, pl.df[[lower.ci.col]])
  lower.ci <- append(lower.ci, ce.df[[lower.ci.col]])
  lower.ci <- append(lower.ci, aa.df[[lower.ci.col]])
  lower.ci <- append(lower.ci, cbm.df[[lower.ci.col]])
  
  upper.ci <- gh.df[[upper.ci.col]]
  upper.ci <- append(upper.ci, gt.df[[upper.ci.col]])
  upper.ci <- append(upper.ci, pl.df[[upper.ci.col]])
  upper.ci <- append(upper.ci, ce.df[[upper.ci.col]])
  upper.ci <- append(upper.ci, aa.df[[upper.ci.col]])
  upper.ci <- append(upper.ci, cbm.df[[upper.ci.col]])
  
  cazy.class <- rep('GH', nrow(gh.df))
  cazy.class <- append(cazy.class, rep('GT', nrow(gt.df)))
  cazy.class <- append(cazy.class, rep('PL', nrow(pl.df)))
  cazy.class <- append(cazy.class, rep('CE', nrow(ce.df)))
  cazy.class <- append(cazy.class, rep('AA', nrow(aa.df)))
  cazy.class <- append(cazy.class, rep('CBM', nrow(cbm.df)))
  
  df <- data.frame(means, pts, lower.ci, upper.ci, cazy.class)
  colnames(df) <- c('Mean', 'Prediction_tool', 'LowerCI', 'UpperCI', 'CAZy_class')
  
  df$Prediction_tool <- factor(df$Prediction_tool, levels = classifiers_full) # set order data is presented
  df$CAZy_class <- factor(df$CAZy_class, levels = cazy_class_list) # set order data is presented
  
  p.class.stat.ci = ggplot(df %>% dplyr::group_by(CAZy_class),
                   aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
    scale_colour_manual(values = colour_set_tools) +
    geom_point() +
    geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
    geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
    geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
    ylim(0.45,1.02) +
    theme(legend.position = "none",
          plot.background = element_rect(fill = figbg, color = figbg),
          axis.text=element_text(size=10),
          axis.title=element_text(size=12,face="bold"),
          axis.text.x=element_text(angle = 45, size=8, vjust=1, hjust=1)) +
    xlab("Classifier") + 
    ylab(stat.name) +
    facet_wrap(~ CAZy_class, ncol=2)
  
  return(p.class.stat.ci)
  
}

p.class.f1.ci.shrtned <- build.stat.ci.plot.limit(
  'F1-score',
  'F1-score Mean', 'F1-score CI Lower', 'F1-score CI Upper',
  gh_binary_summary_df,
  gt_binary_summary_df,
  pl_binary_summary_df,
  ce_binary_summary_df,
  aa_binary_summary_df,
  cbm_binary_summary_df
)

pdf(file = "../report/cazy_class_classification/F1.CI.shortened.pdf", width = 6.3, height = 8)
p.class.f1.ci.shrtned
dev.off()
```

```{r cazyClassF1Scatter, echo=FALSE}
p.class.f1.scatter <- build.cazy.class.scatter.STAT.plt(gh_subset_f1,gt_subset_f1, pl_subset_f1, ce_subset_f1, aa_subset_f1, cbm_subset_f1, "F1-score")
p.class.f1.scatter
```

```{r classF1ScatterSave, include=FALSE}
pdf(file = "../report/cazy_class_classification/F1-score.scatter.pdf", width = 6.3, height = 8.2)
p.class.f1.scatter
dev.off()
```

Two-way ANOVA to test for differences between the CAZy classes and prediction tools.

```{r clf1TwoANOVA, echo=FALSE}
cl.f1.2.aov <- aov(Statistic_value ~ CAZy_class * Prediction_tool, data = class_subset_f1.df)
write.csv(
  summary(cl.f1.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_class_classification/class.f1.two-way-ANOVA.csv"
)
summary(cl.f1.2.aov)
```
Statistically significant difference was detected, so follow up with a Tukey test to determine if there are differences between specific groups.

```{r clf1Tukey, echo=FALSE}
class.f1.tukey.2 <- TukeyHSD(cl.f1.2.aov, "CAZy_class:Prediction_tool")
write.csv(
  as.data.frame(class.f1.tukey.2$`CAZy_class:Prediction_tool`),
  "../report/cazy_class_classification/class.f1.anova2.tukeyHSD.csv"
)
class.f1.tukey.2
```

### CAZy class accuracy

```{r classAcc, echo=FALSE}
p.class.acc.ci <- build.stat.ci.plot(
  'Accuracy',
  'Acc Mean', 'Acc CI Lower', 'Acc CI Upper',
  gh_binary_summary_df,
  gt_binary_summary_df,
  pl_binary_summary_df,
  ce_binary_summary_df,
  aa_binary_summary_df,
  cbm_binary_summary_df
)
p.class.acc.ci
```

```{r classSpecSave, include=FALSE}
pdf(file = "../report/cazy_class_classification/Accuracy.CI.pdf", width = 6.3, height = 8.2)
p.class.acc.ci
dev.off()
```

```{r cazyClassAccScatter, echo=FALSE}
p.class.acc.scatter <- build.cazy.class.scatter.STAT.plt(gh_subset_acc,gt_subset_acc, pl_subset_acc, ce_subset_acc, aa_subset_acc, cbm_subset_acc, "Accuracy")
p.class.acc.scatter
```

```{r classAccScatterSave, include=FALSE}
pdf(file = "../report/cazy_class_classification/Accuracy.scatter.pdf", width = 6.3, height = 8.2)
p.class.acc.scatter
dev.off()
```

Two-way ANOVA to test for differences between the CAZy classes and prediction tools.

```{r claccTwoANOVA, echo=FALSE}
cl.acc.2.aov <- aov(Statistic_value ~ CAZy_class * Prediction_tool, data = class_subset_acc.df)
write.csv(
  summary(cl.acc.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_class_classification/class.acc.two-way-ANOVA.csv"
)
summary(cl.acc.2.aov)
```
Statistically significant difference was detected, so follow up with a Tukey test to determine if there are differences between specific groups.

```{r claccTukey, echo=FALSE}
class.acc.tukey.2 <- TukeyHSD(cl.acc.2.aov, "CAZy_class:Prediction_tool")
write.csv(
  as.data.frame(class.acc.tukey.2$`CAZy_class:Prediction_tool`),
  "../report/cazy_class_classification/class.acc.anova2.tukeyHSD.csv"
)
class.acc.tukey.2
```

# CAZy class multilabel classification

A single CAZyme can be included in multiple CAZy classes leading to the multilabel classification of CAZymes. To address this and evaluate the multilabel classification of CAZy classes the Rand Index (RI) and Adjusted Rand Index (ARI) were calculated.

The RI is the measure of accuracy across all potential classifications of a protein. The RI ranges from 0 (no correct annotations) to 1 (all annotations correct). The ARI is the RI adjusted for chance, where 0 is the equivalent to assigning the CAZy class annotations randomly, -1 where the annotations are systematically handed out incorrectly and 1 where the annotations are all correct.

```{r riCalcClass, include=FALSE}
class_ri_ari_raw_df$Prediction_tool <- factor(class_ri_ari_raw_df$Prediction_tool, levels = classifiers_full) # set order data is presented

class_ri_stats_df <- class_ri_ari_raw_df %>% 
  dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Rand_index_binary)[3],
    "Mean"=mean(Rand_index_binary),
    "Upper CI"=CI(Rand_index_binary)[1],
    "Standard Deviation"=sd(Rand_index_binary)
  )

dir.create("../report/cazy_class_classification/multilabel_classification/", recursive=FALSE)
write.csv(class_ri_stats_df, "../report/cazy_class_classification/multilabel_classification/ri_table.csv", row.names=FALSE)

kable(class_ri_stats_df, caption="Rand Index of CAZyme classifier classification of CAZy class annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

One-way ANOVA to test for statistically significant differences between the means.

```{r classRiAnova, echo=FALSE}
class_ri_ari_raw_df$Prediction_tool <- as.factor(class_ri_ari_raw_df$Prediction_tool)
# subset_f1.df$Prediction_tool <- factor(subset_f1.df$Prediction_tool, levels = classifiers_full)

class_ri_ari_raw_df <- class_ri_ari_raw_df[complete.cases(class_ri_ari_raw_df), ]

class.ri.aov <- aov(Rand_index_freq ~ Prediction_tool, data = class_ri_ari_raw_df)
summary(class.ri.aov)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classRiTurkey, echo=FALSE}
class.ri.turkey <- TukeyHSD(class.ri.aov, conf.level=.95)
write.csv(
  as.data.frame(class.ri.turkey$Prediction_tool),
  "../report/cazy_class_classification/multilabel_classification/class.ri.tukeyHSD.csv"
)
class.ri.turkey
```


```{r riCalc, include=FALSE}
class_ri_ari_raw_df$Prediction_tool <- factor(class_ri_ari_raw_df$Prediction_tool, levels = classifiers_full) # set order data is presented

class_ri_stats_df <- class_ri_ari_raw_df %>% 
  dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Adjusted_Rand_index_binary)[3],
    "Mean"=mean(Adjusted_Rand_index_binary),
    "Upper CI"=CI(Adjusted_Rand_index_binary)[1],
    "Standard Deviation"=sd(Adjusted_Rand_index_binary)
  )

dir.create("../report/cazy_class_classification/multilabel_classification/", recursive=FALSE)
write.csv(class_ri_stats_df, "../report/cazy_class_classification/multilabel_classification/ari_table.csv", row.names=FALSE)

kable(class_ri_stats_df, caption="Adjusted Rand Index of CAZyme classifier classification of CAZy class annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

One-way ANOVA to test for statistically significant differences between the means.

```{r classARiAnova, echo=FALSE}
class_ri_ari_raw_df$Prediction_tool <- as.factor(class_ri_ari_raw_df$Prediction_tool)
# subset_f1.df$Prediction_tool <- factor(subset_f1.df$Prediction_tool, levels = classifiers_full)

class_ri_ari_raw_df <- class_ri_ari_raw_df[complete.cases(class_ri_ari_raw_df), ]

class.ari.aov <- aov(Adjusted_Rand_index_freq ~ Prediction_tool, data = class_ri_ari_raw_df)
summary(class.ari.aov)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r classARiTurkey, echo=FALSE}
class.ari.turkey <- TukeyHSD(class.ari.aov, conf.level=.95)
write.csv(
  as.data.frame(class.ari.turkey$Prediction_tool),
  "../report/cazy_class_classification/multilabel_classification/class.ari.tukeyHSD.csv"
)
class.ari.turkey
```

# CAZy class taxonomic performance

```{r classTaxPerformance, include=FALSE}
build.tax.class.stat.sum <- function(bact.df, euk.df, all.df, stat.param){
  # have Three dfs, all with the same structure
  # cols = X, Genomic_accession, Prediction_tool, CAZy_class, Statistic_parameter, Statistic_value
  # cazy_class_df: all.df - contains data for all test sets
  # bact.cazy_class_df: bact.df - only bacteria test sets - therefore, half the length of cazy_class_df
  # euk.cazy_class_df: euk.df - only eukaryotic test sets - therefore, half the length of cazy_class_df
  
	# subset stat param
	bact.df.stat <- bact.df[which(bact.df$Statistic_parameter == stat.param), ]
	bact.df.stat  <- bact.df.stat[complete.cases(bact.df.stat), ]
	bact.df.stat.sum <- bact.df.stat %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
		"Mean"=mean(Statistic_value),
		"Standard Deviation"=sd(Statistic_value),
		"Lower CI"=CI(Statistic_value)[3],
		"Upper CI"=CI(Statistic_value)[1]
	)
	bact.df.stat.sum$Tax_group = rep("Bacteria", nrow(bact.df.stat.sum))
	 
	euk.df.stat <- euk.df[which(euk.df$Statistic_parameter == stat.param), ]
	euk.df.stat  <- euk.df.stat[complete.cases(euk.df.stat), ]
	euk.df.stat.sum <- euk.df.stat %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
		"Mean"=mean(Statistic_value),
		"Standard Deviation"=sd(Statistic_value),
		"Lower CI"=CI(Statistic_value)[3],
		"Upper CI"=CI(Statistic_value)[1]
	)
	euk.df.stat.sum$Tax_group = rep("Eukaryote", nrow(euk.df.stat.sum))
	
  all.df.stat <- all.df[which(all.df$Statistic_parameter == stat.param), ]
	all.df.stat  <- all.df.stat[complete.cases(all.df.stat), ]
	all.df.stat.sum <- all.df.stat %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
		"Mean"=mean(Statistic_value),
		"Standard Deviation"=sd(Statistic_value),
		"Lower CI"=CI(Statistic_value)[3],
		"Upper CI"=CI(Statistic_value)[1]
	)
	all.df.stat.sum$Tax_group <- rep("All", nrow(all.df.stat.sum))
	
	df.stat.sum <- rbind(all.df.stat.sum, bact.df.stat.sum)
	df.stat.sum <- rbind(df.stat.sum, euk.df.stat.sum)
	
	df.stat.sum$Prediction_tool <- factor(df.stat.sum$Prediction_tool, levels=classifiers_full, ordered = TRUE) # set order data is presented
	
	return(df.stat.sum)
}

build.class.tax.p.scatter <- function(bact.df.stat, euk.df.stat, all.df.stat, stat.param){
	bact.df.stat <- bact.df.stat[which(bact.df.stat$Statistic_parameter == stat.param), ]
	bact.df.stat  <- bact.df.stat[complete.cases(bact.df.stat), ]
	bact.df.stat$Tax_group = rep("Bacteria", nrow(bact.df.stat))
	 
	euk.df.stat <- euk.df.stat[which(euk.df.stat$Statistic_parameter == stat.param), ]
	euk.df.stat  <- euk.df.stat[complete.cases(euk.df.stat), ]
	euk.df.stat$Tax_group = rep("Eukaryote", nrow(euk.df.stat))
	
	all.df.stat <- all.df.stat[which(all.df.stat$Statistic_parameter == stat.param), ]
	all.df.stat  <- all.df.stat[complete.cases(all.df.stat), ]
	all.df.stat$Tax_group <- rep('All', nrow(all.df.stat))
	
  df.stat.all <- rbind(all.df.stat, bact.df.stat)
	df.stat.all <- rbind(df.stat.all, euk.df.stat)
	
	class.stat.scat.p = ggplot(df.stat.all %>% dplyr::group_by(Prediction_tool),
						 aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
		geom_boxplot(outlier.shape=NA) +
		geom_jitter(width=0.1, height=0, size=0.3) +
		geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
		geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
		scale_fill_manual(values = colour_set) +
		theme(legend.position = "none",
			  plot.background = element_rect(fill = figbg, color = figbg),
			  axis.text=element_text(size=10),
			  axis.title=element_text(size=11,face="bold")) +
		xlab("Kingdom") + 
		ylab(stat.param) +
		facet_wrap(~ Prediction_tool, ncol=2)
	
	return(class.stat.scat.p)
}

build.class.tax.p <- function(stat_sum_df, stat.val){
	colnames(stat_sum_df) <- c('Prediction_tool', 'Mean', 'SD', 'LowerCI', 'UpperCI', 'Taxonomic_group')
	stat_sum_df$Prediction_tool <- factor(stat_sum_df$Prediction_tool, levels = classifiers_full)
	
	p.t.b.ci <- ggplot(stat_sum_df %>% dplyr::group_by(Taxonomic_group),
				  aes(x=Taxonomic_group, y=Mean, color=Taxonomic_group)) +
	  geom_point() +
	  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
	  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
	  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
	  scale_colour_manual(values = colour_set) +
	  theme(legend.position = "none",
			plot.background = element_rect(fill = figbg, color = figbg),
			axis.text=element_text(size=10),
			axis.title=element_text(size=12,face="bold")) +
	  xlab("Classifier") + 
	  ylab(stat.val) +
	  facet_wrap(~ Prediction_tool, ncol=2)
	  
	return(p.t.b.ci)
}

```

## Across all of CAZy

### Specificity

```{r classTaxSpec, echo=FALSE}
# have three dfs at this point, all with the same structure
# cols = X, Genomic_accession, Prediction_tool, CAZy_class, Statistic_parameter, Statistic_value
# cazy_class_df contains data for all test sets
# bact.cazy_class_df only bacteria test sets - therefore, half the length of cazy_class_df
# euk.cazy_class_df only eukaryotic test sets - therefore, half the length of cazy_class_df

cazy_class_df$Prediction_tool <- factor(cazy_class_df$Prediction_tool, levels=classifiers_full)
bact.cazy_class_df$Prediction_tool <- factor(bact.cazy_class_df$Prediction_tool, levels=classifiers_full)
euk.cazy_class_df$Prediction_tool <- factor(euk.cazy_class_df$Prediction_tool, levels=classifiers_full)

cazy_class_df.spec.sum.df <- build.tax.class.stat.sum(bact.cazy_class_df, euk.cazy_class_df, cazy_class_df, "Specificity")
cazy_class_df.spec.sum.df$Prediction_tool <- ordered(cazy_class_df.spec.sum.df$Prediction_tool, levels=classifiers_full)

dir.create("../report/cazy_class_classification/tax_performance", recursive=FALSE)
write.csv(cazy_class_df.spec.sum.df, "../report/cazy_class_classification/tax_performance/specificity.summaryTable.csv", row.names=FALSE)

kable(cazy_class_df.spec.sum.df, caption="Overall performance (represented by the Specificity) of CAZy class classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r classTaxSpecCi, echo=FALSE}
p.class.tax.spec.ci <- build.class.tax.p(cazy_class_df.spec.sum.df, "Specificity")
p.class.tax.spec.ci
```

```{r classTaxSpecScatter1, echo=FALSE}
# plot scatter plot of spec per test set
p.class.tax.spec.scatter <- build.class.tax.p.scatter(bact.cazy_class_df, euk.cazy_class_df, cazy_class_df, "Specificity")
p.class.tax.spec.scatter
```


```{r famTaxSpecSave2, include=FALSE}
# save plots
pdf(file = "../report/cazy_class_classification/tax_performance/spec.class.tax.ci.pdf",  width = 6.23, height = 8)
p.class.tax.spec.ci
dev.off()

pdf(file = "../report/cazy_class_classification/tax_performance/spec.class.tax.scatter.pdf",  width = 6.23, height = 8)
p.class.tax.spec.scatter
dev.off()
```


### Sensitivity

```{r classTaxSens, echo=FALSE}
cazy_class_df.sens.sum.df <- build.tax.class.stat.sum(bact.cazy_class_df, euk.cazy_class_df, cazy_class_df, "Sensitivity")
cazy_class_df.sens.sum.df$Prediction_tool <- ordered(cazy_class_df.sens.sum.df$Prediction_tool, levels=classifiers_full)

dir.create("../report/cazy_class_classification/tax_performance", recursive=FALSE)
write.csv(cazy_class_df.sens.sum.df, "../report/cazy_class_classification/tax_performance/sensitivity.summaryTable.csv", row.names=FALSE)

cazy_class_df.sens.sum.df.b.e <- cazy_class_df.sens.sum.df[which(cazy_class_df.sens.sum.df$Tax_group != "All"), ]
kable(cazy_class_df.sens.sum.df.b.e, caption="Overall performance (represented by the Sensitivity) of CAZy class classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r classTaxSensCi, echo=FALSE}

p.class.tax.sens.ci <- build.class.tax.p(cazy_class_df.sens.sum.df, "Sensitivity")
p.class.tax.sens.ci
```

```{r classTaxSensScatter, echo=FALSE}
# plot scatter plot of sens per test set
p.class.tax.sens.scatter <- build.class.tax.p.scatter(bact.cazy_class_df, euk.cazy_class_df, cazy_class_df, "Sensitivity")
p.class.tax.sens.scatter
```


```{r classTaxSensSave, include=FALSE}
# save plots
pdf(file = "../report/cazy_class_classification/tax_performance/sens.class.tax.ci.pdf",  width = 8, height = 11)
p.class.tax.sens.ci
dev.off()

pdf(file = "../report/cazy_class_classification/tax_performance/sens.class.tax.scatter.pdf",  width = 6.23, height = 8)
p.class.tax.sens.scatter
dev.off()
```

### Precision

```{r classTaxPrec, echo=FALSE}
cazy_class_df.prec.sum.df <- build.tax.class.stat.sum(bact.cazy_class_df, euk.cazy_class_df, cazy_class_df, "Precision")
cazy_class_df.prec.sum.df$Prediction_tool <- ordered(cazy_class_df.prec.sum.df$Prediction_tool, levels=classifiers_full)

dir.create("../report/cazy_class_classification/tax_performance", recursive=FALSE)
write.csv(cazy_class_df.prec.sum.df, "../report/cazy_class_classification/tax_performance/precision.summaryTable.csv", row.names=FALSE)

cazy_class_df.prec.sum.df.b.e <- cazy_class_df.prec.sum.df[which(cazy_class_df.prec.sum.df$Tax_group != "All"), ]
kable(cazy_class_df.prec.sum.df.b.e, caption="Overall performance (represented by the Precision) of CAZy class classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r classTaxPrecCi, echo=FALSE}
p.class.tax.prec.ci <- build.class.tax.p(cazy_class_df.prec.sum.df, "Precision")
p.class.tax.prec.ci
```

```{r classTaxPrecScatter, echo=FALSE}
# plot scatter plot of prec per test set
p.class.tax.prec.scatter <- build.class.tax.p.scatter(bact.cazy_class_df, euk.cazy_class_df, cazy_class_df, "Precision")
p.class.tax.prec.scatter
```


```{r classTaxPrecSave, include=FALSE}
# save plots
pdf(file = "../report/cazy_class_classification/tax_performance/prec.class.tax.ci.pdf",  width = 6.23, height = 8)
p.class.tax.prec.ci
dev.off()

pdf(file = "../report/cazy_class_classification/tax_performance/prec.class.tax.scatter.pdf",  width = 6.23, height = 8)
p.class.tax.prec.scatter
dev.off()
```

### F1-score

```{r classTaxF1, echo=FALSE}
bact.cazy_class_df[bact.cazy_class_df == 'Fbeta_score'] <- 'F1-score'
euk.cazy_class_df[euk.cazy_class_df == 'Fbeta_score'] <- 'F1-score'
cazy_class_df[cazy_class_df == 'Fbeta_score'] <- 'F1-score'
cazy_class_df.f1.sum.df <- build.tax.class.stat.sum(bact.cazy_class_df, euk.cazy_class_df, cazy_class_df, "F1-score")

y <- as.character(cazy_class_df.f1.sum.df$Prediction_tool)
cazy_class_df.f1.sum.df$Prediction_tool <- factor(y, levels=classifiers_full)
cazy_class_df.f1.sum.df$Prediction_tool <- ordered(cazy_class_df.f1.sum.df$Prediction_tool, levels=classifiers_full)
                     
dir.create("../report/cazy_class_classification/tax_performance", recursive=FALSE)
write.csv(cazy_class_df.f1.sum.df, "../report/cazy_class_classification/tax_performance/F1-score.summaryTable.csv", row.names=FALSE)

cazy_class_df.f1.sum.df.b.e <- cazy_class_df.f1.sum.df[which(cazy_class_df.f1.sum.df$Tax_group != "All"), ]
kable(cazy_class_df.f1.sum.df.b.e, caption="Overall performance (represented by the F1-score) of CAZy class classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r classTaxF1Ci, echo=FALSE}
p.class.tax.f1.ci <- build.class.tax.p(cazy_class_df.f1.sum.df, "F1-score")
p.class.tax.f1.ci
```

```{r classTaxF1Scatter, echo=FALSE}
# plot scatter plot of f1 per test set
p.class.tax.f1.scatter <- build.class.tax.p.scatter(bact.cazy_class_df, euk.cazy_class_df, cazy_class_df, "F1-score")
p.class.tax.f1.scatter
```


```{r classTaxF1Save2, include=FALSE}
# save plots
pdf(file = "../report/cazy_class_classification/tax_performance/f1.class.tax.ci.pdf",  width = 6.23, height = 8)
p.class.tax.f1.ci
dev.off()

pdf(file = "../report/cazy_class_classification/tax_performance/f1.class.tax.scatter.pdf",  width = 6.23, height = 8)
p.class.tax.f1.scatter
dev.off()
```

### Accuracy

```{r classTaxAcc, echo=FALSE}
cazy_class_df.acc.sum.df <- build.tax.class.stat.sum(bact.cazy_class_df, euk.cazy_class_df, cazy_class_df, "Accuracy")
cazy_class_df.acc.sum.df$Prediction_tool <- ordered(cazy_class_df.acc.sum.df$Prediction_tool, levels=classifiers_full)

dir.create("../report/cazy_class_classification/tax_performance", recursive=FALSE)
write.csv(cazy_class_df.acc.sum.df, "../report/cazy_class_classification/tax_performance/accuracy.summaryTable.csv", row.names=FALSE)

cazy_class_df.acc.sum.df.b.e <- cazy_class_df.acc.sum.df[which(cazy_class_df.acc.sum.df$Tax_group != "All"), ]
kable(cazy_class_df.acc.sum.df.b.e, caption="Overall performance (represented by the Accuracy) of CAZy class classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r classTaxAccCi, echo=FALSE}
p.class.tax.acc.ci <- build.class.tax.p(cazy_class_df.acc.sum.df, "Accuracy")
p.class.tax.acc.ci
```

```{r classTaxAccScatter, echo=FALSE}
# plot scatter plot of acc per test set
p.class.tax.acc.scatter <- build.class.tax.p.scatter(bact.cazy_class_df, euk.cazy_class_df, cazy_class_df, "Accuracy")
p.class.tax.acc.scatter
```


```{r classTaxAccSave2, include=FALSE}
# save plots
pdf(file = "../report/cazy_class_classification/tax_performance/acc.class.tax.ci.pdf",  width = 6.23, height = 8)
p.class.tax.acc.ci
dev.off()

pdf(file = "../report/cazy_class_classification/tax_performance/acc.class.tax.scatter.pdf",  width = 6.23, height = 8)
p.class.tax.acc.scatter
dev.off()
```


## Per CAZy class

```{r perCzyClassTax, include=FALSE}
build.class.tax.df <- function(bact.df.stat, euk.df.stat, all.df.stat, stat.param){
	bact.df.stat <- bact.df.stat[which(bact.df.stat$Statistic_parameter == stat.param), ]
	bact.df.stat  <- bact.df.stat[complete.cases(bact.df.stat), ]
	bact.df.stat$Tax_group = rep("Bacteria", nrow(bact.df.stat))
	 
	euk.df.stat <- euk.df.stat[which(euk.df.stat$Statistic_parameter == stat.param), ]
	euk.df.stat  <- euk.df.stat[complete.cases(euk.df.stat), ]
	euk.df.stat$Tax_group = rep("Eukaryote", nrow(euk.df.stat))
	
	all.df.stat <- all.df.stat[which(all.df.stat$Statistic_parameter == stat.param), ]
	all.df.stat  <- all.df.stat[complete.cases(all.df.stat), ]
	all.df.stat$Tax_group <- rep('All', nrow(all.df.stat))
	
  df.stat.all <- rbind(all.df.stat, bact.df.stat)
	df.stat.all <- rbind(df.stat.all, euk.df.stat)
	
	return(df.stat.all)
}


build.class.tax.per.class.p <- function(stat_sum_df, stat.val){
  # df has colnames:
  # Prediction_tool, Mean, Standard Deviation, LowerCI, UpperCI, Tax_group, CAZy_clas
	colnames(stat_sum_df) <- c('Prediction_tool', 'Mean', 'SD', 'LowerCI', 'UpperCI', 'Taxonomic_group', 'CAZy_class')
	stat_sum_df$Prediction_tool <- factor(stat_sum_df$Prediction_tool, levels = classifiers_full)
	stat_sum_df$CAZy_class <- factor(stat_sum_df$CAZy_class, levels=cazy_class_list)
	
	p.t.b.ci <- ggplot(stat_sum_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::group_by(Taxonomic_group),
				  aes(x=Prediction_tool, y=Mean, color=Taxonomic_group, alpha=0.85)) +
	  geom_point(aes(shape=Taxonomic_group), position=position_dodge(width=0.75)) +
	  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI), width = 1, position=position_dodge(width=0.75)) +
	  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
	  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
	  geom_vline(xintercept=stat_sum_df$Prediction_tool, linetype="dashed", color = "#9c9c9c") +
	  scale_colour_manual(values = colour_set) +
	  theme(legend.position = "bottom",
			plot.background = element_rect(fill = figbg, color = figbg),
			axis.text=element_text(size=10),
			axis.title=element_text(size=12,face="bold"),
			axis.text.x=element_text(angle = 90, size=9, vjust=0.45)) +
	  xlab("Classifier") + 
	  ylab(stat.val) +
	  facet_wrap(~ CAZy_class, ncol=2)
	  
	return(p.t.b.ci)
}

build.class.tax.p.scatter.per.cl <- function(all.df.stat, bact.df.stat, euk.df.stat, stat.param){
	bact.df.stat <- bact.df.stat[which(bact.df.stat$Statistic_parameter == stat.param), ]
	bact.df.stat  <- bact.df.stat[complete.cases(bact.df.stat), ]
	bact.df.stat$Tax_group = rep("Bacteria", nrow(bact.df.stat))
	 
	euk.df.stat <- euk.df.stat[which(euk.df.stat$Statistic_parameter == stat.param), ]
	euk.df.stat  <- euk.df.stat[complete.cases(euk.df.stat), ]
	euk.df.stat$Tax_group = rep("Eukaryote", nrow(euk.df.stat))
	
	all.df.stat <- all.df.stat[which(all.df.stat$Statistic_parameter == stat.param), ]
	all.df.stat  <- all.df.stat[complete.cases(all.df.stat), ]
	all.df.stat$Tax_group <- rep('All', nrow(all.df.stat))
	
	all.df.stat <- rbind(all.df.stat, bact.df.stat)
	all.df.stat <- rbind(all.df.stat, euk.df.stat)
	
	all.df.stat$Prediction_tool <- factor(all.df.stat$Prediction_tool, levels = classifiers_full)
	all.df.stat$CAZy_class <- factor(all.df.stat$CAZy_class, levels=cazy_class_list)
	
	class.stat.scat.p = ggplot(all.df.stat %>% dplyr::group_by(Prediction_tool) %>% dplyr::group_by(Tax_group),
						 aes(x=Prediction_tool, y=Statistic_value, fill=Tax_group)) +
		geom_boxplot(outlier.shape=NA, position=position_dodge(width=1)) +
		geom_jitter(position=position_dodge(width=1), size=0.3, aes(alpha=0.5)) +
		geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
		geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
	  geom_vline(xintercept=all.df.stat$Prediction_tool, linetype="dashed", color = "#9c9c9c") +
		scale_fill_manual(values = colour_set) +
		theme(legend.position = "none",
			  plot.background = element_rect(fill = figbg, color = figbg),
			  axis.text=element_text(size=10),
			  axis.title=element_text(size=11,face="bold"),
			  axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
		xlab("Kingdom") + 
		ylab(stat.param) +
		facet_wrap(~ CAZy_class, ncol=2)
	
	return(class.stat.scat.p)
}

build.class.tax.sum.GH <- function(cazy.class.df, bact.class.df, euk.class.df, stat_param){
  # 3 dataframes have the same structure.
  # colnames: "X", "Genomic_accession", "Prediction_tool", "CAZy_class", "Statistic_parameter", "Statistic_value"
  
  # subset for CAZy class
  bact.class_df.class <- bact.class.df[which(bact.class.df$CAZy_class == "GH"), ]
  euk.class_df.class <- euk.class.df[which(euk.class.df$CAZy_class == "GH"), ]
  cazy.class.df.class <- cazy.class.df[which(cazy.class.df$CAZy_class == "GH"), ]
  
  # subset statistical parameter
  bact.class_df.stat <- bact.class_df.class[which(bact.class_df.class$Statistic_parameter == stat_param), ]
  euk.class_df.stat <- euk.class_df.class[which(euk.class_df.class$Statistic_parameter == stat_param), ]
  cazy.class.df.stat <- cazy.class.df.class[which(cazy.class.df.class$Statistic_parameter == stat_param), ]
  
  bact.class_df.stat  <- bact.class_df.stat[complete.cases(bact.class_df.stat), ]
  euk.class_df.stat  <- euk.class_df.stat[complete.cases(euk.class_df.stat), ]
  cazy.class.df.stat  <- cazy.class.df.stat[complete.cases(cazy.class.df.stat), ]
  
  # calculate stats
  bact.means <- bact.class_df.stat %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Mean"=mean(Statistic_value),
    "Standard Deviation"=sd(Statistic_value),
    "LowerCI"=CI(Statistic_value)[3],
    "UpperCI"=CI(Statistic_value)[1]
  )
  euk.means <- euk.class_df.stat %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Mean"=mean(Statistic_value),
    "Standard Deviation"=sd(Statistic_value),
    "LowerCI"=CI(Statistic_value)[3],
    "UpperCI"=CI(Statistic_value)[1]
  )
  all.means <- cazy.class.df.stat %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Mean"=mean(Statistic_value),
    "Standard Deviation"=sd(Statistic_value),
    "LowerCI"=CI(Statistic_value)[3],
    "UpperCI"=CI(Statistic_value)[1]
  )
  
  all.means$Tax_group <- rep("All", nrow(all.means))
  bact.means$Tax_group <- rep("Bacteria", nrow(bact.means))
  euk.means$Tax_group <- rep("Eukaryote", nrow(euk.means))
  
  class_df.class.stat.tax <- rbind(all.means, bact.means)
  class_df.class.stat.tax <- rbind(class_df.class.stat.tax, euk.means)
  
  class_df.class.stat.tax$Prediction_tool <- factor(class_df.class.stat.tax$Prediction_tool, levels=classifiers_full)
  class_df.class.stat.tax$CAZy_class <- rep("GH", nrow(class_df.class.stat.tax))
  
  return(class_df.class.stat.tax)
}

build.class.tax.sum.class <- function(cazy.class.df, bact.class.df, euk.class.df, stat_param, cazy_class){
  # subset for CAZy class
  bact.class_df.class <- bact.class.df[which(bact.class.df$CAZy_class == cazy_class), ]
  euk.class_df.class <- euk.class.df[which(euk.class.df$CAZy_class == cazy_class), ]
  all.class_df.class <- cazy.class.df[which(cazy.class.df$CAZy_class == cazy_class), ]
  
  # subset statistical parameter
  bact.class_df.stat <- bact.class_df.class[which(bact.class_df.class$Statistic_parameter == stat_param), ]
  euk.class_df.stat <- euk.class_df.class[which(euk.class_df.class$Statistic_parameter == stat_param), ]
  all.class_df.stat <- all.class_df.class[which(all.class_df.class$Statistic_parameter == stat_param), ]
  
  bact.class_df.stat  <- bact.class_df.stat[complete.cases(bact.class_df.stat), ]
  euk.class_df.stat  <- euk.class_df.stat[complete.cases(euk.class_df.stat), ]
  all.class_df.stat  <- all.class_df.stat[complete.cases(all.class_df.stat), ]
  
  # calculate stats
  bact.means <- bact.class_df.stat %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Mean"=mean(Statistic_value),
    "Standard Deviation"=sd(Statistic_value),
    "LowerCI"=CI(Statistic_value)[3],
    "UpperCI"=CI(Statistic_value)[1]
  )
  euk.means <- euk.class_df.stat %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Mean"=mean(Statistic_value),
    "Standard Deviation"=sd(Statistic_value),
    "LowerCI"=CI(Statistic_value)[3],
    "UpperCI"=CI(Statistic_value)[1]
  )
  all.means <- all.class_df.stat %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Mean"=mean(Statistic_value),
    "Standard Deviation"=sd(Statistic_value),
    "LowerCI"=CI(Statistic_value)[3],
    "UpperCI"=CI(Statistic_value)[1]
  )
  
  all.means$Tax_group <- rep("All", nrow(all.means))
  bact.means$Tax_group <- rep("Bacteria", nrow(bact.means))
  euk.means$Tax_group <- rep("Eukaryote", nrow(euk.means))
  
  class_df.class.stat.tax <- rbind(all.means, bact.means)
  class_df.class.stat.tax <- rbind(class_df.class.stat.tax, euk.means)
  
  class_df.class.stat.tax$Prediction_tool <- factor(class_df.class.stat.tax$Prediction_tool, levels=classifiers_full)
  class_df.class.stat.tax$CAZy_class <- rep(cazy_class, nrow(class_df.class.stat.tax))
  
  return(class_df.class.stat.tax)
}

build.class.tax.sum <- function(cazy.class.df, bact.class.df, euk.class.df, stat_param){
  cazy.class.df  <- cazy.class.df[complete.cases(cazy.class.df), ]
  bact.class.df  <- bact.class.df[complete.cases(bact.class.df), ]
  euk.class.df  <- euk.class.df[complete.cases(euk.class.df), ]
  # 3 dataframes have the same structure.
  # colnames: "X", "Genomic_accession", "Prediction_tool", "CAZy_class", "Statistic_parameter", "Statistic_value"
  
  # auto parse CAZy class GH to produce starting point for the dataframe
  class_df.class.stat.tax <- build.class.tax.sum.GH(cazy.class.df, bact.class.df, euk.class.df, stat_param)
  
  for (cazy_class in cazy_class_list){
    if (cazy_class == "GH"){next}
    else {
      class_df.class.stat.tax.class <- build.class.tax.sum.class(cazy.class.df, bact.class.df, euk.class.df, stat_param, cazy_class)
      class_df.class.stat.tax <- rbind(class_df.class.stat.tax, class_df.class.stat.tax.class)
    }
  }
  
  return(class_df.class.stat.tax)
}

```


### Specificity

```{r classTaxSpecTable, echo=FALSE}
cazy_class_df$Prediction_tool <- factor(cazy_class_df$Prediction_tool, levels=classifiers_full)

spec.tax.class.sum.df <- build.class.tax.sum(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, "Specificity")

dir.create("../report/cazy_class_classification/tax_performance/per_cazy_class", recursive=FALSE)
write.csv(
  spec.tax.class.sum.df,
  "../report/cazy_class_classification/tax_performance/per_cazy_class/specificity.summaryTable.csv",
  row.names=FALSE
)

kable(spec.tax.class.sum.df, caption="Overall performance (represented by the Specificity) of CAZy class classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r classTaxSpecCiPerClass, echo=FALSE}
p.class.tax.spec.ci.per.cl <- build.class.tax.per.class.p(spec.tax.class.sum.df, "Specificity")
p.class.tax.spec.ci.per.cl
```


```{r classTaxSpecScatter, echo=FALSE}
p.class.tax.spec.scatter.per.cl <- build.class.tax.p.scatter.per.cl(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, "Specificity")
p.class.tax.spec.scatter.per.cl
```

```{r classTaxSpecSave, include=FALSE}
# save plots
pdf(file = "../report/cazy_class_classification/tax_performance/per_cazy_class/spec.class.tax.ci.pdf",  width = 11, height = 8)
p.class.tax.spec.ci.per.cl
dev.off()

pdf(file = "../report/cazy_class_classification/tax_performance/per_cazy_class/spec.class.tax.scatter.pdf",  width = 11, height = 8)
p.class.tax.spec.scatter.per.cl
dev.off()
```

#### Testing per CAZy class

For each CAZy class run a two-way ANOVA to determine if there statistically significant differences between the tools and taxonomic kingdoms.

```{r classTaxspecAnova, echo=FALSE}
build.class.tax.df <- function(all.df.stat, bact.df.stat, euk.df.stat, stat.param){
	bact.df.stat <- bact.df.stat[which(bact.df.stat$Statistic_parameter == stat.param), ]
	bact.df.stat  <- bact.df.stat[complete.cases(bact.df.stat), ]
	bact.df.stat$Tax_group = rep("Bacteria", nrow(bact.df.stat))
	 
	euk.df.stat <- euk.df.stat[which(euk.df.stat$Statistic_parameter == stat.param), ]
	euk.df.stat  <- euk.df.stat[complete.cases(euk.df.stat), ]
	euk.df.stat$Tax_group = rep("Eukaryote", nrow(euk.df.stat))
	
	all.df.stat <- all.df.stat[which(all.df.stat$Statistic_parameter == stat.param), ]
	all.df.stat  <- all.df.stat[complete.cases(all.df.stat), ]
	all.df.stat$Tax_group <- rep('All', nrow(all.df.stat))
	
  df.stat.all <- rbind(all.df.stat, bact.df.stat)
	df.stat.all <- rbind(df.stat.all, euk.df.stat)
	
	return(df.stat.all)
}

cl.spec.tax.df <- build.class.tax.df(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, 'Specificity')


for (cazy_class in c('GH','GT','PL','CE','AA','CBM')){
  two.anova.out <- formatted_string <- sprintf("../report/cazy_class_classification/tax_performance/%s.class.spec.two-way-ANOVA.csv", cazy_class)
  tukey.out <- formatted_string <- sprintf("../report/cazy_class_classification/tax_performance/%s.class.spec.classifiers.tukeyHSD.csv", cazy_class)
  
  # TWo-way ANOVA
  cl.tax.spec.2.aov <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = cl.spec.tax.df[cl.spec.tax.df$CAZy_class==cazy_class,])
  
  write.csv(
    summary(cl.tax.spec.2.aov) %>% lapply(tidy) %>% bind_rows(),
    file = two.anova.out
  )
  summary(cl.tax.spec.2.aov)
  print("----------------------------------------------------")
  print(cazy_class)
  print(summary(cl.tax.spec.2.aov))
  
  # Tukey HSD if found stat signigicant results
  aov.df <- summary(cl.tax.spec.2.aov) %>% lapply(tidy) %>% bind_rows()
  if (nrow(aov.df[aov.df$"p.value" < 0.05, ]) > 0){
    class.tax.spec.tukey <- TukeyHSD(cl.tax.spec.2.aov, "Tax_group:Prediction_tool")
    write.csv(
      as.data.frame(class.tax.spec.tukey$`Tax_group:Prediction_tool`),
      tukey.out
    )
    class.tax.spec.tukey
    
    tukey.df <- as.data.frame(class.tax.spec.tukey$`Tax_group:Prediction_tool`)
    if (nrow(tukey.df[tukey.df$"p adj" < 0.05, ] ) > 0){
      print(sprintf("Found pairs of statistically signficant different means for %s", cazy_class))
    }
  }
  
}
```


### Sensitivity

```{r classTaxSensTable, echo=FALSE}
cazy_class_df$Prediction_tool <- factor(cazy_class_df$Prediction_tool, levels=classifiers_full)

sens.tax.class.sum.df <- build.class.tax.sum(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, "Sensitivity")

dir.create("../report/cazy_class_classification/tax_performance/per_cazy_class", recursive=FALSE)
write.csv(
  sens.tax.class.sum.df,
  "../report/cazy_class_classification/tax_performance/per_cazy_class/sensitivity.summaryTable.csv",
  row.names=FALSE
)

kable(sens.tax.class.sum.df, caption="Overall performance (represented by the Sensitivity) of CAZy class classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r classTaxSensCiPerClass, echo=FALSE}
build.class.tax.per.class.p.LIMITED <- function(stat_sum_df, stat.val){
  # df has colnames:
  # Prediction_tool, Mean, Standard Deviation, LowerCI, UpperCI, Tax_group, CAZy_clas
	colnames(stat_sum_df) <- c('Prediction_tool', 'Mean', 'SD', 'LowerCI', 'UpperCI', 'Taxonomic_group', 'CAZy_class')
	stat_sum_df$Prediction_tool <- factor(stat_sum_df$Prediction_tool, levels = classifiers_full)
	stat_sum_df$CAZy_class <- factor(stat_sum_df$CAZy_class, levels=cazy_class_list)
	
	p.t.b.ci <- ggplot(stat_sum_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::group_by(Taxonomic_group),
				  aes(x=Prediction_tool, y=Mean, color=Taxonomic_group)) +
	  geom_point(aes(shape=Taxonomic_group), position=position_dodge(width=0.75)) +
	  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI), width = 1, position=position_dodge(width=0.75)) +
	  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
	  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
	  geom_vline(xintercept=stat_sum_df$Prediction_tool, linetype="dashed", color = "#9c9c9c") +
	  scale_colour_manual(values = colour_set) +
	  theme(legend.position = "bottom",
			plot.background = element_rect(fill = figbg, color = figbg),
			axis.text=element_text(size=10),
			axis.title=element_text(size=12,face="bold"),
			axis.text.x=element_text(angle = 90, size=9, vjust=0.45)) +
	  xlab("Classifier") + 
	  ylab(stat.val) +
	  ylim(0.4,1.07)+
	  facet_wrap(~ CAZy_class, ncol=2)
	  
	return(p.t.b.ci)
}

p.class.tax.sens.ci.per.cl <- build.class.tax.per.class.p.LIMITED(sens.tax.class.sum.df, "Sensitivity")
p.class.tax.sens.ci.per.cl
```


```{r classTaxSensScatterClass, echo=FALSE}
# build.class.tax.p.scatter.per.cl(cazy_class_df
p.class.tax.sens.scatter.per.cl <- build.class.tax.p.scatter.per.cl(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, "Sensitivity")
p.class.tax.sens.scatter.per.cl
```

```{r classTaxSensPerClassSave, include=FALSE}
# save plots
pdf(file = "../report/cazy_class_classification/tax_performance/per_cazy_class/sens.class.tax.ci.pdf",  width = 6, height = 8)
p.class.tax.sens.ci.per.cl
dev.off()

pdf(file = "../report/cazy_class_classification/tax_performance/per_cazy_class/sens.class.tax.scatter.pdf",  width = 11, height = 8)
p.class.tax.sens.scatter.per.cl
dev.off()
```

#### Testing per CAZy class

For each CAZy class run a two-way ANOVA to determine if there statistically significant differences between the tools and taxonomic kingdoms.

```{r classTaxsensAnova, echo=FALSE}
cl.sens.tax.df <- build.class.tax.df(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, 'Sensitivity')

for (cazy_class in c('GH','GT','PL','CE','AA','CBM')){
  two.anova.out <- formatted_string <- sprintf("../report/cazy_class_classification/tax_performance/%s.class.sens.two-way-ANOVA.csv", cazy_class)
  tukey.out <- formatted_string <- sprintf("../report/cazy_class_classification/tax_performance/%s.class.sens.classifiers.tukeyHSD.csv", cazy_class)
  
  # TWo-way ANOVA
  cl.tax.sens.2.aov <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = cl.sens.tax.df[cl.sens.tax.df$CAZy_class==cazy_class,])
  
  write.csv(
    summary(cl.tax.sens.2.aov) %>% lapply(tidy) %>% bind_rows(),
    file = two.anova.out
  )
  summary(cl.tax.sens.2.aov)
  
  # Tukey HSD if found stat significant results
  aov.df <- summary(cl.tax.sens.2.aov) %>% lapply(tidy) %>% bind_rows()
  if (nrow(aov.df[aov.df$"p.value" < 0.05, ]) > 0){
    class.tax.sens.tukey <- TukeyHSD(cl.tax.sens.2.aov, "Tax_group:Prediction_tool")
    write.csv(
      as.data.frame(class.tax.sens.tukey$`Tax_group:Prediction_tool`),
      tukey.out
    )
    class.tax.sens.tukey
    
    tukey.df <- as.data.frame(class.tax.sens.tukey$`Tax_group:Prediction_tool`)
    if (nrow(tukey.df[tukey.df$"p adj" < 0.05, ] ) > 0){
      print(sprintf("Found pairs of statistically signficant different means for %s", cazy_class))
    }
  }
  
}
```

### Precision

```{r classTaxPrecTable, echo=FALSE}
cazy_class_df$Prediction_tool <- factor(cazy_class_df$Prediction_tool, levels=classifiers_full)

prec.tax.class.sum.df <- build.class.tax.sum(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, "Precision")

write.csv(
  prec.tax.class.sum.df,
  "../report/cazy_class_classification/tax_performance/per_cazy_class/precision.summaryTable.csv",
  row.names=FALSE
)

kable(prec.tax.class.sum.df, caption="Overall performance (represented by the Precision) of CAZy class classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r classTaxPrecCiPerClass, echo=FALSE}
p.class.tax.prec.ci.per.cl <- build.class.tax.per.class.p(prec.tax.class.sum.df, "Precision")
p.class.tax.prec.ci.per.cl
```


```{r classTaxPrecScatterClass, echo=FALSE}
p.class.tax.prec.scatter.per.cl <- build.class.tax.p.scatter.per.cl(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, "Precision")
p.class.tax.prec.scatter.per.cl
```

```{r classTaxPrecSave2, include=FALSE}
# save plots
pdf(file = "../report/cazy_class_classification/tax_performance/per_cazy_class/prec.class.tax.ci.pdf",  width = 11, height = 8)
p.class.tax.prec.ci.per.cl
dev.off()

pdf(file = "../report/cazy_class_classification/tax_performance/per_cazy_class/prec.class.tax.scatter.pdf",  width = 11, height = 8)
p.class.tax.prec.scatter.per.cl
dev.off()
```


#### Testing per CAZy class

For each CAZy class run a two-way ANOVA to determine if there statistically significant differences between the tools and taxonomic kingdoms.

```{r classTaxprecAnova, echo=FALSE}
cl.prec.tax.df <- build.class.tax.df(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, 'Precision')

for (cazy_class in c('GH','GT','PL','CE','AA','CBM')){
  two.anova.out <- formatted_string <- sprintf("../report/cazy_class_classification/tax_performance/%s.class.prec.two-way-ANOVA.csv", cazy_class)
  tukey.out <- formatted_string <- sprintf("../report/cazy_class_classification/tax_performance/%s.class.prec.classifiers.tukeyHSD.csv", cazy_class)
  
  # TWo-way ANOVA
  cl.tax.prec.2.aov <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = cl.prec.tax.df[cl.prec.tax.df$CAZy_class==cazy_class,])
  
  write.csv(
    summary(cl.tax.prec.2.aov) %>% lapply(tidy) %>% bind_rows(),
    file = two.anova.out
  )
  summary(cl.tax.prec.2.aov)
  
  # Tukey HSD if found stat significant results
  aov.df <- summary(cl.tax.prec.2.aov) %>% lapply(tidy) %>% bind_rows()
  if (nrow(aov.df[aov.df$"p.value" < 0.05, ]) > 0){
    class.tax.prec.tukey <- TukeyHSD(cl.tax.prec.2.aov, "Tax_group:Prediction_tool")
    write.csv(
      as.data.frame(class.tax.prec.tukey$`Tax_group:Prediction_tool`),
      tukey.out
    )
    class.tax.prec.tukey
    
    tukey.df <- as.data.frame(class.tax.prec.tukey$`Tax_group:Prediction_tool`)
    if (nrow(tukey.df[tukey.df$"p adj" < 0.05, ] ) > 0){
      print(sprintf("Found pairs of statistically signficant different means for %s", cazy_class))
    }
  }
  
}
```

### F1-score

```{r classTaxF1Table, echo=FALSE}
cazy_class_df[cazy_class_df == 'Fbeta_score'] <- 'F1-score'
bact.cazy_class_df[bact.cazy_class_df == 'Fbeta_score'] <- 'F1-score'
euk.cazy_class_df[euk.cazy_class_df == 'Fbeta_score'] <- 'F1-score'

cazy_class_df$Prediction_tool <- factor(cazy_class_df$Prediction_tool, levels=classifiers_full)

f1.tax.class.sum.df <- build.class.tax.sum(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, "F1-score")

write.csv(
  f1.tax.class.sum.df,
  "../report/cazy_class_classification/tax_performance/per_cazy_class/f1-score.summaryTable.csv",
  row.names=FALSE
)

kable(f1.tax.class.sum.df, caption="Overall performance (represented by the F1-score) of CAZy class classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r classTaxF1CiPerClass, echo=FALSE}
p.class.tax.f1.ci.per.cl <- build.class.tax.per.class.p(f1.tax.class.sum.df, "F1-score")
p.class.tax.f1.ci.per.cl
```


```{r classTaxF1CScatterPerClass, echo=FALSE}
p.class.tax.f1.scatter.per.cl <- build.class.tax.p.scatter.per.cl(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, "F1-score")
p.class.tax.f1.scatter.per.cl
```

```{r classTaxF1Save, include=FALSE}
# save plots
pdf(file = "../report/cazy_class_classification/tax_performance/per_cazy_class/f1.class.tax.ci-FULL.pdf",  width = 11, height = 8)
p.class.tax.f1.ci.per.cl
dev.off()
build.class.tax.per.class.p.LIMITED <- function(stat_sum_df, stat.val){
  # df has colnames:
  # Prediction_tool, Mean, Standard Deviation, LowerCI, UpperCI, Tax_group, CAZy_clas
	colnames(stat_sum_df) <- c('Prediction_tool', 'Mean', 'SD', 'LowerCI', 'UpperCI', 'Taxonomic_group', 'CAZy_class')
	stat_sum_df$Prediction_tool <- factor(stat_sum_df$Prediction_tool, levels = classifiers_full)
	stat_sum_df$CAZy_class <- factor(stat_sum_df$CAZy_class, levels=cazy_class_list)
	
	p.t.b.ci <- ggplot(stat_sum_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::group_by(Taxonomic_group),
				  aes(x=Prediction_tool, y=Mean, color=Taxonomic_group)) +
	  geom_point(aes(shape=Taxonomic_group), position=position_dodge(width=0.75)) +
	  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI), width = 1, position=position_dodge(width=0.75)) +
	  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
	  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
	  geom_vline(xintercept=stat_sum_df$Prediction_tool, linetype="dashed", color = "#9c9c9c") +
	  scale_colour_manual(values = colour_set) +
	  theme(legend.position = "bottom",
			plot.background = element_rect(fill = figbg, color = figbg),
			axis.text=element_text(size=10),
			axis.title=element_text(size=12,face="bold"),
			axis.text.x=element_text(angle = 90, size=9, vjust=0.45)) +
	  xlab("Classifier") + 
	  ylab(stat.val) +
	  ylim(0.3,1.05)+
	  facet_wrap(~ CAZy_class, ncol=2)
	  
	return(p.t.b.ci)
}
pdf(file = "../report/cazy_class_classification/tax_performance/per_cazy_class/f1.class.tax.ci.pdf",  width = 6, height = 8)
build.class.tax.per.class.p.LIMITED(f1.tax.class.sum.df, "F1-score")
dev.off()

pdf(file = "../report/cazy_class_classification/tax_performance/per_cazy_class/f1.class.tax.scatter.pdf",  width = 11, height = 8)
p.class.tax.f1.scatter.per.cl
dev.off()
```

#### Testing per CAZy class

For each CAZy class run a two-way ANOVA to determine if there statistically significant differences between the tools and taxonomic kingdoms.

```{r classTaxf1Anova, echo=FALSE}

cl.f1.tax.df <- build.class.tax.df(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, 'F1-score')


for (cazy_class in c('GH','GT','PL','CE','AA','CBM')){
  two.anova.out <- formatted_string <- sprintf("../report/cazy_class_classification/tax_performance/%s.class.f1.two-way-ANOVA.csv", cazy_class)
  tukey.out <- formatted_string <- sprintf("../report/cazy_class_classification/tax_performance/%s.class.f1.classifiers.tukeyHSD.csv", cazy_class)
  
  # TWo-way ANOVA
  cl.tax.f1.2.aov <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = cl.f1.tax.df[cl.f1.tax.df$CAZy_class==cazy_class,])
  
  write.csv(
    summary(cl.tax.f1.2.aov) %>% lapply(tidy) %>% bind_rows(),
    file = two.anova.out
  )
  summary(cl.tax.f1.2.aov)
  print("----------------------------------------------------")
  print(cazy_class)
  print(summary(cl.tax.f1.2.aov))
  
  # Tukey HSD if found stat significant results
  aov.df <- summary(cl.tax.f1.2.aov) %>% lapply(tidy) %>% bind_rows()
  if (nrow(aov.df[aov.df$"p.value" < 0.05, ]) > 0){
    class.tax.f1.tukey <- TukeyHSD(cl.tax.f1.2.aov, "Tax_group:Prediction_tool")
    write.csv(
      as.data.frame(class.tax.f1.tukey$`Tax_group:Prediction_tool`),
      tukey.out
    )
    class.tax.f1.tukey
    
    tukey.df <- as.data.frame(class.tax.f1.tukey$`Tax_group:Prediction_tool`)
    if (nrow(tukey.df[tukey.df$"p adj" < 0.05, ] ) > 0){
      print(sprintf("Found pairs of statistically signficant different means for %s", cazy_class))
    }
  }
  
}
```

### Accuracy

```{r classTaxAccTable, echo=FALSE}
cazy_class_df$Prediction_tool <- factor(cazy_class_df$Prediction_tool, levels=classifiers_full)

acc.tax.class.sum.df <- build.class.tax.sum(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, "Accuracy")

write.csv(
  acc.tax.class.sum.df,
  "../report/cazy_class_classification/tax_performance/per_cazy_class/accuracy.summaryTable.csv",
  row.names=FALSE
)

kable(acc.tax.class.sum.df, caption="Overall performance (represented by the Accuracy) of CAZy class classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r classTaxAccCiPerClass, echo=FALSE}
p.class.tax.acc.ci.per.cl <- build.class.tax.per.class.p(acc.tax.class.sum.df, "Accuracy")
p.class.tax.acc.ci.per.cl
```


```{r classTaxAccScatterPerClass, echo=FALSE}
p.class.tax.acc.scatter.per.cl <- build.class.tax.p.scatter.per.cl(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, "Accuracy")
p.class.tax.acc.scatter.per.cl
```

```{r classTaxAccSave, include=FALSE}
# save plots
pdf(file = "../report/cazy_class_classification/tax_performance/per_cazy_class/acc.class.tax.ci.pdf",  width = 11, height = 8)
p.class.tax.acc.ci.per.cl
dev.off()

pdf(file = "../report/cazy_class_classification/tax_performance/per_cazy_class/acc.class.tax.scatter.pdf",  width = 11, height = 8)
p.class.tax.acc.scatter.per.cl
dev.off()
```

#### Testing per CAZy class

For each CAZy class run a two-way ANOVA to determine if there statistically significant differences between the tools and taxonomic kingdoms.

```{r classTaxaccAnova, echo=FALSE}
cl.acc.tax.df <- build.class.tax.df(cazy_class_df, bact.cazy_class_df, euk.cazy_class_df, 'Accuracy')

for (cazy_class in c('GH','GT','PL','CE','AA','CBM')){
  two.anova.out <- formatted_string <- sprintf("../report/cazy_class_classification/tax_performance/%s.class.acc.two-way-ANOVA.csv", cazy_class)
  tukey.out <- formatted_string <- sprintf("../report/cazy_class_classification/tax_performance/%s.class.acc.classifiers.tukeyHSD.csv", cazy_class)
  
  # TWo-way ANOVA
  cl.tax.acc.2.aov <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = cl.acc.tax.df[cl.acc.tax.df$CAZy_class==cazy_class,])
  
  write.csv(
    summary(cl.tax.acc.2.aov) %>% lapply(tidy) %>% bind_rows(),
    file = two.anova.out
  )
  summary(cl.tax.acc.2.aov)
  
  # Tukey HSD if found stat significant results
  aov.df <- summary(cl.tax.acc.2.aov) %>% lapply(tidy) %>% bind_rows()
  if (nrow(aov.df[aov.df$"p.value" < 0.05, ]) > 0){
    class.tax.acc.tukey <- TukeyHSD(cl.tax.acc.2.aov, "Tax_group:Prediction_tool")
    write.csv(
      as.data.frame(class.tax.acc.tukey$`Tax_group:Prediction_tool`),
      tukey.out
    )
    class.tax.acc.tukey
    
    tukey.df <- as.data.frame(class.tax.acc.tukey$`Tax_group:Prediction_tool`)
    if (nrow(tukey.df[tukey.df$"p adj" < 0.05, ] ) > 0){
      print(sprintf("Found pairs of statistically signficant different means for %s", cazy_class))
    }
  }
  
}
```

# CAZy class multilabel classification tax performance

```{r riCalcClassTax, echo=FALSE}
class_tax_ri_ari_df.bact <- class_tax_ri_ari_df[which(class_tax_ri_ari_df$Tax_group == "Bacteria"), ]
class_tax_ri_ari_df.euk <- class_tax_ri_ari_df[which(class_tax_ri_ari_df$Tax_group == "Eukaryote"), ]
class_tax_ri_ari_df.all <- data.frame(class_tax_ri_ari_df)
class_tax_ri_ari_df.all$Tax_group <- rep("All", nrow(class_tax_ri_ari_df.all))

class_tax_ri_ari_df.bact$Prediction_tool <- factor(class_tax_ri_ari_df.bact$Prediction_tool, levels = classifiers_full) # set order data is presented
class_tax_ri_ari_df.euk$Prediction_tool <- factor(class_tax_ri_ari_df.euk$Prediction_tool, levels = classifiers_full)
class_tax_ri_ari_df.all$Prediction_tool <- factor(class_tax_ri_ari_df.all$Prediction_tool, levels = classifiers_full)

class_ri_stats_df_tax.bact <- class_tax_ri_ari_df.bact %>% dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Rand_index_binary)[3],
    "Mean"=mean(Rand_index_binary),
    "Upper CI"=CI(Rand_index_binary)[1],
    "Standard Deviation"=sd(Rand_index_binary)
  )

class_ri_stats_df_tax.bact$Tax_group <- rep("Bacteria", nrow(class_ri_stats_df_tax.bact))
class_ri_stats_df_tax.euk <- class_tax_ri_ari_df.euk %>% dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Rand_index_binary)[3],
    "Mean"=mean(Rand_index_binary),
    "Upper CI"=CI(Rand_index_binary)[1],
    "Standard Deviation"=sd(Rand_index_binary)
  )
class_ri_stats_df_tax.euk$Tax_group <- rep("Eukaryote", nrow(class_ri_stats_df_tax.euk))

class_ri_stats_df_tax.all <- class_tax_ri_ari_df.all %>% dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Rand_index_binary)[3],
    "Mean"=mean(Rand_index_binary),
    "Upper CI"=CI(Rand_index_binary)[1],
    "Standard Deviation"=sd(Rand_index_binary)
  )
class_ri_stats_df_tax.all$Tax_group <- rep("All", nrow(class_ri_stats_df_tax.all))

class_ri_stats_df_tax.all <- rbind(class_ri_stats_df_tax.all, class_ri_stats_df_tax.bact)
class_ri_stats_df_tax.all <- rbind(class_ri_stats_df_tax.all, class_ri_stats_df_tax.euk)

write.csv(class_ri_stats_df_tax.all, "../report/cazy_class_classification/multilabel_classification/ri_table_tax_performance.csv", row.names=FALSE)

kable(class_ri_stats_df_tax.all, caption="Rand Index of CAZyme classifier classification of CAZy class annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r riCalcPlot, echo=FALSE}
colnames(class_ri_stats_df_tax.all) <- c("Prediction_tool", "Lower.CI", "Mean", "Upper.CI", "SD", "Taxonomic.Group")
class_ri_stats_df_tax.all$Taxonomic.Group <- factor(class_ri_stats_df_tax.all$Taxonomic.Group, levels=c('Bacteria','All','Eukaryote'))

p.ri.class.tax.ci <- ggplot(class_ri_stats_df_tax.all %>% dplyr::group_by(Taxonomic.Group),
			  aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=Lower.CI, ymax=Upper.CI)) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  scale_colour_manual(values = colour_set_tools) +
  theme(legend.position = "none",
		plot.background = element_rect(fill = figbg, color = figbg),
		axis.text=element_text(size=10),
		axis.title=element_text(size=12,face="bold"),
		axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Rand Index") +
  facet_wrap(~ Taxonomic.Group, ncol=3)
p.ri.class.tax.ci
```


```{r ariCalc, include=FALSE}
class_tax_ri_ari_df.bact <- class_tax_ri_ari_df[which(class_tax_ri_ari_df$Tax_group == "Bacteria"), ]
class_tax_ri_ari_df.euk <- class_tax_ri_ari_df[which(class_tax_ri_ari_df$Tax_group == "Eukaryote"), ]
class_tax_ri_ari_df.all <- data.frame(class_tax_ri_ari_df)
class_tax_ri_ari_df.all$Tax_group <- rep("All", nrow(class_tax_ri_ari_df.all))

class_tax_ri_ari_df.bact$Prediction_tool <- factor(class_tax_ri_ari_df.bact$Prediction_tool, levels = classifiers_full) # set order data is presented
class_tax_ri_ari_df.euk$Prediction_tool <- factor(class_tax_ri_ari_df.euk$Prediction_tool, levels = classifiers_full)
class_tax_ri_ari_df.all$Prediction_tool <- factor(class_tax_ri_ari_df.all$Prediction_tool, levels = classifiers_full)

class_ri_stats_df_tax.bact <- class_tax_ri_ari_df.bact %>% dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Adjusted_Rand_index_binary)[3],
    "Mean"=mean(Adjusted_Rand_index_binary),
    "Upper CI"=CI(Adjusted_Rand_index_binary)[1],
    "Standard Deviation"=sd(Adjusted_Rand_index_binary)
  )

class_ri_stats_df_tax.bact$Tax_group <- rep("Bacteria", nrow(class_ri_stats_df_tax.bact))
class_ri_stats_df_tax.euk <- class_tax_ri_ari_df.euk %>% dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Adjusted_Rand_index_binary)[3],
    "Mean"=mean(Adjusted_Rand_index_binary),
    "Upper CI"=CI(Adjusted_Rand_index_binary)[1],
    "Standard Deviation"=sd(Adjusted_Rand_index_binary)
  )
class_ri_stats_df_tax.euk$Tax_group <- rep("Eukaryote", nrow(class_ri_stats_df_tax.euk))

class_ari_stats_df_tax.all <- class_tax_ri_ari_df.all %>% dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Adjusted_Rand_index_binary)[3],
    "Mean"=mean(Adjusted_Rand_index_binary),
    "Upper CI"=CI(Adjusted_Rand_index_binary)[1],
    "Standard Deviation"=sd(Adjusted_Rand_index_binary)
  )
class_ari_stats_df_tax.all$Tax_group <- rep("All", nrow(class_ari_stats_df_tax.all))

class_ari_stats_df_tax.all <- rbind(class_ari_stats_df_tax.all, class_ri_stats_df_tax.bact)
class_ari_stats_df_tax.all <- rbind(class_ari_stats_df_tax.all, class_ri_stats_df_tax.euk)

write.csv(class_ari_stats_df_tax.all, "../report/cazy_class_classification/multilabel_classification/ari_table_tax_performance.csv", row.names=FALSE)

kable(class_ari_stats_df_tax.all, caption="Adjusted Rand Index of CAZyme classifier classification of CAZy class annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r ariCalcPlot, echo=FALSE}
colnames(class_ari_stats_df_tax.all) <- c("Prediction_tool", "Lower.CI", "Mean", "Upper.CI", "SD", "Taxonomic.Group")
class_ari_stats_df_tax.all$Taxonomic.Group <- factor(class_ari_stats_df_tax.all$Taxonomic.Group, levels=c('Bacteria','All','Eukaryote'))

p.ari.class.tax.ci <- ggplot(class_ari_stats_df_tax.all %>% dplyr::group_by(Taxonomic.Group),
			  aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=Lower.CI, ymax=Upper.CI)) +
  geom_hline(yintercept=0.9, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  scale_colour_manual(values = colour_set_tools) +
  theme(legend.position = "none",
		plot.background = element_rect(fill = figbg, color = figbg),
		axis.text=element_text(size=10),
		axis.title=element_text(size=12,face="bold"),
		axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Adjusted Rand Index") +
  facet_wrap(~ Taxonomic.Group, ncol=3)
p.ari.class.tax.ci
```

```{r saveClassMlc, include=FALSE}
pdf(file = "../report/cazy_class_classification/multilabel_classification/cazyClassMLC.RI.ci.pdf",  width = 8.23, height = 5)
p.ri.class.tax.ci
dev.off()

pdf(file = "../report/cazy_class_classification/multilabel_classification/cazyClassMLC.ARI.ci.pdf",  width = 8.23, height = 5)
p.ari.class.tax.ci
dev.off()
```

Test for statistically significant differences between the means.

```{r classRiTaxAnova, echo=FALSE}
class_tax_ri_ari_df.df <- rbind(class_tax_ri_ari_df.all, class_tax_ri_ari_df.bact)
class_tax_ri_ari_df.df <- rbind(class_tax_ri_ari_df.df, class_tax_ri_ari_df.euk)

class.ri.tax.aov <- aov(Rand_index_freq ~ Tax_group * Prediction_tool, data = class_tax_ri_ari_df.df)
write.csv(
  summary(class.ri.tax.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_class_classification/multilabel_classification/ri.aov.tax.csv"
)
summary(class.ri.tax.aov)
```
Statistically significant difference was detected so follow up with a Tukey test to determine if there are differences between groups.

```{r classritaxtukey1, echo=FALSE}
class.ri.tax.tukey <- TukeyHSD(class.ri.tax.aov, "Tax_group:Prediction_tool")
write.csv(
  as.data.frame(class.ri.tax.tukey$`Tax_group:Prediction_tool`),
  "../report/cazy_class_classification/multilabel_classification/class.ri.tukeyHSD.tax.csv"
)
class.ri.tax.tukey
```

```{r classARiTaxAnova, echo=FALSE}
class_tax_ri_ari_df.df <- rbind(class_tax_ri_ari_df.all, class_tax_ri_ari_df.bact)
class_tax_ri_ari_df.df <- rbind(class_tax_ri_ari_df.df, class_tax_ri_ari_df.euk)

class.ari.tax.aov <- aov(Adjusted_Rand_index_freq ~ Tax_group * Prediction_tool, data = class_tax_ri_ari_df.df)
write.csv(
  summary(class.ari.tax.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_class_classification/multilabel_classification/ari.aov.tax.csv"
)
summary(class.ari.tax.aov)
```
Statistically significant difference was detected so follow up with a Tukey test to determine if there are differences between groups.

```{r classAritaxtukey, echo=FALSE}
class.ari.tax.tukey <- TukeyHSD(class.ari.tax.aov, "Tax_group:Prediction_tool")
write.csv(
  as.data.frame(class.ari.tax.tukey$`Tax_group:Prediction_tool`),
  "../report/cazy_class_classification/multilabel_classification/class.ari.tukeyHSD.tax.csv"
)
class.ari.tax.tukey
```

# CAZy family classification

The following section evaluates the performance of the CAZyme classifiers to predict CAZy family classifications.

## General trends in performance across all CAZy families

Below is a table summarising the overall CAZy family classifications for across all CAZy families, when pooling all test sets.

```{r cazyFamStasTable, echo=FALSE}
# Calculate statistics
fam_subset_spec <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Specificity"), ]
fam_subset_spec  <- fam_subset_spec[complete.cases(fam_subset_spec), ]
fam_specificity <- fam_subset_spec %>% 
  dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Spec Mean"=mean(Statistic_value),
    "Spec Standard Deviation"=sd(Statistic_value),
    "Spec Lower CI"=CI(Statistic_value)[3],
    "Spec Upper CI"=CI(Statistic_value)[1]
  )

fam_subset_sens <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Sensitivity"), ]
fam_subset_sens  <- fam_subset_sens[complete.cases(fam_subset_sens), ]
fam_sensitivity <- fam_subset_sens %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Sens Mean"=mean(Statistic_value),
    "Sens Standard Deviation"=sd(Statistic_value),
    "Sens Lower CI"=CI(Statistic_value)[3],
    "Sens Upper CI"=CI(Statistic_value)[1]
  )

fam_subset_prec <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Precision"), ]
fam_subset_prec  <- fam_subset_prec[complete.cases(fam_subset_prec), ]
fam_precision <- fam_subset_prec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Prec Mean"=mean(Statistic_value),
    "Prec Standard Deviation"=sd(Statistic_value),
    "Prec Lower CI"=CI(Statistic_value)[3],
    "Prec Upper CI"=CI(Statistic_value)[1]
  )

fam_subset_f1 <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Fbeta_score"), ]
fam_subset_f1  <- fam_subset_f1[complete.cases(fam_subset_f1), ]
fam_f1_score <- fam_subset_f1 %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "F1-score Mean"=mean(Statistic_value),
    "F1-score Standard Deviation"=sd(Statistic_value),
    "F1-score Lower CI"=CI(Statistic_value)[3],
    "F1-score Upper CI"=CI(Statistic_value)[1]
  )

fam_subset_acc <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Accuracy"), ]
fam_subset_acc  <- fam_subset_acc[complete.cases(fam_subset_acc), ]
fam_accuracy <- fam_subset_acc %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Acc Mean"=mean(Statistic_value),
    "Acc Standard Deviation"=sd(Statistic_value),
    "Acc Lower CI"=CI(Statistic_value)[3],
    "Acc Upper CI"=CI(Statistic_value)[1]
  )

# combine data and build a single dataframe
fam_summary_df <- merge(fam_specificity, fam_sensitivity)
fam_summary_df <- merge(fam_summary_df, fam_precision)
fam_summary_df <- merge(fam_summary_df, fam_f1_score)
fam_summary_df <- merge(fam_summary_df, fam_accuracy)

# define factors
fam_summary_df$Prediction_tool <- factor(fam_summary_df$Prediction_tool, levels = classifiers_full) # set order data is presented

names(fam_summary_df)[names(fam_summary_df) == "Prediction_tool"] <- "Classifier"
# reorder the rows

row.names(fam_summary_df) = NULL  # hides row names which are added by reordering the rows

dir.create("../report/cazy_family_classification/", recursive=FALSE)
write.csv(fam_summary_df, "../report/cazy_family_classification/summaryTable.csv", row.names=FALSE)

kable(
  fam_summary_df,
  caption="Overall performance of CAZyme classifiers CAZy family classification performance across all CAZy families",
  align='c',
  digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

Here we plot the 95% confidence intervals for CAZy class classification, when pooling all CAZy classes and test sets.

```{r famCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of CAZy family classification, plotting the mean plus and minus the 95% confidence interval."}

fam.spec.ci <- cal_tool_ci(fam_subset_spec)
fam.spec.ci$Statistic_parameter <- rep('Specificity', nrow(fam.spec.ci))
fam.sens.ci <- cal_tool_ci(fam_subset_sens)
fam.sens.ci$Statistic_parameter <- rep('Sensitivity', nrow(fam.sens.ci))
fam.prec.ci <- cal_tool_ci(fam_subset_prec)
fam.prec.ci$Statistic_parameter <- rep('Precision', nrow(fam.prec.ci))
fam.f1.ci <- cal_tool_ci(fam_subset_f1)
fam.f1.ci$Statistic_parameter <- rep('F1-score', nrow(fam.f1.ci))
fam.acc.ci <- cal_tool_ci(fam_subset_acc)
fam.acc.ci$Statistic_parameter <- rep('Accuracy', nrow(fam.acc.ci))

fam.ci_df <- rbind(fam.spec.ci, fam.sens.ci)
fam.ci_df <- rbind(fam.ci_df, fam.prec.ci)
fam.ci_df <- rbind(fam.ci_df, fam.f1.ci)
fam.ci_df <- rbind(fam.ci_df, fam.acc.ci)

fam.ci_df$Prediction_tool <- factor(fam.ci_df$Prediction_tool, levels = classifiers_full)
fam.ci_df$Statistic_parameter <- factor(fam.ci_df$Statistic_parameter, levels = c("Specificity", "Sensitivity", "Precision", "F1-score", "Accuracy"))

p.fam.CI = ggplot(fam.ci_df %>% dplyr::group_by(Statistic_parameter),
                   aes(x=Prediction_tool, y=MeanCI, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  scale_colour_manual(values = colour_set_tools) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Statistical value") +
  facet_wrap(~ Statistic_parameter, ncol=2)
p.fam.CI
```

```{r famAllScatter, echo=FALSE}
p.fam.scatter = ggplot(fam_subset_f1 %>% dplyr::group_by(Statistical_parameter),
                   aes(x=Prediction_tool, y=Statistic_value, fille=Prediction_tool)) +
	geom_boxplot(outlier.shape=NA, position=position_dodge(width=1)) +
	geom_jitter(position=position_dodge(width=1), size=0.3, aes(alpha=0.5)) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  scale_colour_manual(values = colour_set_tools) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Statistical value") +
  facet_wrap(~ Statistical_parameter, ncol=2)
p.fam.scatter
```
```{r famAccAllScatter, echo=FALSE}
p.fam.scatter = ggplot(fam_subset_acc %>% dplyr::group_by(Statistical_parameter),
                   aes(x=Prediction_tool, y=Statistic_value, fille=Prediction_tool)) +
	geom_boxplot(outlier.shape=NA, position=position_dodge(width=1)) +
	geom_jitter(position=position_dodge(width=1), size=0.3, aes(alpha=0.5)) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  scale_colour_manual(values = colour_set_tools) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Statistical value") +
  facet_wrap(~ Statistical_parameter, ncol=2)
p.fam.scatter
```

```{r saveClassCI, include=FALSE}
dir.create("../report/cazy_family_classification", recursive=FALSE)
pdf(
  file = "../report/cazy_family_classification/statsConfidenceIntervals.pdf",
  width = 6.3,
  height = 8.2
)
p.fam.CI
dev.off()
```

To evaluate the overall performance of each classifier, for each CAZy family, the F1-score was calculated for every family. Families were grouped by their parent CAZy class and the distribution of the F1-scores is shown in figure \@ref(fig:fbetaclass).

```{r fbetaclass, echo=FALSE, fig.cap="Proportaional area plot of F1-score per CAZy distribution per CAZy class."}
# F-beta scores already subset previously, and are stored in the var fam_subset_f1

# classify the Fbeta-scores into bins
val.fam <- vector()

for(i in 1:nrow(fam_subset_f1)){
  if(fam_subset_f1[i, 5] == 1){val.fam <- append(val.fam, '[1.00]')} 
  else if (fam_subset_f1[i, 5] == 0){val.fam <- append(val.fam, '[0.00]')}
  else if (fam_subset_f1[i, 5] < 1 && fam_subset_f1[i, 5] >= 0.95){val.fam <- append(val.fam, '(0.95, 1.00]')}
  else if (fam_subset_f1[i, 5] < 0.95 && fam_subset_f1[i, 5] >= 0.9){val.fam <- append(val.fam, '(0.90, 0.95]')}
  else if (fam_subset_f1[i, 5] < 0.90 && fam_subset_f1[i, 5] >= 0.85){val.fam <- append(val.fam, '(0.85, 0.90]')}
  else if (fam_subset_f1[i, 5] < 0.85 && fam_subset_f1[i, 5] >= 0.80){val.fam <- append(val.fam, '(0.80, 0.85]')}
  else if (fam_subset_f1[i, 5] < 0.80 && fam_subset_f1[i, 5] >= 0.75){val.fam <- append(val.fam, '(0.75, 0.80]')}
  else if (fam_subset_f1[i, 5] < 0.75 && fam_subset_f1[i, 5] >= 0.70){val.fam <- append(val.fam, '(0.70, 0.75]')}
  else if (fam_subset_f1[i, 5] < 0.70 && fam_subset_f1[i, 5] >= 0.65){val.fam <- append(val.fam, '(0.65, 0.70]')}
  else if (fam_subset_f1[i, 5] < 0.65 && fam_subset_f1[i, 5] >= 0.60){val.fam <- append(val.fam, '(0.60, 0.65]')}
  else if (fam_subset_f1[i, 5] < 0.60 && fam_subset_f1[i, 5] >= 0.55){val.fam <- append(val.fam, '(0.55, 0.60]')}
  else if (fam_subset_f1[i, 5] < 0.55 && fam_subset_f1[i, 5] >= 0.50){val.fam <- append(val.fam, '(0.50, 0.55]')}
  else if (fam_subset_f1[i, 5] < 0.50 && fam_subset_f1[i, 5] >= 0.45){val.fam <- append(val.fam, '(0.45, 0.50]')}
  else if (fam_subset_f1[i, 5] < 0.45 && fam_subset_f1[i, 5] >= 0.40){val.fam <- append(val.fam, '(0.40, 0.45]')}
  else if (fam_subset_f1[i, 5] < 0.40 && fam_subset_f1[i, 5] >= 0.35){val.fam <- append(val.fam, '(0.35, 0.40]')}
  else if (fam_subset_f1[i, 5] < 0.35 && fam_subset_f1[i, 5] >= 0.30){val.fam <- append(val.fam, '(0.30, 0.35]')}
  else if (fam_subset_f1[i, 5] < 0.30 && fam_subset_f1[i, 5] >= 0.25){val.fam <- append(val.fam, '(0.25, 0.30]')}
  else if (fam_subset_f1[i, 5] < 0.25 && fam_subset_f1[i, 5] >= 0.20){val.fam <- append(val.fam, '(0.20, 0.25]')}
  else if (fam_subset_f1[i, 5] < 0.20 && fam_subset_f1[i, 5] >= 0.15){val.fam <- append(val.fam, '(0.15, 0.20]')}
  else if (fam_subset_f1[i, 5] < 0.15 && fam_subset_f1[i, 5] >= 0.10){val.fam <- append(val.fam, '(0.10, 0.15]')}
  else if (fam_subset_f1[i, 5] < 0.10 && fam_subset_f1[i, 5] >= 0.05){val.fam <- append(val.fam, '(0.05, 0.10]')}
  else if (fam_subset_f1[i, 5] < 0.05 && fam_subset_f1[i, 5] >= 0){val.fam <- append(val.fam, '(0.00, 0.05]')}
  else {val.fam <- append(val.fam, '(0.00, 0.05]')} # 
}
# add the bins to the df
fam_subset_f1$val.fam <- val.fam
fam_subset_f1[fam_subset_f1 == 'Fbeta_score'] <- 'F1-score'


# add the parent CAZy class to the df for facet wrapping

# retrieve all the family names
dbcan.fam.subset <- cazy_family_long_df[which(cazy_family_long_df$Prediction_tool == "dbCAN_2"), ]
tn.dbcan.fam.subset <- dbcan.fam.subset[which(dbcan.fam.subset$Statistical_parameter == "TNs"), ]

# names were seperated into one vector per CAZy class in the setup block

# create subsets by CAZy class and associated each data set with the name of its CAZy class
gh.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% gh.names), ]
gh.class <- rep('GH', nrow(gh.subset))
gh.subset$CAZy_class <- gh.class

gt.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% gt.names), ]
gt.class <- rep('GT', nrow(gt.subset))
gt.subset$CAZy_class <- gt.class
  
pl.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% pl.names), ]
pl.class <- rep('PL', nrow(pl.subset))
pl.subset$CAZy_class <- pl.class
  
ce.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% ce.names), ]
ce.class <- rep('CE', nrow(ce.subset))
ce.subset$CAZy_class <- ce.class
  
aa.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% aa.names), ]
aa.class <- rep('AA', nrow(aa.subset))
aa.subset$CAZy_class <- aa.class
  
cbm.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% cbm.names), ]
cbm.class <- rep('CBM', nrow(cbm.subset))
cbm.subset$CAZy_class <- cbm.class

# combine the class dfs in to one
fam_fbeta_df <- rbind(gh.subset, gt.subset)
fam_fbeta_df <- rbind(fam_fbeta_df, pl.subset)
fam_fbeta_df <- rbind(fam_fbeta_df, ce.subset)
fam_fbeta_df <- rbind(fam_fbeta_df, aa.subset)
fam_fbeta_df <- rbind(fam_fbeta_df, cbm.subset)

# set order data is presented
fam_fbeta_df$Prediction_tool <- factor(fam_fbeta_df$Prediction_tool, levels = classifiers_full) 
fam_fbeta_df$CAZy_class <- factor(fam_fbeta_df$CAZy_class, levels = rev(cazy_class_list))
fam_fbeta_df$val.fam <- factor(fam_fbeta_df$val.fam, levels = c(
  '[0.00]',
  '(0.00, 0.05]', '(0.05, 0.10]',
  '(0.10, 0.15]', '(0.15, 0.20]',
  '(0.20, 0.25]', '(0.25, 0.30]',
  '(0.30, 0.35]', '(0.35, 0.40]',
  '(0.40, 0.45]', '(0.45, 0.50]',
  '(0.50, 0.55]', '(0.55, 0.60]',
  '(0.60, 0.65]', '(0.65, 0.70]',
  '(0.70, 0.75]', '(0.75, 0.80]',
  '(0.80, 0.85]', '(0.85, 0.90]',
  '(0.90, 0.95]', '(0.95, 1.00]',
  '[1.00]'))

p.famF1 = ggally_count(fam_fbeta_df, mapping=ggplot2::aes(x=Prediction_tool, y=CAZy_class, fill=val.fam)) +
  scale_fill_manual(values = colour_grad, drop=FALSE) +
  xlab("Classifier") + 
  ylab("CAZy class") +
  theme(legend.background = element_rect(fill = figbg, color = figbg),
        legend.position = 'bottom',
        plot.background = element_rect(fill = figbg, color = figbg),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=9, vjust=0.45)) +
  labs(fill = "F1-score") +
  guides(fill = guide_legend(reverse = TRUE))
p.famF1
```

```{r saveFambeta, include=FALSE}
pdf(file = "../report/cazy_family_classification/cazyFamF1paPlShort.pdf",  width = 5.5, height = 8)
p.famF1
dev.off()

p.famF1.LS = ggally_count(fam_fbeta_df, mapping=ggplot2::aes(x=Prediction_tool, y=CAZy_class, fill=val.fam)) +
  scale_fill_manual(values = colour_grad, drop=FALSE) +
  xlab("Classifier") + 
  ylab("CAZy class") +
  theme(legend.background = element_rect(fill = figbg, color = figbg),
        legend.position = 'right',
        plot.background = element_rect(fill = figbg, color = figbg),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=9, vjust=0.45)) +
  labs(fill = "F1-score") +
  guides(fill = guide_legend(reverse = TRUE))

pdf(file = "../report/cazy_family_classification/cazyFamF1paPlto-LegendSide.pdf",  width = 5.5, height = 8)
p.famF1.LS
dev.off()
```

\@ref(fig:fbetaclass)
Below is a table displaying the number of test sets in which each CAZy class was present, and were used to draw the proporitonal areas for each class in figure\@ref(fig:fbetaclass).

```{r calcFamFbetaSampleSize, echo=FALSE}
fam_pt_row_1 = calc_class_sample_size(fam_fbeta_df, 'dbCAN_2')
fam_pt_row_2 = calc_class_sample_size(fam_fbeta_df, 'dbCAN_2:HMMER')
fam_pt_row_3 = calc_class_sample_size(fam_fbeta_df, 'dbCAN_2:DIAMOND')
fam_pt_row_4 = calc_class_sample_size(fam_fbeta_df, 'dbCAN_2:Hotpep')
fam_pt_row_5 = calc_class_sample_size(fam_fbeta_df, 'dbCAN_3')
fam_pt_row_6 = calc_class_sample_size(fam_fbeta_df, 'dbCAN_3:HMMER')
fam_pt_row_7 = calc_class_sample_size(fam_fbeta_df, 'dbCAN_3:DIAMOND')
fam_pt_row_8 = calc_class_sample_size(fam_fbeta_df, 'dbCAN_3:eCAMI')
fam_pt_row_9 = calc_class_sample_size(fam_fbeta_df, 'dbCAN_4')
fam_pt_row_10 = calc_class_sample_size(fam_fbeta_df, 'dbCAN_4:HMMER')
fam_pt_row_11 = calc_class_sample_size(fam_fbeta_df, 'dbCAN_4:DIAMOND')
fam_pt_row_12 = calc_class_sample_size(fam_fbeta_df, 'dbCAN_4:dbCAN-sub')
fam_pt_row_13 = calc_class_sample_size(fam_fbeta_df, 'CUPP')

cazy_class_fam_sample_size_df <- rbind(fam_pt_row_1, fam_pt_row_2)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_pt_row_3)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_pt_row_4)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_pt_row_5)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_pt_row_6)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_pt_row_7)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_pt_row_8)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_pt_row_9)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_pt_row_10)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_pt_row_11)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_pt_row_12)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_pt_row_13)
cazy_class_fam_sample_size_df <- as.data.frame(cazy_class_fam_sample_size_df)
names(cazy_class_fam_sample_size_df) <- c('Prediction_tool', 'GH', 'GT', 'PL', 'CE', 'AA', 'CBM')

kable(
  cazy_class_fam_sample_size_df,
  caption="The number of CAZy families included in the evaluation of CAZy family classification for each CAZyme classifier",
  align='c',
  digits = 4) %>% kable_styling(full_width = F)
```

Now the distribution of F1-scores of CAZyme family classification is calculated for each tool, per CAZyme class.

```{r famF1TableDistribution, echo=FALSE}
get.f1.count <- function(df, tool, cazy.class, threshold){
  count = 0
  if(threshold == 1){count = nrow(subset(df, CAZy_class==cazy.class & Prediction_tool==tool & Statistic_value==1))}
  else if (threshold == 0.9){count = nrow(subset(df, CAZy_class==cazy.class & Prediction_tool==tool & Statistic_value>=0.9))}
  else{count = nrow(subset(df, CAZy_class==cazy.class & Prediction_tool==tool & Statistic_value<=0.75))}
  # now calculate the proportion of the tests that contained the CAZyme class or 
  # was predicted to contain the CAZyme clas (i.e. excluding true negatives)
  # that achieved an F1-score within each range/threshold
  num.test.sets <- as.numeric(subset(cazy_class_fam_sample_size_df, Prediction_tool==tool)[, cazy.class])
  return((count / num.test.sets)*100)
}

col.gh.1 <- c()
col.gh.0.9 <- c()
col.gh.0.75 <- c()
for (tool in classifiers_full){
  col.gh.1 <- append(col.gh.1, get.f1.count(fam_fbeta_df, tool, "GH", 1))
  col.gh.0.9 <- append(col.gh.0.9, get.f1.count(fam_fbeta_df, tool, "GH", 0.9))
  col.gh.0.75 <- append(col.gh.0.75, get.f1.count(fam_fbeta_df, tool, "GH", 0.75))
}

col.gt.1 <- c()
col.gt.0.9 <- c()
col.gt.0.75 <- c()
for (tool in classifiers_full){
  col.gt.1 <- append(col.gt.1, get.f1.count(fam_fbeta_df, tool, "GT", 1))
  col.gt.0.9 <- append(col.gt.0.9, get.f1.count(fam_fbeta_df, tool, "GT", 0.9))
  col.gt.0.75 <- append(col.gt.0.75, get.f1.count(fam_fbeta_df, tool, "GT", 0.75))
}

col.pl.1 <- c()
col.pl.0.9 <- c()
col.pl.0.75 <- c()
for (tool in classifiers_full){
  col.pl.1 <- append(col.pl.1, get.f1.count(fam_fbeta_df, tool, "PL", 1))
  col.pl.0.9 <- append(col.pl.0.9, get.f1.count(fam_fbeta_df, tool, "PL", 0.9))
  col.pl.0.75 <- append(col.pl.0.75, get.f1.count(fam_fbeta_df, tool, "PL", 0.75))
}

col.ce.1 <- c()
col.ce.0.9 <- c()
col.ce.0.75 <- c()
for (tool in classifiers_full){
  col.ce.1 <- append(col.ce.1, get.f1.count(fam_fbeta_df, tool, "CE", 1))
  col.ce.0.9 <- append(col.ce.0.9, get.f1.count(fam_fbeta_df, tool, "CE", 0.9))
  col.ce.0.75 <- append(col.ce.0.75, get.f1.count(fam_fbeta_df, tool, "CE", 0.75))
}

col.aa.1 <- c()
col.aa.0.9 <- c()
col.aa.0.75 <- c()
for (tool in classifiers_full){
  col.aa.1 <- append(col.aa.1, get.f1.count(fam_fbeta_df, tool, "AA", 1))
  col.aa.0.9 <- append(col.aa.0.9, get.f1.count(fam_fbeta_df, tool, "AA", 0.9))
  col.aa.0.75 <- append(col.aa.0.75, get.f1.count(fam_fbeta_df, tool, "AA", 0.75))
}

col.cbm.1 <- c()
col.cbm.0.9 <- c()
col.cbm.0.75 <- c()
for (tool in classifiers_full){
  col.cbm.1 <- append(col.cbm.1, get.f1.count(fam_fbeta_df, tool, "CBM", 1))
  col.cbm.0.9 <- append(col.cbm.0.9, get.f1.count(fam_fbeta_df, tool, "CBM", 0.9))
  col.cbm.0.75 <- append(col.cbm.0.75, get.f1.count(fam_fbeta_df, tool, "CBM", 0.75))
}

fam.f1.distribution.df <- data.frame(
  Prediction_tool = classifiers_full,
  GH_1 = col.gh.1,
  GH_0.9 = col.gh.0.9,
  GH_0.75 = col.gh.0.75,
  GT_1 = col.gt.1,
  GT_0.9 = col.gt.0.9,
  GT_0.75 = col.gt.0.75,
  PL_1 = col.pl.1,
  PL_0.9 = col.pl.0.9,
  PL_0.75 = col.pl.0.75,
  CE_1 = col.ce.1,
  CE_0.9 = col.ce.0.9,
  CE_0.75 = col.ce.0.75,
  AA_1 = col.aa.1,
  AA_0.9 = col.aa.0.9,
  AA_0.75 = col.aa.0.75,
  CBM_1 = col.cbm.1,
  CBM_0.9 = col.cbm.0.9,
  CBM_0.75 = col.cbm.0.75
)
write.csv(fam.f1.distribution.df, "../report/cazy_family_classification/family.f1-score.distributions.csv", row.names=FALSE)
fam.f1.distribution.df
```


<!-- ## Performance per CAZy family -->

To evaluate the performance of predicting each CAZy family independent of all other CAZy families, the sensitivity and precision for each CAZy family, for each CAZyme classifier was calculated and plotted against each other. Whereas sensitivity was plotted against sensitivity for CAZy classes, owing to the extremely small variation in specificity scores, sensitivity was plotted as a percentage against log10 of the specificity percentage.

The following plots present the specificity (Fig.\@ref(fig:famsSpec)), sensitivity (Fig.\@ref(fig:famsSens)), precision (Fig.\@ref(fig:famsPrec)), F1-score (Fig.\@ref(fig:famsF1)) and accuracy (Fig.\@ref(fig:famsAcc)) for each CAZy family per classifier. In accompaniment to each plot is a table summarising the mean statistic value for each classifier across all CAZy families for each CAZy class.

```{r prepPerformancePerFam, include=FALSE}
# func for adding CAZy class column for a given CAZy class
add_parent_class <- function(fam.df, fam.names, cazy.class){
  fam.df.subset <- fam.df[which(fam.df$CAZy_family %in% fam.names), ]
  class.names <- rep(cazy.class, nrow(fam.df.subset))
  fam.df.subset$CAZy_class <- class.names
  
  return(fam.df.subset)
}

# add parent CAZy classes
fam_subset_spec.gh <- add_parent_class(fam_subset_spec, gh.names, 'GH')
fam_subset_spec.gt <- add_parent_class(fam_subset_spec, gt.names, 'GT')
fam_subset_spec.pl <- add_parent_class(fam_subset_spec, pl.names, 'PL')
fam_subset_spec.ce <- add_parent_class(fam_subset_spec, ce.names, 'CE')
fam_subset_spec.aa <- add_parent_class(fam_subset_spec, aa.names, 'AA')
fam_subset_spec.cbm <- add_parent_class(fam_subset_spec, cbm.names, 'CBM')

fam_subset_sens.gh <- add_parent_class(fam_subset_sens, gh.names, 'GH')
fam_subset_sens.gt <- add_parent_class(fam_subset_sens, gt.names, 'GT')
fam_subset_sens.pl <- add_parent_class(fam_subset_sens, pl.names, 'PL')
fam_subset_sens.ce <- add_parent_class(fam_subset_sens, ce.names, 'CE')
fam_subset_sens.aa <- add_parent_class(fam_subset_sens, aa.names, 'AA')
fam_subset_sens.cbm <- add_parent_class(fam_subset_sens, cbm.names, 'CBM')

fam_subset_prec.gh <- add_parent_class(fam_subset_prec, gh.names, 'GH')
fam_subset_prec.gt <- add_parent_class(fam_subset_prec, gt.names, 'GT')
fam_subset_prec.pl <- add_parent_class(fam_subset_prec, pl.names, 'PL')
fam_subset_prec.ce <- add_parent_class(fam_subset_prec, ce.names, 'CE')
fam_subset_prec.aa <- add_parent_class(fam_subset_prec, aa.names, 'AA')
fam_subset_prec.cbm <- add_parent_class(fam_subset_prec, cbm.names, 'CBM')

fam_subset_f1.gh <- add_parent_class(fam_subset_f1, gh.names, 'GH')
fam_subset_f1.gt <- add_parent_class(fam_subset_f1, gt.names, 'GT')
fam_subset_f1.pl <- add_parent_class(fam_subset_f1, pl.names, 'PL')
fam_subset_f1.ce <- add_parent_class(fam_subset_f1, ce.names, 'CE')
fam_subset_f1.aa <- add_parent_class(fam_subset_f1, aa.names, 'AA')
fam_subset_f1.cbm <- add_parent_class(fam_subset_f1, cbm.names, 'CBM')

fam_subset_acc.gh <- add_parent_class(fam_subset_acc, gh.names, 'GH')
fam_subset_acc.gt <- add_parent_class(fam_subset_acc, gt.names, 'GT')
fam_subset_acc.pl <- add_parent_class(fam_subset_acc, pl.names, 'PL')
fam_subset_acc.ce <- add_parent_class(fam_subset_acc, ce.names, 'CE')
fam_subset_acc.aa <- add_parent_class(fam_subset_acc, aa.names, 'AA')
fam_subset_acc.cbm <- add_parent_class(fam_subset_acc, cbm.names, 'CBM')

# recombine the dataframes
fam_subset_spec_all <- rbind(fam_subset_spec.gh, fam_subset_spec.gt)
fam_subset_spec_all <- rbind(fam_subset_spec_all, fam_subset_spec.pl)
fam_subset_spec_all <- rbind(fam_subset_spec_all, fam_subset_spec.ce)
fam_subset_spec_all <- rbind(fam_subset_spec_all, fam_subset_spec.aa)
fam_subset_spec_all <- rbind(fam_subset_spec_all, fam_subset_spec.cbm)

fam_subset_sens_all <- rbind(fam_subset_sens.gh, fam_subset_sens.gt)
fam_subset_sens_all <- rbind(fam_subset_sens_all, fam_subset_sens.pl)
fam_subset_sens_all <- rbind(fam_subset_sens_all, fam_subset_sens.ce)
fam_subset_sens_all <- rbind(fam_subset_sens_all, fam_subset_sens.aa)
fam_subset_sens_all <- rbind(fam_subset_sens_all, fam_subset_sens.cbm)

fam_subset_prec_all <- rbind(fam_subset_prec.gh, fam_subset_prec.gt)
fam_subset_prec_all <- rbind(fam_subset_prec_all, fam_subset_prec.pl)
fam_subset_prec_all <- rbind(fam_subset_prec_all, fam_subset_prec.ce)
fam_subset_prec_all <- rbind(fam_subset_prec_all, fam_subset_prec.aa)
fam_subset_prec_all <- rbind(fam_subset_prec_all, fam_subset_prec.cbm)

fam_subset_f1_all <- rbind(fam_subset_f1.gh, fam_subset_f1.gt)
fam_subset_f1_all <- rbind(fam_subset_f1_all, fam_subset_f1.pl)
fam_subset_f1_all <- rbind(fam_subset_f1_all, fam_subset_f1.ce)
fam_subset_f1_all <- rbind(fam_subset_f1_all, fam_subset_f1.aa)
fam_subset_f1_all <- rbind(fam_subset_f1_all, fam_subset_f1.cbm)

fam_subset_acc_all <- rbind(fam_subset_acc.gh, fam_subset_acc.gt)
fam_subset_acc_all <- rbind(fam_subset_acc_all, fam_subset_acc.pl)
fam_subset_acc_all <- rbind(fam_subset_acc_all, fam_subset_acc.ce)
fam_subset_acc_all <- rbind(fam_subset_acc_all, fam_subset_acc.aa)
fam_subset_acc_all <- rbind(fam_subset_acc_all, fam_subset_acc.cbm)

# rename columns to faciltate merging
names(fam_subset_sens_all)[names(fam_subset_sens_all) == "Statistic_value"] <- "Sensitivity"
names(fam_subset_spec_all)[names(fam_subset_spec_all) == "Statistic_value"] <- "Specificity"

# drop the column called X - from the index number created by pandas in Python
fam_subset_sens_all <- fam_subset_sens_all[c("CAZy_family", "Prediction_tool", "Sensitivity", "CAZy_class")]
fam_subset_spec_all <- fam_subset_spec_all[c("CAZy_family", "Prediction_tool", "Specificity", "CAZy_class")]

# merge the dataframes
cazy_fam_spec_sense_df <- merge(
  fam_subset_sens_all,
  fam_subset_spec_all,
  by=c("Prediction_tool", "CAZy_class", "CAZy_family")
)

# set order data is presented
fam_subset_spec_all$Prediction_tool <- factor(fam_subset_spec_all$Prediction_tool, levels = classifiers_full) 
fam_subset_spec_all$CAZy_class <- factor(fam_subset_spec_all$CAZy_class, levels = cazy_class_list)

fam_subset_sens_all$Prediction_tool <- factor(fam_subset_sens_all$Prediction_tool, levels = classifiers_full) 
fam_subset_sens_all$CAZy_class <- factor(fam_subset_sens_all$CAZy_class, levels = cazy_class_list)

fam_subset_prec_all$Prediction_tool <- factor(fam_subset_prec_all$Prediction_tool, levels = classifiers_full) 
fam_subset_prec_all$CAZy_class <- factor(fam_subset_prec_all$CAZy_class, levels = cazy_class_list)

fam_subset_f1_all$Prediction_tool <- factor(fam_subset_f1_all$Prediction_tool, levels = classifiers_full) 
fam_subset_f1_all$CAZy_class <- factor(fam_subset_f1_all$CAZy_class, levels = cazy_class_list)

fam_subset_acc_all$Prediction_tool <- factor(fam_subset_acc_all$Prediction_tool, levels = classifiers_full) 
fam_subset_acc_all$CAZy_class <- factor(fam_subset_acc_all$CAZy_class, levels = cazy_class_list)

cazy_fam_spec_sense_df$Prediction_tool <- factor(cazy_fam_spec_sense_df$Prediction_tool, levels = classifiers_full) 
cazy_fam_spec_sense_df$CAZy_class <- factor(cazy_fam_spec_sense_df$CAZy_class, levels = cazy_class_list)

# calculate sensitivity and specificity as a percentage
cazy_fam_spec_sense_df$Sens_percent <- cazy_fam_spec_sense_df$Sensitivity * 100
cazy_fam_spec_sense_df$Spec_percent <- cazy_fam_spec_sense_df$Specificity * 100

# log10 of specificity is calculated when plotting the chart
# define func for plotting sens % vs spec % log10
plot_fam_sens_vs_spec <- function(
  fam.class.df,
  cazy.class
  ){
  # subset for the class of interest
  fam.class.df.class.subset <- fam.class.df[which(fam.class.df$CAZy_class == cazy.class), ]
  
  # generate the plot
  p.fam = ggplot(fam.class.df.class.subset %>% dplyr::group_by(Prediction_tool), aes(
      x=Sensitivity,
      y=Specificity*100,
      color=Prediction_tool,
    )) +
    geom_point() +
    scale_color_manual(values=colour_set_tools) +
    theme(
          legend.position = 'none',
          plot.background = element_rect(fill = figbg, color = figbg),
          axis.text=element_text(size=10),
          axis.title=element_text(size=11,face="bold"),
          strip.text = element_text(size=11),
          axis.text.x=element_text(angle = 90, size=9, vjust=0.45)) +
    labs(x = "Sensitivity", y = "Specificity (log10)", color="Classifer") +
    scale_y_continuous(trans='log10') +
    coord_trans(y ='log10') +
    facet_wrap(~ Prediction_tool)
  #
  return(p.fam)
}

plot.fam.sens.jitter <- function(fam.class.df, cazy.class, plot_colour){
  # subset for the class of interest
  fam.class.df.class.subset <- fam.class.df[which(fam.class.df$CAZy_class == cazy.class), ]
  
  p.fam.sens <- ggplot(fam.class.df.class.subset %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Sensitivity, fill=Prediction_tool)) +
	  geom_boxplot(outlier.shape=NA) +
	  geom_jitter(width=0.1, height=0) +
	  scale_fill_manual(values = plot_colour) +
	  theme(legend.position = "none",
		plot.background = element_rect(fill = figbg, color = figbg),
		axis.text=element_text(size=10),
		axis.title=element_text(size=12,face="bold")) +
	  xlab("Classifier") + 
	  ylab("Sensitivity") 
  
  return(p.fam.sens)
}

plot.fam.ci <- function(subset_mean_df, stat_param){
  p.fam.CI = ggplot(subset_mean_df %>% dplyr::group_by(Prediction_tool),
                   aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  scale_colour_manual(values = colour_set_tools) +
  geom_point() +
  geom_errorbar(aes(ymin='Lower CI', ymax='Upper CI')) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab(stat_param) +
  facet_wrap(~ CAZy_class, ncol=2)
  return(p.fam.CI)
}

plot.fam.scatter <- function(subset_df, stat_param){
  p.fam.scat <- ggplot(subset_df %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Statistical_parameter, fill=Prediction_tool)) +
    geom_boxplot(outlier.shape=NA) +
    geom_jitter(width=0.1, height=0, size=0.3) +
    geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
    geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
    scale_fill_manual(values = colour_set_tools) +
    theme(legend.position = "none",
  	plot.background = element_rect(fill = figbg, color = figbg),
  	axis.text=element_text(size=10),
  	axis.title=element_text(size=12,face="bold"),
  	axis.text.x=element_text(angle=90, vjust=0.45, size=9)) +
    xlab("Classifier") + 
    ylab(stat_param) +
    facet_wrap(~ CAZy_class, ncol=2)
  
  return(p.fam.scat)
}
```

## Statsitically testing across all CAZyme classes

Pooling all CAZymes across all test sets, and pooling all CAzyme families from all CAZyme classes, the mean of each statistical metric is tested for statistically significant differences across the CAZyme prediction tools.

### Comparing means between CAZyme class and CAZyme family classification performances

Test for a statistically significant difference in the mean between the CAZyme class and CAZyme family classification performances.

```{r classVsFamF1Score, echo=FALSE}
fam_subset_f1.df <- data.frame(fam_subset_f1_all)
fam_subset_f1.df$Group <- rep('Family', nrow(fam_subset_f1.df))
class_subset_f1.df <- data.frame(class_subset_f1)
class_subset_f1.df$Group <- rep('Class', nrow(class_subset_f1.df))

# need columns Prediction_tool, Statistic_parameter from fams and Statistic_value frpm class

fam_subset_f1.df <- fam_subset_f1.df[, c("Prediction_tool", "Statistical_parameter")]
colnames(fam_subset_f1.df) <- c("Prediction_tool", "Statistic_value")
class_subset_f1.df <- class_subset_f1.df[, c("Prediction_tool", "Statistic_value")]

class_fam_subset_f1.df <- rbind(fam_subset_f1.df, class_subset_f1.df)
class_fam_subset_f1.df$Prediction_tool <- factor(class_fam_subset_f1.df$Prediction_tool, levels=classifiers_full)
class_fam_subset_f1.df <- class_fam_subset_f1.df[complete.cases(class_fam_subset_f1.df), ]

class_fam_subset_f1.df.filtered <- class_fam_subset_f1.df[!grepl("F1-score", class_fam_subset_f1.df$Statistic_value, ignore.case = TRUE), ]
cl.fam.f1.anova <- aov(Statistic_value ~ Prediction_tool, data=class_fam_subset_f1.df.filtered)
summary(cl.fam.f1.anova)
```


### Specificity

```{r famsSpecTable, echo=FALSE}
colnames(fam_subset_spec_all) <- c('CAZy_family', 'Prediction_tool', 'Statistical_parameter', 'CAZy_class')
fam_subset_spec_all.means <- fam_subset_spec_all %>% dplyr::group_by(CAZy_class, Prediction_tool) %>%
  dplyr::summarise(
    "Mean"=mean(Statistical_parameter),
    "Standard Deviation"=sd(Statistical_parameter),
    "Lower CI"=CI(Statistical_parameter)[3],
    "Upper CI"=CI(Statistical_parameter)[1]
)

write.csv(fam_subset_spec_all.means, "../report/cazy_family_classification/specificityTable.csv", row.names=FALSE)

kable(fam_subset_spec_all.means, caption="Specificity of CAZy family classification per CAZyme classifier", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famsSpecCI, echo=FALSE, fig.cap="95% confidence intervals of the specificity of CAZy family classification across all CAZy families per CAZy class, for each CAZyme classifier."}
colnames(fam_subset_spec_all.means) <- c('CAZy_class','Prediction_tool','Mean','SD','LowerCI','UpperCI')
p.fam.spec.CI = ggplot(fam_subset_spec_all.means %>% dplyr::group_by(CAZy_class),
                 aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  scale_colour_manual(values = colour_set_tools) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Specificity") +
  facet_wrap(~ CAZy_class, ncol=2)
p.fam.spec.CI
```
```{r saveFamspecCI, include=FALSE}
pdf(file = "../report/cazy_family_classification/specificityCI.pdf",  width = 6.23, height = 8.2)
p.fam.spec.CI
dev.off()
```

```{r famsSpec, echo=FALSE, fig.cap="Scatter plot of overlaying a one-dimensional box-and-whisker plot of specificity for each CAZy family for each CAZyme classifier. Each CAZy family is represented as a single point on the plot."}
colnames(fam_subset_spec_all) <- c('CAZy_family', 'Prediction_tool', 'Statistical_parameter', 'CAZy_class')

p.fam.spec <- plot.fam.scatter(fam_subset_spec_all, 'Specificity')
p.fam.spec
```

```{r saveFamspec, include=FALSE}
pdf(file = "../report/cazy_family_classification/specificityScatter.pdf",  width = 6.23, height = 8.2)
p.fam.spec
dev.off()
```


Test for significant differences between the means of the tools using a one-way ANOVA.

```{r famspecAnova, echo=FALSE}
fam_subset_spec_all$Prediction_tool <- as.factor(fam_subset_spec_all$Prediction_tool)

fam_subset_spec_all <- fam_subset_spec_all[complete.cases(fam_subset_spec_all), ]

fam.spec.anova <- aov(Statistical_parameter ~ Prediction_tool, data = fam_subset_spec_all)
summary(fam.spec.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r famspecTurkey, echo=FALSE}
fam.spec.turkey <- TukeyHSD(fam.spec.anova, conf.level=.95)
write.csv(
  as.data.frame(fam.spec.turkey$Prediction_tool),
  "../report/cazy_family_classification/fam.spec.tukeyHSD.csv"
)
fam.spec.turkey
```

Now test for statistically significant differences between CAZy classes using a two-way ANOVA.

```{r famspecTwoANOVA, echo=FALSE}
fam.spec.2.aov <- aov(Statistical_parameter ~ CAZy_class * Prediction_tool, data = fam_subset_spec_all)
write.csv(
  summary(fam.spec.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_family_classification/fam.spec.two-way-ANOVA.csv"
)
summary(fam.spec.2.aov)
```

Statistically significant difference was detected so follow up with a Tukey test to determine if there are differences between specific groups.

```{r famspecTwoANOVATurkey, echo=FALSE}
fam.spec.2.tukey <- TukeyHSD(fam.spec.2.aov, "CAZy_class:Prediction_tool")
write.csv(
  as.data.frame(fam.spec.2.tukey$`CAZy_class:Prediction_tool`),
  "../report/cazy_family_classification/fam.spec.aov2.tukeyHSD.csv"
)
fam.spec.2.tukey
```

### Sensitivity

```{r famsSensTable, echo=FALSE}
colnames(fam_subset_sens_all) <- c('CAZy_family', 'Prediction_tool', 'Statistical_parameter', 'CAZy_class')
fam_subset_sens_all.means <- fam_subset_sens_all %>% dplyr::group_by(CAZy_class, Prediction_tool) %>%
  dplyr::summarise(
    "Mean"=mean(Statistical_parameter),
    "Standard Deviation"=sd(Statistical_parameter),
    "Lower CI"=CI(Statistical_parameter)[3],
    "Upper CI"=CI(Statistical_parameter)[1]
)

write.csv(fam_subset_sens_all.means, "../report/cazy_family_classification/sensitivityTable.csv", row.names=FALSE)

kable(fam_subset_sens_all.means, caption="Sensitivity of CAZy family classification per CAZyme classifier", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famsSensCI, echo=FALSE, fig.cap="95% confidence intervals of the sensitivity of CAZy family classification across all CAZy families per CAZy class, for each CAZyme classifier."}
colnames(fam_subset_sens_all.means) <- c('CAZy_class','Prediction_tool','Mean','SD','LowerCI','UpperCI')
p.fam.sens.CI = ggplot(fam_subset_sens_all.means %>% dplyr::group_by(CAZy_class),
                 aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  scale_colour_manual(values = colour_set_tools) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Sensitivity") +
  facet_wrap(~ CAZy_class, ncol=2)
p.fam.sens.CI
```

```{r saveFamsensCI, include=FALSE}
pdf(file = "../report/cazy_family_classification/sensitivityCI.pdf",  width = 6.23, height = 8.2)
p.fam.sens.CI
dev.off()

pdf(file = "../report/cazy_family_classification/sensitivityCI-SHTOR.pdf",  width = 6.23, height = 8.2)
p.fam.sens.CI = ggplot(fam_subset_sens_all.means %>% dplyr::group_by(CAZy_class),
                 aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  scale_colour_manual(values = colour_set_tools) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Sensitivity") +
  ylim(0.7,1.05)+
  facet_wrap(~ CAZy_class, ncol=2)
p.fam.sens.CI
dev.off()
```

```{r famsSens, echo=FALSE, fig.cap="Scatter plot of overlaying a one-dimensional box-and-whisker plot of sensitivity for each CAZy family for each CAZyme classifier. Each CAZy family is represented as a single point on the plot."}
colnames(fam_subset_sens_all) <- c('CAZy_family', 'Prediction_tool', 'Statistical_parameter', 'CAZy_class')

p.fam.sens <- plot.fam.scatter(fam_subset_sens_all, 'Sensitivity')
p.fam.sens
```

```{r saveFamSens, include=FALSE}
pdf(file = "../report/cazy_family_classification/sensitivityScatter.pdf",  width = 6.23, height = 8.2)
p.fam.sens
dev.off()
```

Test for significant differences between the means of the tools using a one-way ANOVA.

```{r famsensAnova, echo=FALSE}
fam_subset_sens_all$Prediction_tool <- as.factor(fam_subset_sens_all$Prediction_tool)

fam_subset_sens_all <- fam_subset_sens_all[complete.cases(fam_subset_sens_all), ]

fam.sens.anova <- aov(Statistical_parameter ~ Prediction_tool, data = fam_subset_sens_all)
summary(fam.sens.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r famsensTurkey, echo=FALSE}
fam.sens.turkey <- TukeyHSD(fam.sens.anova, conf.level=.95)
write.csv(
  as.data.frame(fam.sens.turkey$Prediction_tool),
  "../report/cazy_family_classification/fam.sens.tukeyHSD.csv"
)
fam.sens.turkey
```

Now test for statistically significant differences between CAZy classes using a two-way ANOVA.

```{r famsensTwoANOVA, echo=FALSE}
fam.sens.2.aov <- aov(Statistical_parameter ~ CAZy_class * Prediction_tool, data = fam_subset_sens_all)
write.csv(
  summary(fam.sens.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_family_classification/fam.sens.two-way-ANOVA.csv"
)
summary(fam.sens.2.aov)
```

Statistically significant difference was detected so follow up with a Tukey test to determine if there are differences between specific groups.

```{r famsensTwoANOVATurkey, echo=FALSE}
fam.sens.2.tukey <- TukeyHSD(fam.sens.2.aov, "CAZy_class:Prediction_tool")
write.csv(
  as.data.frame(fam.sens.2.tukey$`CAZy_class:Prediction_tool`),
  "../report/cazy_family_classification/fam.sens.aov2.tukeyHSD.csv"
)
fam.sens.2.tukey
```


### Precision

```{r famsPrecTable, echo=FALSE}
colnames(fam_subset_prec_all) <- c('X', 'CAZy_family', 'Prediction_tool', 'Parameter_name', 'Statistical_parameter', 'CAZy_class')
fam_subset_prec_all.means <- fam_subset_prec_all %>% dplyr::group_by(CAZy_class, Prediction_tool) %>%
  dplyr::summarise(
    "Mean"=mean(Statistical_parameter),
    "Standard Deviation"=sd(Statistical_parameter),
    "Lower CI"=CI(Statistical_parameter)[3],
    "Upper CI"=CI(Statistical_parameter)[1]
)

write.csv(fam_subset_prec_all.means, "../report/cazy_family_classification/precisionTable.csv", row.names=FALSE)

kable(fam_subset_prec_all.means, caption="Precision of CAZy family classification per CAZyme classifier", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famsPrecCI, echo=FALSE, fig.cap="95% confidence intervals of the precision of CAZy family classification across all CAZy families per CAZy class, for each CAZyme classifier."}
colnames(fam_subset_prec_all.means) <- c('CAZy_class','Prediction_tool','Mean','SD','LowerCI','UpperCI')
p.fam.prec.CI = ggplot(fam_subset_prec_all.means %>% dplyr::group_by(CAZy_class),
                 aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  scale_colour_manual(values = colour_set_tools) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Precision") +
  facet_wrap(~ CAZy_class, ncol=2)
p.fam.prec.CI
```

```{r saveFamspecCI2, include=FALSE}
pdf(file = "../report/cazy_family_classification/precisionCI.pdf",  width = 6.23, height = 8.2)
p.fam.prec.CI
dev.off()
```

```{r famsPrec, echo=FALSE, fig.cap="Scatter plot of overlaying a one-dimensional box-and-whisker plot of precision for each CAZy family for each CAZyme classifier. Each CAZy family is represented as a single point on the plot."}
colnames(fam_subset_prec_all) <- c('X', 'CAZy_family', 'Prediction_tool', 'Parameter_name', 'Statistical_parameter', 'CAZy_class')

p.fam.prec <- plot.fam.scatter(fam_subset_prec_all, 'Precision')
p.fam.prec
```

```{r saveFamPrec, include=FALSE}
pdf(file = "../report/cazy_family_classification/precisionScatter.pdf",  width = 6.23, height = 8.2)
p.fam.prec
dev.off()
```

Test for significant differences between the means of the tools using a one-way ANOVA.

```{r famprecAnova, echo=FALSE}
fam_subset_prec_all$Prediction_tool <- as.factor(fam_subset_prec_all$Prediction_tool)

fam_subset_prec_all <- fam_subset_prec_all[complete.cases(fam_subset_prec_all), ]

fam.prec.anova <- aov(Statistical_parameter ~ Prediction_tool, data = fam_subset_prec_all)
summary(fam.prec.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r famprecTurkey, echo=FALSE}
fam.prec.turkey <- TukeyHSD(fam.prec.anova, conf.level=.95)
write.csv(
  as.data.frame(fam.prec.turkey$Prediction_tool),
  "../report/cazy_family_classification/fam.prec.tukeyHSD.csv"
)
fam.prec.turkey
```

Now test for statistically significant differences between CAZy classes using a two-way ANOVA.

```{r famprecTwoANOVA, echo=FALSE}
fam.prec.2.aov <- aov(Statistical_parameter ~ CAZy_class * Prediction_tool, data = fam_subset_prec_all)
write.csv(
  summary(fam.prec.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_family_classification/fam.prec.two-way-ANOVA.csv"
)
summary(fam.prec.2.aov)
```

Statistically significant difference was detected so follow up with a Tukey test to determine if there are differences between specific groups.

```{r famprecTwoANOVATurkey, echo=FALSE}
fam.prec.2.tukey <- TukeyHSD(fam.prec.2.aov, "CAZy_class:Prediction_tool")
write.csv(
  as.data.frame(fam.prec.2.tukey$`CAZy_class:Prediction_tool`),
  "../report/cazy_family_classification/fam.prec.aov2.tukeyHSD.csv"
)
fam.prec.2.tukey
```



### F1-score

```{r famsF1Table, echo=FALSE}
colnames(fam_subset_f1_all) <- c('X', 'CAZy_family', 'Prediction_tool', 'Parameter_name', 'Statistical_parameter', 'CAZy_class', 'cal.form')
fam_subset_f1_all.means <- fam_subset_f1_all %>% dplyr::group_by(CAZy_class, Prediction_tool) %>%
  dplyr::summarise(
    "Mean"=mean(Statistical_parameter),
    "Standard Deviation"=sd(Statistical_parameter),
    "Lower CI"=CI(Statistical_parameter)[3],
    "Upper CI"=CI(Statistical_parameter)[1]
)

write.csv(fam_subset_f1_all.means, "../report/cazy_family_classification/f1-scoreTable.csv", row.names=FALSE)

kable(fam_subset_f1_all.means, caption="F1-score of CAZy family classification per CAZyme classifier", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famsF1CI, echo=FALSE, fig.cap="95% confidence intervals of the F1-score of CAZy family classification across all CAZy families per CAZy class, for each CAZyme classifier."}
colnames(fam_subset_f1_all.means) <- c('CAZy_class','Prediction_tool','Mean','SD','LowerCI','UpperCI')
p.fam.f1.CI.short = ggplot(fam_subset_f1_all.means %>% dplyr::group_by(CAZy_class),
                 aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  scale_colour_manual(values = colour_set_tools) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("F1-score") +
  ylim(0.25, 1.05) +
  facet_wrap(~ CAZy_class, ncol=2)

p.fam.f1.CI = ggplot(fam_subset_f1_all.means %>% dplyr::group_by(CAZy_class),
                 aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  scale_colour_manual(values = colour_set_tools) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("F1-score") +
  ylim(0.4,1.05)+
  facet_wrap(~ CAZy_class, ncol=2)
p.fam.f1.CI
```

```{r saveFamf1CI, include=FALSE}
pdf(file = "../report/cazy_family_classification/f1-scoreCI.pdf",  width = 6.23, height = 8.2)
p.fam.f1.CI
dev.off()

pdf(file = "../report/cazy_family_classification/f1-scoreCI-SHORT.pdf",  width = 6.23, height = 8.2)
p.fam.f1.CI.short
dev.off()

pdf(file = "../report/cazy_family_classification/f1-scoreCI-SHORT-SHORT.pdf",  width = 6.23, height = 8.2)
p.fam.f1.CI.sss = ggplot(fam_subset_f1_all.means %>% dplyr::group_by(CAZy_class),
                 aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  scale_colour_manual(values = colour_set_tools) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("F1-score") +
  ylim(0.7,1.05)+
  facet_wrap(~ CAZy_class, ncol=2)
p.fam.f1.CI.sss
dev.off()
```

```{r famsF1, echo=FALSE, fig.cap="Scatter plot of overlaying a one-dimensional box-and-whisker plot of F1-score for each CAZy family for each CAZyme classifier. Each CAZy family is represented as a single point on the plot."}
colnames(fam_subset_f1_all) <- c('X', 'CAZy_family', 'Prediction_tool', 'Parameter_name', 'Statistical_parameter', 'CAZy_class', 'cal.form')

p.fam.f1 <- plot.fam.scatter(fam_subset_f1_all, 'F1-score')
p.fam.f1
```

```{r saveFamF1, include=FALSE}
pdf(file = "../report/cazy_family_classification/f1-scoreScatter.pdf",  width = 6.23, height = 8.2)
p.fam.f1
dev.off()
```

Test for significant differences between the means of the tools using a one-way ANOVA.

```{r famf1Anova, echo=FALSE}
fam_subset_f1_all$Prediction_tool <- as.factor(fam_subset_f1_all$Prediction_tool)

fam_subset_f1_all <- fam_subset_f1_all[complete.cases(fam_subset_f1_all), ]

fam.f1.anova <- aov(Statistical_parameter ~ Prediction_tool, data = fam_subset_f1_all)
summary(fam.f1.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r famf1Turkey, echo=FALSE}
fam.f1.turkey <- TukeyHSD(fam.f1.anova, conf.level=.95)
write.csv(
  as.data.frame(fam.f1.turkey$Prediction_tool),
  "../report/cazy_family_classification/fam.f1.tukeyHSD.csv"
)
fam.f1.turkey
```


Now test for statistically significant differences between CAZy classes using a two-way ANOVA.

```{r famf1TwoANOVA, echo=FALSE}
fam.f1.2.aov <- aov(Statistical_parameter ~ CAZy_class * Prediction_tool, data = fam_subset_f1_all)
write.csv(
  summary(fam.f1.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_family_classification/fam.f1.two-way-ANOVA.csv"
)
summary(fam.f1.2.aov)
```

Statistically significant difference was detected so follow up with a Tukey test to determine if there are differences between specific groups.

```{r famf1TwoANOVATurkey, echo=FALSE}
fam.f1.2.tukey <- TukeyHSD(fam.f1.2.aov, "CAZy_class:Prediction_tool")
write.csv(
  as.data.frame(fam.f1.2.tukey$`CAZy_class:Prediction_tool`),
  "../report/cazy_family_classification/fam.f1.aov2.tukeyHSD.csv"
)
fam.f1.2.tukey
```



### Accuracy

```{r famsAccTable, echo=FALSE}
colnames(fam_subset_acc_all) <- c('X', 'CAZy_family', 'Prediction_tool', 'Parameter_name', 'Statistical_parameter', 'CAZy_class')
fam_subset_acc_all.means <- fam_subset_acc_all %>% dplyr::group_by(CAZy_class, Prediction_tool) %>%
  dplyr::summarise(
    "Mean"=mean(Statistical_parameter),
    "Standard Deviation"=sd(Statistical_parameter),
    "Lower CI"=CI(Statistical_parameter)[3],
    "Upper CI"=CI(Statistical_parameter)[1]
)

write.csv(fam_subset_acc_all.means, "../report/cazy_family_classification/accuracyTable.csv", row.names=FALSE)

kable(fam_subset_acc_all.means, caption="Accuracy of CAZy family classification per CAZyme classifier", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famsAccCI, echo=FALSE, fig.cap="95% confidence intervals of the accuarcy of CAZy family classification across all CAZy families per CAZy class, for each CAZyme classifier."}
colnames(fam_subset_acc_all.means) <- c('CAZy_class','Prediction_tool','Mean','SD','LowerCI','UpperCI')
p.fam.acc.CI = ggplot(fam_subset_acc_all.means %>% dplyr::group_by(CAZy_class),
                 aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  scale_colour_manual(values = colour_set_tools) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Accuracy") +
  facet_wrap(~ CAZy_class, ncol=2)
p.fam.acc.CI
```
```{r saveFamaccCI, include=FALSE}
pdf(file = "../report/cazy_family_classification/accuracyCI.pdf",  width = 6.23, height = 8.2)
p.fam.acc.CI
dev.off()
```

```{r famsAcc, echo=FALSE, fig.cap="Scatter plot of overlaying a one-dimensional box-and-whisker plot of the accuracy for each CAZy family for each CAZyme classifier. Each CAZy family is represented as a single point on the plot."}
colnames(fam_subset_acc_all) <- c('X', 'CAZy_family', 'Prediction_tool', 'Parameter_name', 'Statistical_parameter', 'CAZy_class')

p.fam.acc <- ggplot(fam_subset_acc_all %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Statistical_parameter, fill=Prediction_tool)) +
    geom_boxplot(outlier.shape=NA) +
    geom_jitter(width=0.1, height=0, size=0.3) +
    geom_hline(yintercept=0.99, linetype="dashed", color = "#5c5c5c") +
    scale_fill_manual(values = colour_set_tools) +
    theme(legend.position = "none",
  	plot.background = element_rect(fill = figbg, color = figbg),
  	axis.text=element_text(size=10),
  	axis.title=element_text(size=12,face="bold"),
  	axis.text.x=element_text(angle=90, vjust=0.45, size=9)) +
    xlab("Classifier") + 
    ylab('Accuracy') +
    facet_wrap(~ CAZy_class, ncol=2)
p.fam.acc
```

```{r saveFamAcc, include=FALSE}
pdf(file = "../report/cazy_family_classification/accuracyScatter.pdf",  width = 6.23, height = 8.2)
p.fam.acc
dev.off()
```

Test for significant differences between the means of the tools using a one-way ANOVA.

```{r famaccAnova, echo=FALSE}
fam_subset_acc_all$Prediction_tool <- as.factor(fam_subset_acc_all$Prediction_tool)

fam_subset_acc_all <- fam_subset_acc_all[complete.cases(fam_subset_acc_all), ]

fam.acc.anova <- aov(Statistical_parameter ~ Prediction_tool, data = fam_subset_acc_all)
summary(fam.acc.anova)
```

The means are statistically significantly different, therefore, use a post hoc Turkey HSD test to determine between which tools the means are significantly difference.

```{r famaccTurkey, echo=FALSE}
fam.acc.turkey <- TukeyHSD(fam.acc.anova, conf.level=.95)
write.csv(
  as.data.frame(fam.acc.turkey$Prediction_tool),
  "../report/cazy_family_classification/fam.acc.tukeyHSD.csv"
)
fam.acc.turkey
```

Now test for statistically significant differences between CAZy classes using a two-way ANOVA.

```{r famaccTwoANOVA, echo=FALSE}
fam.acc.2.aov <- aov(Statistical_parameter ~ CAZy_class * Prediction_tool, data = fam_subset_acc_all)
write.csv(
  summary(fam.acc.2.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_family_classification/fam.acc.two-way-ANOVA.csv"
)
summary(fam.acc.2.aov)
```

Statistically significant difference was detected so follow up with a Tukey test to determine if there are differences between specific groups.

```{r famaccTwoANOVATurkey, echo=FALSE}
fam.acc.2.tukey <- TukeyHSD(fam.acc.2.aov, "CAZy_class:Prediction_tool")
write.csv(
  as.data.frame(fam.acc.2.tukey$`CAZy_class:Prediction_tool`),
  "../report/cazy_family_classification/fam.acc.aov2.tukeyHSD.csv"
)
fam.acc.2.tukey
```



## CAZy family sensitivity against specificity

For better resolution we can group the CAZy families by their parent CAzy classes, and compare the performances of the tools CAZy class, by CAZy class. Owing to the minimal variation in specificity scores, specificity was plotted as the percentage specificity log10.

### Glycoside Hydrolases

Figure \@ref(fig:ghfamrllvspc) shows the plotting of sensitivity against specificity for each Glycoside Hydrolase CAZy family.

```{r ghfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Glycoside Hydrolases. Each GH CAZy family is represented as a single point on the plot."}
p.gh.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'GH')
p.gh.sens.spec.fam
```

```{r saveGhSensSpec, include=FALSE}
pdf(file = "../report/cazy_family_classification/famSpecSensGh.pdf", width = 6.25, height = 8.2)
p.gh.sens.spec.fam
dev.off()
```

### Glycosyltransferases

Figure \@ref(fig:gtfamrllvspc) shows the plotting of sensitivity against specificity for each Glycosyltransferases CAZy family.

```{r gtfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Glycosyltransferases. Each GT CAZy family is represented as a single point on the plot."}
p.gt.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'GT')
p.gt.sens.spec.fam
```

```{r saveGtSensSpec, include=FALSE}
pdf(file = "../report/cazy_family_classification/famSpecSensGt.pdf", width = 6.25, height = 8.2)
p.gt.sens.spec.fam
dev.off()
```

### Polysaccharide Lyases

Figure \@ref(fig:ghfamrllvspc) shows the plotting of sensitivity against specificity for each Polysaccharide Lyases CAZy family.

```{r plfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Polysaccharide Lyases. Each PL CAZy family is represented as a single point on the plot."}
p.pl.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'PL')
p.pl.sens.spec.fam
```

```{r savePlSensSpec, include=FALSE}
pdf(file = "../report/cazy_family_classification/famSpecSensPl.pdf", width = 6.25, height = 8.2)
p.pl.sens.spec.fam
dev.off()
```

### Carbohydrate Esterases

Figure \@ref(fig:cefamrllvspc) shows the plotting of sensitivity against specificity for each Carbohydrate Esterases CAZy family.

```{r cefamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Carbohydrate Esterases. Each CE CAZy family is represented as a single point on the plot."}
p.ce.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'CE')
p.ce.sens.spec.fam
```

```{r saveCeSensSpec, include=FALSE}
pdf(file = "../report/cazy_family_classification/famSpecSensCe.pdf", width = 6.25, height = 8.2)
p.ce.sens.spec.fam
dev.off()
```

### Auxillary Activities

The figure below shows the plotting of sensitivity against specificity for each Auxillary Activities CAZy family.

```{r aafamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Auxillary Activities. Each AA CAZy family is represented as a single point on the plot."}
p.aa.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'AA')
p.aa.sens.spec.fam
```

```{r saveAaSensSpec, include=FALSE}
pdf(file = "../report/cazy_family_classification/famSpecSensAa.pdf", width = 6.25, height = 8.2)
p.aa.sens.spec.fam
dev.off()
```


### Carbohydate Binding Modules

Figure \@ref(fig:cbmfamrllvspc) shows the plotting of sensitivity against specificity for each Carbohydrate Binding Module CAZy family.

```{r cbmfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Carbohydrate Binding Modules. Each CBM CAZy family is represented as a single point on the plot."}
p.cbm.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'CBM')
p.cbm.sens.spec.fam
```

```{r saveCbmSensSpec, include=FALSE}
pdf(file = "../report/cazy_family_classification/famSpecSensCbm.pdf", width = 6.25, height = 8.2)
p.cbm.sens.spec.fam
dev.off()
```

## Consistently poor performing CAZy families

We can pull out CAZy families for which at least three of the evaluated classifiers (when including the individual tools incoporated into dbCAN) produce a sensitivity score of less than 0.75 for said CAZy family.

```{r poorCAZyFams, include=FALSE}
# CAZy family names were retrieved earlier on

# create subsets by CAZy class and associated each data set with the name of its CAZy class
gh.subset <- cazy_family_long_df[which(cazy_family_long_df$CAZy_family %in% gh.names), ]
gh.class <- rep('GH', nrow(gh.subset))
gh.subset$CAZy_class <- gh.class

gt.subset <- cazy_family_long_df[which(cazy_family_long_df$CAZy_family %in% gt.names), ]
gt.class <- rep('GT', nrow(gt.subset))
gt.subset$CAZy_class <- gt.class
  
pl.subset <- cazy_family_long_df[which(cazy_family_long_df$CAZy_family %in% pl.names), ]
pl.class <- rep('PL', nrow(pl.subset))
pl.subset$CAZy_class <- pl.class
  
ce.subset <- cazy_family_long_df[which(cazy_family_long_df$CAZy_family %in% ce.names), ]
ce.class <- rep('CE', nrow(ce.subset))
ce.subset$CAZy_class <- ce.class
  
aa.subset <- cazy_family_long_df[which(cazy_family_long_df$CAZy_family %in% aa.names), ]
aa.class <- rep('AA', nrow(aa.subset))
aa.subset$CAZy_class <- aa.class
  
cbm.subset <- cazy_family_long_df[which(cazy_family_long_df$CAZy_family %in% cbm.names), ]
cbm.class <- rep('CBM', nrow(cbm.subset))
cbm.subset$CAZy_class <- cbm.class

# combine the class dfs in to one
cazy_family_long_df.class <- rbind(gh.subset, gt.subset)
cazy_family_long_df.class <- rbind(cazy_family_long_df.class, pl.subset)
cazy_family_long_df.class <- rbind(cazy_family_long_df.class, ce.subset)
cazy_family_long_df.class <- rbind(cazy_family_long_df.class, aa.subset)
cazy_family_long_df.class <- rbind(cazy_family_long_df.class, cbm.subset)

# set order data is presented
gh.subset$Prediction_tool <- factor(gh.subset$Prediction_tool, levels = classifiers_full) 
gt.subset$Prediction_tool <- factor(gt.subset$Prediction_tool, levels = classifiers_full) 
pl.subset$Prediction_tool <- factor(pl.subset$Prediction_tool, levels = classifiers_full) 
ce.subset$Prediction_tool <- factor(ce.subset$Prediction_tool, levels = classifiers_full) 
aa.subset$Prediction_tool <- factor(aa.subset$Prediction_tool, levels = classifiers_full) 
cbm.subset$Prediction_tool <- factor(cbm.subset$Prediction_tool, levels = classifiers_full) 


ident.poor.fams <- function(df){
  df.sens.subset <- df[which(df$Statistical_parameter == "Sensitivity"), ]
  low.sens <- df.sens.subset[which(df.sens.subset$Statistic_value <= "0.75"), ]
  
  all.poor.fams <- unique(low.sens$CAZy_family)
  
  poor.fams <- c()
  for (fam in all.poor.fams){
    if (length(low.sens[which(low.sens$CAZy_family == fam), ]) > 2){
      poor.fams <- append(poor.fams, fam)
    }
  }
  
  return(poor.fams)
}


get_poor_fams <- function(df){
  df.sens.subset <- df[which(df$Statistical_parameter == "Sensitivity"), ]
  low.sens <- df.sens.subset[which(df.sens.subset$Statistic_value <= "0.75"), ]
  
  # low.sens contains only the rows with a sensitivity less than 0.75, so some tools sensitivity scores are excluded
  low.sens.families = unique(low.sens$CAZy_family)
  
  low.sens.df <- data.frame(
    X=c(),
    CAZy_family=c(),
    Prediction_tool=c(),
    Statistical_parameter=c(),
    Statistic_value=c(),
    CAZy_class=c()
  )

  for (fam in low.sens.families){
     new_df <- df.sens.subset[which(df.sens.subset$CAZy_family == fam), ]
     low.sens.df <- rbind(low.sens.df, new_df)
  }
  
  # this retrieves all CAZy families with at least one tool producing a sensitivity of less than 0.75
  # retrieve CAZy families for which at least 3 tools produced a sensitivity score of less than 0.75
  tools <- classifiers_full
  
  poor.fam.df <- data.frame(
    X=c(),
    CAZy_family=c(),
    Prediction_tool=c(),
    Statistical_parameter=c(),
    Statistic_value=c(),
    CAZy_class=c()
  )
  
  parsed_fams <- vector()
  
  
  for (row in 1:nrow(low.sens.df)){
    no.of.tools <- 0

    # retrieve all rows for the CAZy fam
    cazy.fam <- low.sens.df[row, "CAZy_family"]
    fam.low.sens.df <- low.sens.df[which(low.sens.df$CAZy_fam == cazy.fam), ]
    
    fam.low.sens.df <- fam.low.sens.df[complete.cases(fam.low.sens.df), ]
    if (length(fam.low.sens.df$CAZy_family == 3)){
    
    
      if (!(cazy.fam %in% parsed_fams)){
        
        for (fam.row in 1:nrow(fam.low.sens.df)){
          
          sens.score <- fam.low.sens.df[fam.row, "Statistic_value"]
          if (sens.score <= 0.75) {no.of.tools = no.of.tools + 1}
        }
        
        if (no.of.tools >= 3){poor.fam.df <- rbind(poor.fam.df, fam.low.sens.df)}
      }
      
      parsed_fams = append(parsed_fams, cazy.fam)
      
    }
    
  }
  return(poor.fam.df)
}

add_sample_data <- function(df){
  sample_sizes = c()
  populations = c()
  cazy.fams <- c()
  
  for (row in 1:nrow(df)){
    fam <- df[row, "CAZy_family"]
    
    if (fam %in% cazy.fams){
      next
    } else{
      pop <- fam.populations[[fam]]
      sample <- fam.sample.sizes[[fam]]
    
      sample_sizes = append(sample_sizes, sample)
      populations = append(populations, pop)
      
      cazy.fams <- append(cazy.fams, fam)
    }
  }
  
  new_df <- data.frame(sample_sizes, populations, cazy.fams)
  colnames(new_df) <- c('Sample', 'Population', 'CAZy_family')
  
  return(new_df)
}

build_poor_fam_plot <- function(df, pop_sample_df){
  new.plot <- ggplot(
    df %>% dplyr::group_by(Prediction_tool),
    aes(x=Prediction_tool, y=CAZy_family, fill=Statistic_value)
  ) +
    geom_tile() +
    scale_fill_viridis_c() +
    geom_text(data=df, aes(label=round(df$Statistic_value, digits=3)), size=2.5) +
    geom_text(data=pop_sample_df, inherit.aes=FALSE, aes(label=Sample, x='Sample',  y=CAZy_family), size=2.5) +
    geom_text(data=pop_sample_df,inherit.aes=FALSE, aes(label=Population, x='Population', y=CAZy_family), size=2.5) +
    theme(legend.background = element_rect(fill = figbg, color = figbg),
          plot.background = element_rect(fill = figbg, color = figbg), 
          text = element_text(size=10), 
          legend.text=element_text(size=10),
          legend.title=element_text(size=12),
          axis.text=element_text(size=8),
          axis.title=element_text(size=12,face="bold"),
          axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
    xlab("Classifier") +
    ylab("CAZy family") +
    labs(fill = "Sensitivity")
  
  return(new.plot)
}

```

## GH difficult families

```{r poorGHfams, echo=FALSE}
gh.low.sens <- get_poor_fams(gh.subset)
gh.sample.pop.df <- add_sample_data(gh.low.sens)

# manually set the CAZy family order
gh.fam.order <- c("GH0" ,  "GH22",  "GH36", "GH45",  "GH52",  "GH85",  "GH123", "GH124", "GH133", "GH164", "GH175", "GH176", "GH178", "GH183")
gh.low.sens$CAZy_family <- factor(gh.low.sens$CAZy_family, levels = gh.fam.order) 
gh.low.sens$Prediction_tool <- factor(gh.low.sens$Prediction_tool, levels = classifiers_full) 

# generate plot
p.gh.poor.pops <- build_poor_fam_plot(gh.low.sens, gh.sample.pop.df)
p.gh.poor.pops
```

```{r saveGhPoorFams, include=FALSE}
dir.create("../report/cazy_family_classification/poor_performing_fams/", recursive=FALSE)
pdf(file = "../report/cazy_family_classification/poor_performing_fams/poorFamSensGH.pdf", width = 8, height = 5)
p.gh.poor.pops
dev.off()
```


## GT difficult families

```{r poorGTfams, echo=FALSE}
gt.low.sens <- get_poor_fams(gt.subset)
gt.sample.pop.df <- add_sample_data(gt.low.sens)

# manually set the CAZy family order
gt.fam.order <- c("GT0",  "GT1" ,  "GT31",  "GT44",  "GT49",  "GT52",  "GT61" , "GT80",  "GT109")
gt.low.sens$CAZy_family <- factor(gt.low.sens$CAZy_family, levels = gt.fam.order) 
gt.low.sens$Prediction_tool <- factor(gt.low.sens$Prediction_tool, levels = classifiers_full) 

# generate plot
p.gt.poor.pops <- build_poor_fam_plot(gt.low.sens, gt.sample.pop.df)
p.gt.poor.pops
```

```{r saveGtPoorFams, include=FALSE}
pdf(file = "../report/cazy_family_classification/poor_performing_fams/poorFamSensGT.pdf", width = 8, height = 4)
p.gt.poor.pops
dev.off()
```

## PL difficult families

```{r poorPLfams, echo=FALSE}
pl.low.sens <- get_poor_fams(pl.subset)
pl.pop.sam.df <- add_sample_data(pl.low.sens)

# manually set the CAZy family order
pl.fam.order <- c("PL0",  "PL8",  "PL27")
pl.low.sens$CAZy_family <- factor(pl.low.sens$CAZy_family, levels = pl.fam.order) 
pl.low.sens$Prediction_tool <- factor(pl.low.sens$Prediction_tool, levels = classifiers_full) 
# generate plot
p.pl.poor.pops <- build_poor_fam_plot(pl.low.sens, pl.pop.sam.df)
p.pl.poor.pops
```

```{r savePlPoorFams, include=FALSE}
pdf(file = "../report/cazy_family_classification/poor_performing_fams/poorFamSensPL.pdf", width = 8, height = 3)
p.pl.poor.pops
dev.off()
```

## CE difficult families

```{r poorCEfams, echo=FALSE}
ce.low.sens <- get_poor_fams(ce.subset)
ce.pop.sam.df <- add_sample_data(ce.low.sens)

# manually set the CAZy family order
ce.fam.order <- c("CE0","CE13")
ce.low.sens$CAZy_family <- factor(ce.low.sens$CAZy_family, levels = ce.fam.order) 
ce.low.sens$Prediction_tool <- factor(ce.low.sens$Prediction_tool, levels = classifiers_full) 
# generate plot
p.ce.poor.pops <- build_poor_fam_plot(ce.low.sens,ce.pop.sam.df)
p.ce.poor.pops
```

```{r saveCePoorFams, include=FALSE}
pdf(file = "../report/cazy_family_classification/poor_performing_fams/poorFamSensCE.pdf", width = 8, height = 3)
p.ce.poor.pops
dev.off()
```

## AA difficult families

```{r poorAAfams, echo=FALSE}
aa.low.sens <- get_poor_fams(aa.subset)
aa.pop.sam.df <- add_sample_data(aa.low.sens)

# mannually set the CAZy family order
aa.fam.order <- c("AA0","AA4")
aa.low.sens$CAZy_family <- factor(aa.low.sens$CAZy_family, levels = aa.fam.order) 
aa.low.sens$Prediction_tool <- factor(aa.low.sens$Prediction_tool, levels = classifiers_full) 
# generate plot
p.aa.poor.pops <- build_poor_fam_plot(aa.low.sens, aa.pop.sam.df)
p.aa.poor.pops
```

```{r saveAaPoorFams, include=FALSE}
pdf(file = "../report/cazy_family_classification/poor_performing_fams/poorFamSensAA.pdf", width = 8, height = 2.9)
p.aa.poor.pops
dev.off()
```


## CBM difficult families

```{r poorCBMfams, echo=FALSE}
cbm.low.sens <- get_poor_fams(cbm.subset)
cbm.pop.sam.df <- add_sample_data(cbm.low.sens)

# manually set the CAZy family order
cbm.fam.order <- c("CBM0",  "CBM1",  "CBM5","CBM10", "CBM12", "CBM13", "CBM14", "CBM18", "CBM19", "CBM25", "CBM32", "CBM41","CBM43", "CBM45", "CBM48", "CBM50", "CBM52", "CBM69", "CBM87", "CBM91", "CBM92", "CBM96", "CBM97")
cbm.low.sens$CAZy_family <- factor(cbm.low.sens$CAZy_family, levels = cbm.fam.order) 

# generate plot
p.cbm.poor.pops <- build_poor_fam_plot(cbm.low.sens, cbm.pop.sam.df)
p.cbm.poor.pops
```

```{r saveCbmPoorFams, include=FALSE}
pdf(file = "../report/cazy_family_classification/poor_performing_fams/poorFamSensCbm.pdf", width = 8, height = 6)
p.cbm.poor.pops
dev.off()
```

## All poor performing CAZyme families

```{r allPoorFams, echo=FALSE}
all.fam.order <- append(gh.fam.order, gt.fam.order)
all.fam.order <- append(all.fam.order, pl.fam.order)
all.fam.order <- append(all.fam.order, ce.fam.order)
all.fam.order <- append(all.fam.order, aa.fam.order)
all.fam.order <- append(all.fam.order, cbm.fam.order)

all.low.sens <- rbind(gh.low.sens, gt.low.sens)
all.low.sens <- rbind(all.low.sens, pl.low.sens)
all.low.sens <- rbind(all.low.sens, ce.low.sens)
all.low.sens <- rbind(all.low.sens, aa.low.sens)
all.low.sens <- rbind(all.low.sens, cbm.low.sens)

all.sample.pop.df <- rbind(gh.sample.pop.df, gt.sample.pop.df)
all.sample.pop.df <- rbind(all.sample.pop.df, pl.pop.sam.df)
all.sample.pop.df <- rbind(all.sample.pop.df, ce.pop.sam.df)
all.sample.pop.df <- rbind(all.sample.pop.df, aa.pop.sam.df)
all.sample.pop.df <- rbind(all.sample.pop.df, cbm.pop.sam.df)

all.low.sens$CAZy_family <- factor(all.low.sens$CAZy_family, levels = all.fam.order) 


p.all.poor.pops <- build_poor_fam_plot(all.low.sens, all.sample.pop.df)
p.all.poor.pops
```
```{r saveAllPoorFams, include=FALSE}
pdf(file = "../report/cazy_family_classification/poor_performing_fams/poorFamSensAll.pdf", width = 8, height = 11)
p.all.poor.pops
dev.off()
```

# Evaluation of multi-label CAZy family classification performance

CAZy annotates proteins in a domain-wise manner. Consequently, a single protein may be assigned to multiple CAZy families. The ability of a classifier to assign all the correct CAZy family annotations for a given protein when only evaluating the CAZy family classification performance per CAZy family, independently of all other CAZy classes.

Multilabel classification raises when a single instance can be assinged to multiple classes. In this evaluation a single instance is a protein and the classes are CAZy families, a single CAZyme can be assigned to multiple CAZy families. This is important to take into consideration because the same approaches for statistical evaluation of binary classification provided a limited view of the performance of the classifiers when applied to multilabel classification.

The CAZy family multi-label classification performance is represented by the Rand Index (RI) and Adjusted Rand Index (ARI). The RI is a quantitive measure of similarity between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings. In this case the two clusters are the predicted and groud truth CAZy family annotations. The raw RI score is then “adjusted for chance” into the ARI score using the following scheme:  
`ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)`
This produces a score between 1 and -1. A score of 1 is produced if all predicted and known CAZy family annotations are identical, 0 if completely random clustering of -1 if systematically incorrect clustering and the number of incorrect classifications of proteins is greater than would be expected from randomly annotating proteins with CAZy families.

```{r famRi, echo=FALSE}
fam_classification_df$Prediction_tool <- factor(fam_classification_df$Prediction_tool, levels = classifiers_full) # set order data is presented

fam_ri_stats_df <- fam_classification_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Rand_index_binary),
  "Standard Deviation"=sd(Rand_index_binary),
  "Lower CI"=CI(Rand_index_binary)[3],
  "Upper CI"=CI(Rand_index_binary)[1]
)

dir.create("../report/cazy_family_classification/multilabel_classification/", recursive=FALSE)
write.csv(fam_ri_stats_df, "../report/cazy_family_classification/multilabel_classification/ri_table.csv", row.names=FALSE)

kable(fam_ri_stats_df, caption="Rand Index of CAZyme classifier classification of CAZy family annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famAriCalc, echo=FALSE}
fam_ari_stats_df <- fam_classification_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Adjusted_Rand_index_binary),
  "Standard Deviation"=sd(Adjusted_Rand_index_binary),
  "Lower CI"=CI(Adjusted_Rand_index_binary)[3],
  "Upper CI"=CI(Adjusted_Rand_index_binary)[1]
)

write.csv(fam_ari_stats_df, "../report/cazy_family_classification/multilabel_classification/ri_table.csv", row.names=FALSE)

kable(fam_ari_stats_df, caption="Adjusted Rand Index of CAZyme classifier classification of CAZy family annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```


# CAZy family taxonomic performance

## Across all of CAZy

### Specificity

```{r famTaxSpec, echo=FALSE}
# statistical_parameter needs to be changed to Statistic_parameter for the build.tax.class.stat.sum function
colnames(fam.tax.df) <- c('X', 'CAZy_family', 'Prediction_tool', 'Statistic_parameter', 'Statistic_value', 'CAZy_class', 'Tax_group')
fam.tax.df$Prediction_tool <- factor(fam.tax.df$Prediction_tool, levels=classifiers_full)

fam.df.spec.sum.df <- build.tax.class.stat.sum(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Specificity"
)
fam.df.spec.sum.df$Prediction_tool <- ordered(fam.df.spec.sum.df$Prediction_tool, levels=classifiers_full)

dir.create("../report/cazy_family_classification/tax_performance", recursive=FALSE)
write.csv(
  fam.df.spec.sum.df,
  "../report/cazy_family_classification/tax_performance/specificity.summaryTable.csv",
  row.names=FALSE
)

kable(fam.df.spec.sum.df, caption="Overall performance (represented by the Specificity) of CAZy family classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famTaxSpecCi, echo=FALSE}
p.fam.tax.spec.ci <- build.class.tax.p(fam.df.spec.sum.df, "Specificity")
p.fam.tax.spec.ci
```

```{r famTaxSpecScatter1, echo=FALSE}
# plot scatter plot of spec per test set
p.fam.tax.spec.scatter <- build.class.tax.p.scatter(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Specificity"
)
p.fam.tax.spec.scatter
```


```{r famTaxSpecSave3, include=FALSE}
# save plots
pdf(file = "../report/cazy_family_classification/tax_performance/spec.fam.tax.ci.pdf",  width = 6.23, height = 8)
p.fam.tax.spec.ci
dev.off()

pdf(file = "../report/cazy_family_classification/tax_performance/spec.fam.tax.scatter.pdf",  width = 6.23, height = 8)
p.fam.tax.spec.scatter
dev.off()
```




### Sensitivity

```{r famTaxSens, echo=FALSE}
cazy_fam_df.sens.sum.df <- build.tax.class.stat.sum(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Sensitivity"
)
cazy_fam_df.sens.sum.df$Prediction_tool <- ordered(cazy_fam_df.sens.sum.df$Prediction_tool, levels=classifiers_full)

write.csv(cazy_fam_df.sens.sum.df, "../report/cazy_family_classification/tax_performance/sensitivity.summaryTable.csv", row.names=FALSE)

kable(cazy_fam_df.sens.sum.df, caption="Overall performance (represented by the Sensitivity) of CAZy family classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famTaxSensCi, echo=FALSE}
p.fam.tax.sens.ci <- build.class.tax.p(cazy_fam_df.sens.sum.df, "Sensitivity")
p.fam.tax.sens.ci
```

```{r famTaxSensScatter, echo=FALSE}
# plot scatter plot of sens per test set
p.fam.tax.sens.scatter <- build.class.tax.p.scatter(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Sensitivity"
)
p.fam.tax.sens.scatter
```


```{r famTaxSensSave, include=FALSE}
# save plots
pdf(file = "../report/cazy_family_classification/tax_performance/sens.fam.tax.ci.pdf",  width = 6.23, height = 8)
p.fam.tax.sens.ci
dev.off()

pdf(file = "../report/cazy_family_classification/tax_performance/sens.fam.tax.scatter.pdf",  width = 6.23, height = 8)
p.fam.tax.sens.scatter
dev.off()
```

### Precision

```{r famTaxPrec, echo=FALSE}
cazy_fam_df.prec.sum.df <- build.tax.class.stat.sum(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Precision"
)
cazy_fam_df.prec.sum.df$Prediction_tool <- ordered(cazy_fam_df.prec.sum.df$Prediction_tool, levels=classifiers_full)

dir.create("../report/cazy_family_classification/tax_performance", recursive=FALSE)
write.csv(cazy_fam_df.prec.sum.df, "../report/cazy_family_classification/tax_performance/precision.summaryTable.csv", row.names=FALSE)

kable(cazy_fam_df.prec.sum.df, caption="Overall performance (represented by the Precision) of CAZy family classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famTaxPrecCi, echo=FALSE}
p.fam.tax.prec.ci <- build.class.tax.p(cazy_fam_df.prec.sum.df, "Precision")
p.fam.tax.prec.ci
```

```{r famTaxPrecScatter, echo=FALSE}
# plot scatter plot of prec per test set
p.fam.tax.prec.scatter <- build.class.tax.p.scatter(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Precision"
)
p.fam.tax.prec.scatter
```


```{r famTaxPrecSave, include=FALSE}
# save plots
pdf(file = "../report/cazy_family_classification/tax_performance/prec.fam.tax.ci.pdf",  width = 6.23, height = 8)
p.fam.tax.prec.ci
dev.off()

pdf(file = "../report/cazy_family_classification/tax_performance/prec.fam.tax.scatter.pdf",  width = 6.23, height = 8)
p.fam.tax.prec.scatter
dev.off()
```

### F1-score

```{r famTaxF1, echo=FALSE}
fam.tax.df[fam.tax.df == 'Fbeta_score'] <- 'F1-score'
cazy_fam_df.f1.sum.df <- build.tax.class.stat.sum(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "F1-score"
)

y <- as.character(cazy_fam_df.f1.sum.df$Prediction_tool)
cazy_fam_df.f1.sum.df$Prediction_tool <- factor(y, levels=classifiers_full)
cazy_fam_df.f1.sum.df$Prediction_tool <- ordered(cazy_fam_df.f1.sum.df$Prediction_tool, levels=classifiers_full)
                     
write.csv(cazy_fam_df.f1.sum.df, "../report/cazy_family_classification/tax_performance/F1-score.summaryTable.csv", row.names=FALSE)

kable(
  cazy_fam_df.f1.sum.df,
  caption="Overall performance (represented by the F1-score) of CAZy family classification by CAZy classifiers per taxonomy group",
  align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famTaxF1Ci, echo=FALSE}
p.fam.tax.f1.ci <- build.class.tax.p(cazy_fam_df.f1.sum.df, "F1-score")
p.fam.tax.f1.ci
```

```{r famTaxF1Scatter, echo=FALSE}
# plot scatter plot of f1 per test set
p.fam.tax.f1.scatter <- build.class.tax.p.scatter(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "F1-score"
)
p.fam.tax.f1.scatter
```


```{r famTaxF1Save2, include=FALSE}
# save plots
pdf(file = "../report/cazy_family_classification/tax_performance/f1.fam.tax.ci.pdf",  width = 6.23, height = 8)
p.fam.tax.f1.ci
dev.off()

pdf(file = "../report/cazy_family_classification/tax_performance/f1.fam.tax.scatter.pdf",  width = 6.23, height = 8)
p.fam.tax.f1.scatter
dev.off()
```

### Accuracy

```{r famTaxAcc, echo=FALSE}
cazy_fam_df.acc.sum.df <- build.tax.class.stat.sum(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Accuracy"
)
cazy_fam_df.acc.sum.df$Prediction_tool <- ordered(cazy_fam_df.acc.sum.df$Prediction_tool, levels=classifiers_full)

write.csv(cazy_fam_df.acc.sum.df, "../report/cazy_family_classification/tax_performance/accuracy.summaryTable.csv", row.names=FALSE)

kable(cazy_fam_df.acc.sum.df, caption="Overall performance (represented by the Accuracy) of CAZy family classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famTaxAccCi, echo=FALSE}
p.fam.tax.acc.ci <- build.class.tax.p(cazy_fam_df.acc.sum.df, "Accuracy")
p.fam.tax.acc.ci
```

```{r famTaxAccScatter, echo=FALSE}
p.fam.tax.acc.scatter <- build.class.tax.p.scatter(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Accuracy"
)
p.fam.tax.acc.scatter
```


```{r famTaxAccSave2, include=FALSE}
# save plots
pdf(file = "../report/cazy_family_classification/tax_performance/acc.fam.tax.ci.pdf",  width = 6.23, height = 8)
p.fam.tax.acc.ci
dev.off()

pdf(file = "../report/cazy_family_classification/tax_performance/acc.fam.tax.scatter.pdf",  width = 6.23, height = 8)
p.fam.tax.acc.scatter
dev.off()
```


## Per CAZy class

### Specificity

```{r famTaxSpecTable, echo=FALSE}
fam.tax.df$Prediction_tool <- factor(fam.tax.df$Prediction_tool, levels=classifiers_full)

spec.tax.fam.sum.df <- build.class.tax.sum(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Specificity"
)

dir.create("../report/cazy_family_classification/tax_performance/per_cazy_class", recursive=FALSE)
write.csv(
  spec.tax.fam.sum.df,
  "../report/cazy_family_classification/tax_performance/per_cazy_class/specificity.summaryTable.csv",
  row.names=FALSE
)

kable(spec.tax.fam.sum.df, caption="Overall performance (represented by the Specificity) of CAZy family classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famTaxSpecCiPerClass, echo=FALSE}
p.fam.tax.spec.ci.per.cl <- build.class.tax.per.class.p(spec.tax.fam.sum.df, "Specificity")
p.fam.tax.spec.ci.per.cl
```


```{r famTaxSpecScatter, echo=FALSE}
p.fam.tax.spec.scatter.per.cl <- build.class.tax.p.scatter.per.cl(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Specificity"
)
p.fam.tax.spec.scatter.per.cl
```

```{r famTaxSpecSave, include=FALSE}
# save plots
pdf(file = "../report/cazy_family_classification/tax_performance/per_cazy_class/spec.fam.tax.ci.pdf",  width = 11, height = 8)
p.fam.tax.spec.ci.per.cl
dev.off()

pdf(file = "../report/cazy_family_classification/tax_performance/per_cazy_class/spec.fam.tax.scatter.pdf",  width = 11, height = 8)
p.fam.tax.spec.scatter.per.cl
dev.off()
```


### Sensitivity

```{r famTaxSensTable, echo=FALSE}
# build.class.tax.sum <- function(cazy.class.df, bact.class.df, euk.class.df, stat_param){
sens.tax.fam.sum.df <- build.class.tax.sum(
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  "Sensitivity"
)

dir.create("../report/cazy_family_classification/tax_performance/per_cazy_class", recursive=FALSE)
write.csv(
  sens.tax.fam.sum.df,
  "../report/cazy_family_classification/tax_performance/per_cazy_class/sensitivity.summaryTable.csv",
  row.names=FALSE
)

kable(sens.tax.fam.sum.df, caption="Overall performance (represented by the Sensitivity) of CAZy family classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famsTaxSensCiPerClass, echo=FALSE}
p.fam.tax.sens.ci.per.cl <- build.class.tax.per.class.p(sens.tax.fam.sum.df, "Sensitivity")
p.fam.tax.sens.ci.per.cl
```


```{r famTaxSensScatterClass, echo=FALSE}
# build.class.tax.p.scatter.per.cl(cazy_class_df
p.fam.tax.sens.scatter.per.cl <- build.class.tax.p.scatter.per.cl(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Sensitivity"
)
p.fam.tax.sens.scatter.per.cl
```

```{r famTaxSensPerClassSave, include=FALSE}
# save plots
pdf(file = "../report/cazy_family_classification/tax_performance/per_cazy_class/sens.fam.tax.ci.pdf",  width = 11, height = 8)
p.fam.tax.sens.ci.per.cl
dev.off()



build.class.tax.per.class.p.LIM.LIM <- function(stat_sum_df, stat.val){
  # df has colnames:
  # Prediction_tool, Mean, Standard Deviation, LowerCI, UpperCI, Tax_group, CAZy_clas
	colnames(stat_sum_df) <- c('Prediction_tool', 'Mean', 'SD', 'LowerCI', 'UpperCI', 'Taxonomic_group', 'CAZy_class')
	stat_sum_df$Prediction_tool <- factor(stat_sum_df$Prediction_tool, levels = classifiers_full)
	stat_sum_df$CAZy_class <- factor(stat_sum_df$CAZy_class, levels=cazy_class_list)
	
	p.t.b.ci <- ggplot(stat_sum_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::group_by(Taxonomic_group),
				  aes(x=Prediction_tool, y=Mean, color=Taxonomic_group)) +
	  geom_point(aes(shape=Taxonomic_group), position=position_dodge(width=0.75)) +
	  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI), width = 1, position=position_dodge(width=0.75)) +
	  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
	  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
	  geom_vline(xintercept=stat_sum_df$Prediction_tool, linetype="dashed", color = "#9c9c9c") +
	  scale_colour_manual(values = colour_set) +
	  theme(legend.position = "bottom",
			plot.background = element_rect(fill = figbg, color = figbg),
			axis.text=element_text(size=10),
			axis.title=element_text(size=12,face="bold"),
			axis.text.x=element_text(angle = 90, size=9, vjust=0.45)) +
	  xlab("Classifier") + 
	  ylab(stat.val) +
	  ylim(0.4,1.1) +
	  facet_wrap(~ CAZy_class, ncol=2)
	  
	return(p.t.b.ci)
}

pdf(file = "../report/cazy_family_classification/tax_performance/per_cazy_class/sens.fam.tax.ci-SHORT.pdf",  width = 6, height = 8)
build.class.tax.per.class.p.LIM.LIM(sens.tax.fam.sum.df, "Sensitivity")
dev.off()

pdf(file = "../report/cazy_family_classification/tax_performance/per_cazy_class/sens.fam.tax.scatter.pdf",  width = 11, height = 8)
p.fam.tax.sens.scatter.per.cl
dev.off()
```

### Precision

```{r famTaxPrecTable, echo=FALSE}
prec.tax.fam.sum.df <- build.class.tax.sum(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Precision"
)

write.csv(
  prec.tax.fam.sum.df,
  "../report/cazy_family_classification/tax_performance/per_cazy_class/precision.summaryTable.csv",
  row.names=FALSE
)

kable(prec.tax.fam.sum.df, caption="Overall performance (represented by the Precision) of CAZy family classification by family classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famTaxPrecCiPerClass, echo=FALSE}
p.fam.tax.prec.ci.per.cl <- build.class.tax.per.class.p(prec.tax.fam.sum.df, "Precision")
p.fam.tax.prec.ci.per.cl
```


```{r famTaxPrecScatterClass, echo=FALSE}
p.fam.tax.prec.scatter.per.cl <- build.class.tax.p.scatter.per.cl(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Precision"
)
p.fam.tax.prec.scatter.per.cl
```

```{r famTaxPrecSave3, include=FALSE}
# save plots
pdf(file = "../report/cazy_family_classification/tax_performance/per_cazy_class/prec.fam.tax.ci.pdf",  width = 11, height = 8)
p.fam.tax.prec.ci.per.cl
dev.off()

pdf(file = "../report/cazy_family_classification/tax_performance/per_cazy_class/prec.fam.tax.scatter.pdf",  width = 11, height = 8)
p.fam.tax.prec.scatter.per.cl
dev.off()
```

### F1-score

```{r famTaxF1Table, echo=FALSE}
fam.tax.df  <- fam.tax.df[complete.cases(fam.tax.df), ]
f1.tax.fam.sum.df <- build.class.tax.sum(
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  "F1-score"
)
# build.class.tax.sum <- function(cazy.class.df, bact.class.df, euk.class.df, stat_param){

bact.f1 <- fam.tax.df[which(fam.tax.df$Statistic_parameter == 'F1-score'), ]
bact.f1 <- bact.f1[which(bact.f1$Tax_group == 'Bacteria'), ]
bact.f1.aa <- bact.f1[which(bact.f1$CAZy_class  == 'AA'), ]
bact.f1.aa

write.csv(
  f1.tax.fam.sum.df,
  "../report/cazy_family_classification/tax_performance/per_cazy_class/f1-score.summaryTable.csv",
  row.names=FALSE
)

kable(f1.tax.fam.sum.df, caption="Overall performance (represented by the F1-score) of CAZy family classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famTaxF1CiPerClass, echo=FALSE}
build.class.tax.per.class.p.LIMITED <- function(stat_sum_df, stat.val){
  # df has colnames:
  # Prediction_tool, Mean, Standard Deviation, LowerCI, UpperCI, Tax_group, CAZy_clas
	colnames(stat_sum_df) <- c('Prediction_tool', 'Mean', 'SD', 'LowerCI', 'UpperCI', 'Taxonomic_group', 'CAZy_class')
	stat_sum_df$Prediction_tool <- factor(stat_sum_df$Prediction_tool, levels = classifiers_full)
	stat_sum_df$CAZy_class <- factor(stat_sum_df$CAZy_class, levels=cazy_class_list)
	
	p.t.b.ci <- ggplot(stat_sum_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::group_by(Taxonomic_group),
				  aes(x=Prediction_tool, y=Mean, color=Taxonomic_group)) +
	  geom_point(aes(shape=Taxonomic_group), position=position_dodge(width=0.75)) +
	  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI), width = 1, position=position_dodge(width=0.75)) +
	  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
	  geom_hline(yintercept=0.9, linetype="dashed", color = "#5c5c5c") +
	  geom_vline(xintercept=stat_sum_df$Prediction_tool, linetype="dashed", color = "#9c9c9c") +
	  scale_colour_manual(values = colour_set) +
	  theme(legend.position = "bottom",
			plot.background = element_rect(fill = figbg, color = figbg),
			axis.text=element_text(size=10),
			axis.title=element_text(size=12,face="bold"),
			axis.text.x=element_text(angle = 90, size=9, vjust=0.45)) +
	  xlab("Classifier") + 
	  ylab(stat.val) +
	  ylim(0.3,1.2)+
	  facet_wrap(~ CAZy_class, ncol=2)
	  
	return(p.t.b.ci)
}
p.fam.tax.f1.ci.per.cl <- build.class.tax.per.class.p.LIMITED(f1.tax.fam.sum.df, "F1-score")
p.fam.tax.f1.ci.per.cl
```


```{r famTaxF1CScatterPerClass, echo=FALSE}
p.fam.tax.f1.scatter.per.cl <- build.class.tax.p.scatter.per.cl(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "F1-score"
)
p.fam.tax.f1.scatter.per.cl
```

```{r famTaxF1Save, include=FALSE}
# save plots
pdf(file = "../report/cazy_family_classification/tax_performance/per_cazy_class/f1.fam.tax.ci.pdf",  width = 6, height = 8)
p.fam.tax.f1.ci.per.cl
dev.off()

pdf(file = "../report/cazy_family_classification/tax_performance/per_cazy_class/f1.fam.tax.scatter.pdf",  width = 11, height = 8)
p.fam.tax.f1.scatter.per.cl
dev.off()
```

#### Test for statistically significant differences in the means

```{r famTaxTwoAnova, echo=FALSE}
stat.param <- 'F1-score'

bact.df.stat <- fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ]
bact.df.stat <- bact.df.stat[which(bact.df.stat$Statistic_parameter == stat.param), ]
bact.df.stat  <- bact.df.stat[complete.cases(bact.df.stat), ]
bact.df.stat$Tax_group = rep("Bacteria", nrow(bact.df.stat))
 
euk.df.stat <- fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ]
euk.df.stat <- euk.df.stat[which(euk.df.stat$Statistic_parameter == stat.param), ]
euk.df.stat  <- euk.df.stat[complete.cases(euk.df.stat), ]
euk.df.stat$Tax_group = rep("Eukaryote", nrow(euk.df.stat))

all.df.stat <- fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ]
all.df.stat <- all.df.stat[which(all.df.stat$Statistic_parameter == stat.param), ]
all.df.stat  <- all.df.stat[complete.cases(all.df.stat), ]
all.df.stat$Tax_group <- rep('All', nrow(all.df.stat))

all.df.stat <- rbind(all.df.stat, bact.df.stat)
all.df.stat <- rbind(all.df.stat, euk.df.stat)

all.df.stat$Prediction_tool <- factor(all.df.stat$Prediction_tool, levels = classifiers_full)
all.df.stat$CAZy_class <- factor(all.df.stat$CAZy_class, levels=cazy_class_list)

fam.f1.tax.aov <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = all.df.stat)
write.csv(
  summary(fam.f1.tax.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_family_classification/tax_performance/fam.f1.tax.aov.csv"
)
summary(fam.f1.tax.aov)
```

```{r famTaxTwoAnovaGH, echo=FALSE}
all.df.stat.gh <- all.df.stat[which(all.df.stat$CAZy_class == "GH"), ]

fam.f1.tax.aov.gh <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = all.df.stat.gh)
write.csv(
  summary(fam.f1.tax.aov.gh) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_family_classification/tax_performance/fam.f1.gh.tax.aov.csv"
)
summary(fam.f1.tax.aov.gh)
```

```{r famTaxtukeyGH, echo=FALSE}
fam.gh.tax.tukey <- TukeyHSD(fam.f1.tax.aov.gh, "Tax_group:Prediction_tool")
write.csv(
  as.data.frame(fam.gh.tax.tukey$`Tax_group:Prediction_tool`),
  "../report/cazy_family_classification/tax_performance/fam.GH.tukeyHSD.tax.csv"
)
fam.gh.tax.tukey
```


```{r famTaxTwoAnovaGT, echo=FALSE}
all.df.stat.gt <- all.df.stat[which(all.df.stat$CAZy_class == "GT"), ]

fam.f1.tax.aov.gt <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = all.df.stat.gt)
write.csv(
  summary(fam.f1.tax.aov.gt) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_family_classification/tax_performance/fam.f1.gt.tax.aov.csv"
)
summary(fam.f1.tax.aov.gt)
```

```{r famTaxtukeyGT, echo=FALSE}
fam.gt.tax.tukey <- TukeyHSD(fam.f1.tax.aov.gt, "Tax_group:Prediction_tool")
write.csv(
  as.data.frame(fam.gt.tax.tukey$`Tax_group:Prediction_tool`),
  "../report/cazy_family_classification/tax_performance/fam.GT.tukeyHSD.tax.csv"
)
fam.gt.tax.tukey
```


```{r famTaxTwoAnovaPL, echo=FALSE}
all.df.stat.pl <- all.df.stat[which(all.df.stat$CAZy_class == "PL"), ]

fam.f1.tax.aov.gt <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = all.df.stat.pl)
write.csv(
  summary(fam.f1.tax.aov.gt) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_family_classification/tax_performance/fam.f1.pl.tax.aov.csv"
)
summary(fam.f1.tax.aov.gt)
```

```{r famTaxTwoAnovaCE, echo=FALSE}
all.df.stat.ce <- all.df.stat[which(all.df.stat$CAZy_class == "CE"), ]

fam.f1.tax.aov.ce <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = all.df.stat.ce)
write.csv(
  summary(fam.f1.tax.aov.ce) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_family_classification/tax_performance/fam.f1.ce.tax.aov.csv"
)
summary(fam.f1.tax.aov.ce)
```

```{r famTaxtukeyCE, echo=FALSE}
fam.ce.tax.tukey <- TukeyHSD(fam.f1.tax.aov.ce, "Tax_group:Prediction_tool")
write.csv(
  as.data.frame(fam.ce.tax.tukey$`Tax_group:Prediction_tool`),
  "../report/cazy_family_classification/tax_performance/fam.CE.tukeyHSD.tax.csv"
)
fam.ce.tax.tukey
```



```{r famTaxTwoAnovaAA, echo=FALSE}
all.df.stat.aa <- all.df.stat[which(all.df.stat$CAZy_class == "AA"), ]

fam.f1.tax.aov.gt <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = all.df.stat.aa)
write.csv(
  summary(fam.f1.tax.aov.gt) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_family_classification/tax_performance/fam.f1.aa.tax.aov.csv"
)
summary(fam.f1.tax.aov.gt)
```
```{r famTaxTwoAnovaCBM, echo=FALSE}
all.df.stat.cbm <- all.df.stat[which(all.df.stat$CAZy_class == "CBM"), ]

fam.f1.tax.aov.cbm <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = all.df.stat.cbm)
write.csv(
  summary(fam.f1.tax.aov.cbm) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_family_classification/tax_performance/fam.f1.cbm.tax.aov.csv"
)
summary(fam.f1.tax.aov.cbm)
```
```{r famTaxtukeyCBM, echo=FALSE}
fam.cbm.tax.tukey <- TukeyHSD(fam.f1.tax.aov.cbm, "Tax_group:Prediction_tool")
write.csv(
  as.data.frame(fam.cbm.tax.tukey$`Tax_group:Prediction_tool`),
  "../report/cazy_family_classification/tax_performance/fam.CBM.tukeyHSD.tax.csv"
)
fam.cbm.tax.tukey
```

#### Test for statistically significant differences in the means

```{r famTaxSensTwoAnova, echo=FALSE}
stat.param <- 'Sensitivity'

bact.df.stat <- fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ]
bact.df.stat <- bact.df.stat[which(bact.df.stat$Statistic_parameter == stat.param), ]
bact.df.stat  <- bact.df.stat[complete.cases(bact.df.stat), ]
bact.df.stat$Tax_group = rep("Bacteria", nrow(bact.df.stat))
 
euk.df.stat <- fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ]
euk.df.stat <- euk.df.stat[which(euk.df.stat$Statistic_parameter == stat.param), ]
euk.df.stat  <- euk.df.stat[complete.cases(euk.df.stat), ]
euk.df.stat$Tax_group = rep("Eukaryote", nrow(euk.df.stat))

all.df.stat <- fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ]
all.df.stat <- all.df.stat[which(all.df.stat$Statistic_parameter == stat.param), ]
all.df.stat  <- all.df.stat[complete.cases(all.df.stat), ]
all.df.stat$Tax_group <- rep('All', nrow(all.df.stat))

all.df.stat <- rbind(all.df.stat, bact.df.stat)
all.df.stat <- rbind(all.df.stat, euk.df.stat)

all.df.stat$Prediction_tool <- factor(all.df.stat$Prediction_tool, levels = classifiers_full)
all.df.stat$CAZy_class <- factor(all.df.stat$CAZy_class, levels=cazy_class_list)

fam.sens.tax.aov <- aov(Statistic_value ~ Tax_group * Prediction_tool, data = all.df.stat)
write.csv(
  summary(fam.sens.tax.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_family_classification/tax_performance/fam.sens.tax.aov.csv"
)
summary(fam.sens.tax.aov)
```

### Accuracy

```{r famTaxAccTable, echo=FALSE}
acc.tax.fam.sum.df <- build.class.tax.sum(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Accuracy"
)

write.csv(
  acc.tax.fam.sum.df,
  "../report/cazy_family_classification/tax_performance/per_cazy_class/accuracy.summaryTable.csv",
  row.names=FALSE
)

kable(acc.tax.fam.sum.df, caption="Overall performance (represented by the Accuracy) of CAZy class classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famTaxAccCiPerClass, echo=FALSE}
p.fam.tax.acc.ci.per.cl <- build.class.tax.per.class.p(acc.tax.fam.sum.df, "Accuracy")
p.fam.tax.acc.ci.per.cl
```


```{r famTaxAccScatterPerClass, echo=FALSE}
p.fam.tax.acc.scatter.per.cl <- build.class.tax.p.scatter.per.cl(
  fam.tax.df[which(fam.tax.df$Tax_group == 'Bacteria'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'Eukaryote'), ],
  fam.tax.df[which(fam.tax.df$Tax_group == 'All'), ],
  "Accuracy"
)
p.fam.tax.acc.scatter.per.cl
```

```{r famTaxAccSave, include=FALSE}
# save plots
pdf(file = "../report/cazy_family_classification/tax_performance/per_cazy_class/acc.fam.tax.ci.pdf",  width = 11, height = 8)
p.fam.tax.acc.ci.per.cl
dev.off()

pdf(file = "../report/cazy_family_classification/tax_performance/per_cazy_class/acc.fam.tax.scatter.pdf",  width = 11, height = 8)
p.fam.tax.acc.scatter.per.cl
dev.off()
```


# CAZy family multilabel classification tax performance

```{r riCalcFamTax, echo=FALSE}
fam_tax_ri_ari_df.bact <- cazy_fam_tax_df[which(cazy_fam_tax_df$Tax_group == "Bacteria"), ]
fam_tax_ri_ari_df.euk <- cazy_fam_tax_df[which(cazy_fam_tax_df$Tax_group == "Eukaryote"), ]
fam_tax_ri_ari_df.all <- data.frame(cazy_fam_tax_df)
fam_tax_ri_ari_df.all$Tax_group <- rep("All", nrow(fam_tax_ri_ari_df.all))

fam_tax_ri_ari_df.bact$Prediction_tool <- factor(fam_tax_ri_ari_df.bact$Prediction_tool, levels = classifiers_full) # set order data is presented
fam_tax_ri_ari_df.euk$Prediction_tool <- factor(fam_tax_ri_ari_df.euk$Prediction_tool, levels = classifiers_full)
fam_tax_ri_ari_df.all$Prediction_tool <- factor(fam_tax_ri_ari_df.all$Prediction_tool, levels = classifiers_full)

fam_ri_stats_df_tax.bact <- fam_tax_ri_ari_df.bact %>% dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Rand_index_binary)[3],
    "Mean"=mean(Rand_index_binary),
    "Upper CI"=CI(Rand_index_binary)[1],
    "Standard Deviation"=sd(Rand_index_binary)
  )

fam_ri_stats_df_tax.bact$Tax_group <- rep("Bacteria", nrow(fam_ri_stats_df_tax.bact))
fam_ri_stats_df_tax.euk <- fam_tax_ri_ari_df.euk %>% dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Rand_index_binary)[3],
    "Mean"=mean(Rand_index_binary),
    "Upper CI"=CI(Rand_index_binary)[1],
    "Standard Deviation"=sd(Rand_index_binary)
  )
fam_ri_stats_df_tax.euk$Tax_group <- rep("Eukaryote", nrow(fam_ri_stats_df_tax.euk))

fam_ri_stats_df_tax.all <- fam_tax_ri_ari_df.all %>% dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Rand_index_binary)[3],
    "Mean"=mean(Rand_index_binary),
    "Upper CI"=CI(Rand_index_binary)[1],
    "Standard Deviation"=sd(Rand_index_binary)
  )
fam_ri_stats_df_tax.all$Tax_group <- rep("All", nrow(fam_ri_stats_df_tax.all))

fam_ri_stats_df_tax.all <- rbind(fam_ri_stats_df_tax.all, fam_ri_stats_df_tax.bact)
fam_ri_stats_df_tax.all <- rbind(fam_ri_stats_df_tax.all, fam_ri_stats_df_tax.euk)

dir.create("../report/cazy_family_classification/multilabel_classification", recursive=FALSE)
write.csv(fam_ri_stats_df_tax.all, "../report/cazy_family_classification/multilabel_classification/ri_table_tax_performance.csv", row.names=FALSE)

kable(fam_ri_stats_df_tax.all, caption="Rand Index of CAZyme famifier classificiation of CAZy fam annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r riCalcFamPlot, echo=FALSE}
colnames(fam_ri_stats_df_tax.all) <- c("Prediction_tool", "Lower.CI", "Mean", "Upper.CI", "SD", "Taxonomic.Group")
fam_ri_stats_df_tax.all$Taxonomic.Group <- factor(fam_ri_stats_df_tax.all$Taxonomic.Group, levels=c('Bacteria','All','Eukaryote'))

p.ri.fam.tax.ci <- ggplot(fam_ri_stats_df_tax.all %>% dplyr::group_by(Taxonomic.Group),
			  aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=Lower.CI, ymax=Upper.CI)) +
  geom_hline(yintercept=1, linetype="dashed", color ="#5c5c5c") +
  scale_colour_manual(values = colour_set_tools) +
  theme(legend.position = "none",
		plot.background = element_rect(fill = figbg, color = figbg),
		axis.text=element_text(size=10),
		axis.title=element_text(size=12,face="bold"),
		axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Rand Index") +
  facet_wrap(~ Taxonomic.Group, ncol=3)
p.ri.fam.tax.ci
```


```{r ariFamCalc, include=FALSE}
fam_tax_ri_ari_df.bact <- cazy_fam_tax_df[which(cazy_fam_tax_df$Tax_group == "Bacteria"), ]
fam_tax_ri_ari_df.euk <- cazy_fam_tax_df[which(cazy_fam_tax_df$Tax_group == "Eukaryote"), ]
fam_tax_ri_ari_df.all <- data.frame(cazy_fam_tax_df)
fam_tax_ri_ari_df.all$Tax_group <- rep("All", nrow(fam_tax_ri_ari_df.all))

fam_tax_ri_ari_df.bact$Prediction_tool <- factor(fam_tax_ri_ari_df.bact$Prediction_tool, levels = classifiers_full) # set order data is presented
fam_tax_ri_ari_df.euk$Prediction_tool <- factor(fam_tax_ri_ari_df.euk$Prediction_tool, levels = classifiers_full)
fam_tax_ri_ari_df.all$Prediction_tool <- factor(fam_tax_ri_ari_df.all$Prediction_tool, levels = classifiers_full)

fam_ri_stats_df_tax.bact <- fam_tax_ri_ari_df.bact %>% dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Adjusted_Rand_index_binary)[3],
    "Mean"=mean(Adjusted_Rand_index_binary),
    "Upper CI"=CI(Adjusted_Rand_index_binary)[1],
    "Standard Deviation"=sd(Adjusted_Rand_index_binary)
  )

fam_ri_stats_df_tax.bact$Tax_group <- rep("Bacteria", nrow(fam_ri_stats_df_tax.bact))
fam_ri_stats_df_tax.euk <- fam_tax_ri_ari_df.euk %>% dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Adjusted_Rand_index_binary)[3],
    "Mean"=mean(Adjusted_Rand_index_binary),
    "Upper CI"=CI(Adjusted_Rand_index_binary)[1],
    "Standard Deviation"=sd(Adjusted_Rand_index_binary)
  )
fam_ri_stats_df_tax.euk$Tax_group <- rep("Eukaryote", nrow(fam_ri_stats_df_tax.euk))

fam_ari_stats_df_tax.all <- fam_tax_ri_ari_df.all %>% dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Adjusted_Rand_index_binary)[3],
    "Mean"=mean(Adjusted_Rand_index_binary),
    "Upper CI"=CI(Adjusted_Rand_index_binary)[1],
    "Standard Deviation"=sd(Adjusted_Rand_index_binary)
  )
fam_ari_stats_df_tax.all$Tax_group <- rep("All", nrow(fam_ari_stats_df_tax.all))

fam_ari_stats_df_tax.all <- rbind(fam_ari_stats_df_tax.all, fam_ri_stats_df_tax.bact)
fam_ari_stats_df_tax.all <- rbind(fam_ari_stats_df_tax.all, fam_ri_stats_df_tax.euk)

write.csv(fam_ari_stats_df_tax.all, "../report/cazy_family_classification/multilabel_classification/ari_table_tax_performance.csv", row.names=FALSE)

kable(fam_ari_stats_df_tax.all, caption="Adjusted Rand Index of CAZyme famifier classificiation of CAZy fam annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r ariCalcFamPlot, echo=FALSE}
colnames(fam_ari_stats_df_tax.all) <- c("Prediction_tool", "Lower.CI", "Mean", "Upper.CI", "SD", "Taxonomic.Group")
fam_ari_stats_df_tax.all$Taxonomic.Group <- factor(fam_ari_stats_df_tax.all$Taxonomic.Group, levels=c('Bacteria','All','Eukaryote'))

p.ari.fam.tax.ci <- ggplot(fam_ari_stats_df_tax.all %>% dplyr::group_by(Taxonomic.Group),
			  aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=Lower.CI, ymax=Upper.CI)) +
  geom_hline(yintercept=0.9, linetype="dashed", color ="#5c5c5c") +
  geom_hline(yintercept=0.95, linetype="dashed", color ="#5c5c5c") +
  scale_colour_manual(values = colour_set_tools) +
  theme(legend.position = "none",
		plot.background = element_rect(fill = figbg, color = figbg),
		axis.text=element_text(size=10),
		axis.title=element_text(size=12,face="bold"),
		axis.text.x=element_text(angle = 90, size=8, vjust=0.45)) +
  xlab("Classifier") + 
  ylab("Adjusted Rand Index") +
  facet_wrap(~ Taxonomic.Group, ncol=3)
p.ari.fam.tax.ci
```

```{r riAriFamSave, include=FALSE}
pdf(file = "../report/cazy_family_classification/multilabel_classification/cazyFamMLC.RI.ci.pdf",  width = 8.23, height = 5)
p.ri.fam.tax.ci
dev.off()

pdf(file = "../report/cazy_family_classification/multilabel_classification/cazyFamMLC.ARI.ci.pdf",  width = 8.23, height = 5)
p.ari.fam.tax.ci
dev.off()
```

## Testing for statistically significant differences in the ARI

All classifiers achieved a mean RI across all proteins of greater than 0.99. However, the ARI was more variable between the tools. Therefore, a two-way ANOVA was performed to test for statistially significant differences between the taxonomic kingdoms of the test sets and the CAZyme classifiers.

```{r ariAnovaTaxFamClassification, include=FALSE}
fam_ari_df.bact <- fam_tax_ri_ari_df.bact[, c("Prediction_tool", "Adjusted_Rand_index_freq", "Tax_group")]
fam_ari_df.euk <- fam_tax_ri_ari_df.euk[, c("Prediction_tool", "Adjusted_Rand_index_freq", "Tax_group")]
fam_ari_df.all <- fam_tax_ri_ari_df.all[, c("Prediction_tool", "Adjusted_Rand_index_freq", "Tax_group")]

fam_ari_df.all <- rbind(fam_ari_df.all, fam_ari_df.bact)
fam_ari_df.all <- rbind(fam_ari_df.all, fam_ari_df.euk)

fam.ari.tax.aov <- aov(Adjusted_Rand_index_freq ~ Tax_group * Prediction_tool, data = fam_ari_df.all)
write.csv(
  summary(fam.ari.tax.aov) %>% lapply(tidy) %>% bind_rows(),
  file = "../report/cazy_family_classification/multilabel_classification/ari.aov.tax.csv"
)
summary(fam.ari.tax.aov)
```

Statistically significant difference was detected so follow up with a Tukey test to determine if there are differences between groups.

```{r classritaxtukey, echo=FALSE}
fam.ari.tax.tukey <- TukeyHSD(fam.ari.tax.aov, "Tax_group:Prediction_tool")
write.csv(
  as.data.frame(fam.ari.tax.tukey$`Tax_group:Prediction_tool`),
  "../report/cazy_family_classification/multilabel_classification/fam.ari.tukeyHSD.tax.csv"
)
fam.ari.tax.tukey
```
